---
title: "Quantitative Methods for Risk & Portfolio Analysis"
subtitle: "Homework 1: Return Analysis, Continuous Compounding & Modern Portfolio Theory"
author: "Adivya Singh | 2023IPM014"
date: 2026-02-02
date-format: "D MMMM YYYY"
format:
  html:
    theme: cosmo
    toc: true
    toc-location: right
    toc-depth: 4
    toc-title: "Contents"
    code-fold: true
    code-tools:
      source: false
      toggle: true
    code-summary: "Show Code"
    smooth-scroll: true
    link-external-newwindow: true
    self-contained: true
    fig-width: 10
    fig-height: 6
execute:
  warning: false
  message: false
jupyter: python3
---

```{=html}
<style>
/* ============================================================
   CUSTOM CSS: Clean, Professional Styling
   ============================================================ */

/* ==== PAGE LAYOUT: Reduce margins for wider content ==== */
.page-columns .content {
    padding-left: 1rem !important;
    padding-right: 1rem !important;
}

.quarto-container {
    max-width: 1200px !important;
    margin-left: auto;
    margin-right: auto;
}

h1 {
    color: #2C3E50;
    margin-top: 2rem;
}

h2 {
    color: #34495E;
}

h3 {
    color: #2C3E50;
    font-weight: 600;
}

h4 {
    font-weight: 600;
}

/* ==== Override Quarto Callout Colors (match previous report) ==== */
/* Only graded answers remain green (handled by .answer-card / .question-summary) */
/* Callout-tip: BLUE (no fill) */
.callout-tip {
    background-color: transparent !important;
    border-left-color: #2780E3 !important;
}

.callout-tip .callout-title {
    color: #2780E3 !important;
}

/* Force blue lightbulb icon (avoid green mix) */
.callout-tip .callout-icon::before {
    filter: none !important;
    background-image: url("data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20width='16'%20height='16'%20fill='%232780e3'%20viewBox='0%200%2016%2016'%3E%3Cpath%20d='M2%206a6%206%200%201%201%2010.174%204.31c-.203.196-.359.4-.453.619l-.762%201.769A.5.5%200%200%201%2010.5%2013a.5.5%200%200%201%200%201%20.5.5%200%200%201%200%201l-.224.447a1%201%200%200%201-.894.553H6.618a1%201%200%200%201-.894-.553L5.5%2015a.5.5%200%200%201%200-1%20.5.5%200%200%201%200-1%20.5.5%200%200%201-.46-.302l-.761-1.77a1.964%201.964%200%200%200-.453-.618A5.984%205.984%200%200%201%202%206zm6-5a5%205%200%200%200-3.479%208.592c.263.254.514.564.676.941L5.83%2012h4.342l.632-1.467c.162-.377.413-.687.676-.941A5%205%200%200%200%208%201z'/%3E%3C/svg%3E") !important;
}

/* Callout-note: BLUE (no fill) */
.callout-note {
    background-color: transparent !important;
}

.callout-note .callout-title {
    color: #2780E3 !important;
}

/* Callout-warning: RED accent */
.callout-warning {
    background-color: transparent !important;
}

.callout-warning .callout-title {
    color: #FF0039 !important;
}

.callout-note.callout-style-default > .callout-header,
.callout-tip.callout-style-default > .callout-header {
    background-color: rgb(233.4, 242.3, 252.2) !important;
}

.callout-warning.callout-style-simple > .callout-header {
    background-color: rgb(255, 243, 230) !important;
}

.callout-important.callout-style-default > .callout-header,
.callout-important.callout-style-simple > .callout-header {
    background-color: rgb(255, 229.5, 235.2) !important;
}

/* ==== Executive Summary Hero ==== */
.exec-summary-hero {
    background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
    border-left: 5px solid #3498DB;
    padding: 1.5rem 2rem;
    margin: 1.5rem 0;
    border-radius: 0 8px 8px 0;
}

.exec-summary-hero h2 {
    color: #2C3E50;
    margin-top: 0;
    border-bottom: none;
}

/* ==== Answer Card - GREEN: ONLY for graded answer boxes ==== */
.answer-card {
    border-left: 5px solid #27AE60;
    background-color: #f0f9f4;
    padding: 1rem 1.5rem;
    margin: 1.5rem 0;
    border-radius: 0 6px 6px 0;
}

.answer-card h4 {
    color: #27AE60;
    margin-top: 0;
    margin-bottom: 0.6rem;
    font-weight: 600;
    font-size: 1.05rem;
}

/* ==== Question Summary Card - GREEN: for graded question summaries ==== */
.question-summary {
    background-color: #f0f9f4;
    border: 1px solid #c8e6c9;
    border-left: 5px solid #27AE60;
    padding: 1.2rem 1.5rem;
    border-radius: 0 8px 8px 0;
    margin: 1.5rem 0;
}

.question-summary h3 {
    color: #2C3E50;
    margin-top: 0;
    font-size: 1.15rem;
    border-bottom: 2px solid rgba(39, 174, 96, 0.3);
    padding-bottom: 0.4rem;
}

.question-summary table {
    width: 100%;
    margin-top: 0.8rem;
    font-size: 0.9rem;
}

.question-summary th {
    text-align: left;
    color: #2C3E50;
    font-weight: 600;
    padding: 0.4rem 0.5rem;
    border-bottom: 1px solid #c8e6c9;
}

.question-summary td {
    padding: 0.4rem 0.5rem;
    border-bottom: 1px solid rgba(200, 230, 201, 0.5);
    color: #34495E;
}

.question-summary td:first-child {
    font-weight: 600;
    color: #2C3E50;
}

.question-summary a {
    color: #27AE60;
    font-weight: 500;
}

/* ==== Info Card - PASTEL BLUE: for normal insights/info ==== */
.info-card {
    border-left: 5px solid #3498DB;
    background-color: #f0f7fc;
    padding: 1rem 1.5rem;
    margin: 1.5rem 0;
    border-radius: 0 6px 6px 0;
}

.info-card h4 {
    color: #3498DB;
    margin-top: 0;
    margin-bottom: 0.6rem;
    font-weight: 600;
    font-size: 1.05rem;
}

/* ==== Critical Card - PASTEL ORANGE/RED: for critical insights ==== */
.critical-card {
    border-left: 5px solid #E74C3C;
    background-color: #fdf2f2;
    padding: 1rem 1.5rem;
    margin: 1.5rem 0;
    border-radius: 0 6px 6px 0;
}

.critical-card h4 {
    color: #E74C3C;
    margin-top: 0;
    margin-bottom: 0.6rem;
    font-weight: 600;
    font-size: 1.05rem;
}

/* ==== Warning Card - PASTEL AMBER: for warnings/cautions ==== */
.warning-card {
    border-left: 5px solid #F39C12;
    background-color: #fefcf3;
    padding: 1rem 1.5rem;
    margin: 1.5rem 0;
    border-radius: 0 6px 6px 0;
}

.warning-card h4 {
    color: #E67E22;
    margin-top: 0;
    margin-bottom: 0.6rem;
    font-weight: 600;
    font-size: 1.05rem;
}

/* Section Divider */
.section-divider {
    border: none;
    height: 2px;
    background: linear-gradient(90deg, transparent, #bdc3c7, transparent);
    margin: 2.5rem 0;
}

/* Table Enhancements */
.styled-table {
    border-collapse: collapse;
    width: 100%;
    margin: 1rem 0;
    font-size: 0.95rem;
}

.styled-table th {
    background: #2C3E50;
    color: white;
    padding: 10px 12px;
    text-align: left;
    font-weight: 600;
}

.styled-table td {
    padding: 8px 12px;
    border-bottom: 1px solid #ddd;
}

.styled-table tr:nth-child(even) {
    background-color: #f8f9fa;
}

.styled-table tr:hover {
    background-color: #e8f4f8;
}

/* ==== Appendix Sections - Proper width containment ==== */
.appendix-section {
    max-width: 100%;
    width: 100%;
    box-sizing: border-box;
    overflow: hidden;
}

.appendix-section details {
    max-width: 100%;
    width: 100%;
    box-sizing: border-box;
    overflow: hidden;
}

.appendix-section details > * {
    max-width: 100%;
    box-sizing: border-box;
}

.appendix-section h3 {
    max-width: 100%;
    margin-left: 0;
    margin-right: 0;
    padding-left: 1rem;
}

.appendix-section table {
    max-width: calc(100% - 2rem);
    width: auto;
    display: table;
    table-layout: auto;
    margin-left: 1rem;
    margin-right: 1rem;
}

.appendix-section p, 
.appendix-section ul, 
.appendix-section ol {
    max-width: 100%;
    padding-left: 1rem;
    padding-right: 1rem;
    box-sizing: border-box;
}

/* Ensure MathJax equations don't overflow */
.appendix-section .MathJax {
    max-width: 100%;
    overflow-x: auto;
}

/* Fix Plotly figures in appendix sections */
.appendix-section .plotly-graph-div {
    max-width: 100% !important;
    width: 100% !important;
    margin: 0 auto;
}

.appendix-section .js-plotly-plot {
    max-width: 100% !important;
}

/* Ensure details content properly contained */
details .plotly-graph-div {
    max-width: 100% !important;
}

/* Fix table scrolling in details */
details .cell-output-display {
    max-width: 100%;
    overflow-x: auto;
}

/* Subtle attribution footer - only visible on screen, not in print */
@media screen {
  body::after {
    white-space: pre;
    text-align: right; 
    content: "Developed by\A Adivya Singh\A (2023IPM014)";
    position: fixed;
    bottom: 5px;
    right: 10px;
    font-size: 13px;
    color: rgba(100, 100, 100, 0.5);
    font-family: 'Segoe UI', Arial, sans-serif;
    font-weight: 400;
    z-index: 1000;
    pointer-events: none;
    white-space: pre;
    text-align: right; 
  }
}
</style>

<!-- JavaScript to handle Plotly re-rendering when details are opened -->
<script>
document.addEventListener('DOMContentLoaded', function() {
    function autoSizePlotlyTables(root) {
        if (!window.Plotly) return;
        const scope = root || document;
        scope.querySelectorAll('.js-plotly-plot').forEach(function(plot) {
            try {
                if (!plot || !plot.data || !plot.data.length) return;
                const table = plot.data.find(d => d.type === 'table');
                if (!table || !table.cells || !table.cells.values) return;

                const values = table.cells.values;
                const rowCount = (values[0] && values[0].length) ? values[0].length : 0;
                const headerHeight = (table.header && table.header.height) ? table.header.height : 30;
                let rowHeight = 28;
                if (table.cells.height) {
                    rowHeight = Array.isArray(table.cells.height) ? table.cells.height[0] : table.cells.height;
                }
                const padding = 60;
                const totalHeight = Math.round(headerHeight + rowHeight * rowCount + padding);

                Plotly.relayout(plot, { height: totalHeight, autosize: true });
                Plotly.Plots.resize(plot);
            } catch (e) {
                console.log('Plotly table auto-size error:', e);
            }
        });
    }

    // Initial sizing for all tables
    autoSizePlotlyTables(document);

    // Handle Plotly resize and table autosize on details toggle
    document.querySelectorAll('details').forEach(function(details) {
        details.addEventListener('toggle', function() {
            if (details.open) {
                // Slight delay to let DOM update first
                setTimeout(function() {
                    autoSizePlotlyTables(details);
                }, 100);
            }
        });
    });
    
    // Also handle window resize events
    window.addEventListener('resize', function() {
        autoSizePlotlyTables(document);
    });
});
</script>
```

```{python}
#| label: setup
#| include: false

# ============================================================
# SETUP: Import Libraries and Initialize Environment
# ============================================================

import pandas as pd
import numpy as np
from scipy import stats
from scipy.optimize import minimize
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# Set display options for pandas
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', '{:.6f}'.format)

# ============================================================
# COLOR PALETTE: Consistent Visual Design
# ============================================================

COLORS = {
    'primary_blue': '#3498DB',
    'secondary_blue': '#2980B9',
    'product_a': '#3498DB',      # Blue for Product A
    'product_b': '#E67E22',      # Orange for Product B
    'positive': '#27AE60',       # Green for positive/efficient
    'negative': '#E74C3C',       # Red for negative/inefficient
    'warning': '#F39C12',        # Yellow/Orange for warnings
    'neutral': '#95A5A6',        # Gray for neutral
    'dark': '#2C3E50',           # Dark blue for headers
    'light_bg': '#F8F9FA',       # Light background
    'purple': '#9B59B6',         # Purple for deep dives
}

# ============================================================
# DATA: Product A and Product B Monthly Portfolio Values
# ============================================================

# Initial investment
V0 = 100000

# Product A: Month-end portfolio values (INR)
product_a_values = [
    100000,   # Initial (Jan 1)
    100650,   # Jan 31
    101304,   # Feb 28
    101963,   # Mar 31
    102628,   # Apr 30
    103297,   # May 31
    103973,   # Jun 30
    104654,   # Jul 31
    105341,   # Aug 31
    106034,   # Sep 30
    106733,   # Oct 31
    107439,   # Nov 30
    108151,   # Dec 31
]

# Product B: Month-end portfolio values (INR)
product_b_values = [
    100000,   # Initial (Jan 1)
    100700,   # Jan 31
    101200,   # Feb 28
    101900,   # Mar 31
    102400,   # Apr 30
    103100,   # May 31
    103600,   # Jun 30
    104300,   # Jul 31
    104800,   # Aug 31
    105500,   # Sep 30
    106000,   # Oct 31
    106700,   # Nov 30
    107200,   # Dec 31
]

# Month labels
months = ['Initial', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

# Create master dataframe
df_portfolio = pd.DataFrame({
    'Month': months,
    'Month_Num': range(13),
    'Product_A': product_a_values,
    'Product_B': product_b_values,
})

# ============================================================
# CORE CALCULATIONS: Returns Computation
# ============================================================

# --- One-Year Returns (Q1a) ---
annual_return_a = (product_a_values[-1] - V0) / V0
annual_return_b = (product_b_values[-1] - V0) / V0

# Continuously compounded annual returns
annual_cc_return_a = np.log(product_a_values[-1] / V0)
annual_cc_return_b = np.log(product_b_values[-1] / V0)

# --- Monthly Discrete Returns (Q1c) ---
monthly_discrete_a = []
monthly_discrete_b = []

for i in range(1, 13):
    r_a = (product_a_values[i] - product_a_values[i-1]) / product_a_values[i-1]
    r_b = (product_b_values[i] - product_b_values[i-1]) / product_b_values[i-1]
    monthly_discrete_a.append(r_a)
    monthly_discrete_b.append(r_b)

# --- Monthly Log Returns (Q1d) ---
monthly_log_a = []
monthly_log_b = []

for i in range(1, 13):
    g_a = np.log(product_a_values[i] / product_a_values[i-1])
    g_b = np.log(product_b_values[i] / product_b_values[i-1])
    monthly_log_a.append(g_a)
    monthly_log_b.append(g_b)

# Create returns dataframe
df_returns = pd.DataFrame({
    'Month': months[1:],  # Exclude 'Initial'
    'Month_Num': range(1, 13),
    'A_Discrete': monthly_discrete_a,
    'A_Log': monthly_log_a,
    'B_Discrete': monthly_discrete_b,
    'B_Log': monthly_log_b,
})

# --- Summary Statistics (Q1e) ---
def compute_return_stats(returns, name):
    """Compute comprehensive statistics for a return series."""
    return {
        'Series': name,
        'Min': np.min(returns),
        'Max': np.max(returns),
        'Mean': np.mean(returns),
        'Median': np.median(returns),
        'Std_Dev': np.std(returns, ddof=1),
        'Variance': np.var(returns, ddof=1),
        'Skewness': stats.skew(returns),
        'Kurtosis': stats.kurtosis(returns),
        'Range': np.max(returns) - np.min(returns),
        'CV': np.std(returns, ddof=1) / np.mean(returns) if np.mean(returns) != 0 else np.nan,
    }

stats_a_discrete = compute_return_stats(monthly_discrete_a, 'Product A (Discrete)')
stats_a_log = compute_return_stats(monthly_log_a, 'Product A (Log)')
stats_b_discrete = compute_return_stats(monthly_discrete_b, 'Product B (Discrete)')
stats_b_log = compute_return_stats(monthly_log_b, 'Product B (Log)')

df_stats = pd.DataFrame([stats_a_discrete, stats_a_log, stats_b_discrete, stats_b_log])

# --- Model Identification Analysis (Q1f) ---
# Check for near-constant returns (std dev near zero, allowing for rounding)
std_a_discrete = np.std(monthly_discrete_a, ddof=1)
std_a_log = np.std(monthly_log_a, ddof=1)
std_b_discrete = np.std(monthly_discrete_b, ddof=1)
std_b_log = np.std(monthly_log_b, ddof=1)

# Tolerance for near-constant returns (used in summary model labels)
threshold_constant = 1e-4

# Mean returns for model fitting
mean_a_discrete = np.mean(monthly_discrete_a)
mean_a_log = np.mean(monthly_log_a)
mean_b_discrete = np.mean(monthly_discrete_b)
mean_b_log = np.mean(monthly_log_b)

# --- Sharpe Ratio Calculation (Risk-Adjusted Returns) ---
# Assuming risk-free rate of 0 for simplicity (or can use 4% annual = ~0.33% monthly)
rf_monthly = 0.04 / 12  # 4% annual risk-free rate

sharpe_a = (np.mean(monthly_discrete_a) - rf_monthly) / np.std(monthly_discrete_a, ddof=1) if np.std(monthly_discrete_a, ddof=1) > 0 else np.nan
sharpe_b = (np.mean(monthly_discrete_b) - rf_monthly) / np.std(monthly_discrete_b, ddof=1) if np.std(monthly_discrete_b, ddof=1) > 0 else np.nan

# Annualized Sharpe Ratios
sharpe_a_annual = sharpe_a * np.sqrt(12) if not np.isnan(sharpe_a) else np.nan
sharpe_b_annual = sharpe_b * np.sqrt(12) if not np.isnan(sharpe_b) else np.nan

# --- Volatility Comparison ---
volatility_a_annual = np.std(monthly_discrete_a, ddof=1) * np.sqrt(12)
volatility_b_annual = np.std(monthly_discrete_b, ddof=1) * np.sqrt(12)

# --- Pre-formatted strings for inline display ---
annual_return_a_pct = f"{annual_return_a:.4%}"
annual_return_b_pct = f"{annual_return_b:.4%}"
annual_cc_a_pct = f"{annual_cc_return_a:.4%}"
annual_cc_b_pct = f"{annual_cc_return_b:.4%}"
return_diff_pct = f"{(annual_return_a - annual_return_b):.4%}"
return_diff_pct_3sig = f"{(annual_return_a - annual_return_b) * 100:.3g}%"

sharpe_a_str = f"{sharpe_a_annual:.2f}" if not np.isnan(sharpe_a_annual) else "N/A"
sharpe_b_str = f"{sharpe_b_annual:.2f}" if not np.isnan(sharpe_b_annual) else "N/A"

vol_a_str = f"{volatility_a_annual:.2%}"
vol_b_str = f"{volatility_b_annual:.2%}"

# Determine winner for each metric
return_winner = "Product A" if annual_return_a > annual_return_b else "Product B"
sharpe_winner = "Product A" if sharpe_a_annual > sharpe_b_annual else "Product B"
vol_winner = "Product A" if volatility_a_annual < volatility_b_annual else "Product B"

# Model identification results (consistent tolerance with Q1f)
# Note: Model 1 and Model 2 are two ways of describing the same steady growth idea.
# If a product grows by a constant monthly simple return c, then the constant monthly log return is g = ln(1+c).
# With month-end values reported as integers, tiny rounding effects can make either convention look like the better fit.

is_a_constant_discrete = std_a_discrete < threshold_constant
is_a_constant_log = std_a_log < threshold_constant
is_b_constant_discrete = std_b_discrete < threshold_constant
is_b_constant_log = std_b_log < threshold_constant

# Quantify how tiny the Model 1 vs Model 2 difference is for Product A
std_gap_a = std_a_discrete - std_a_log
std_gap_a_pct = std_gap_a * 100

def classify_model(std_discrete, std_log, threshold=threshold_constant):
    """Classify return model using a small-variance cutoff."""
    if std_discrete < threshold and std_log < threshold:
        return (
            "Model 2 (Deliberately Chosen; Model 1 also plausible)",
            "Both simple and log returns are almost constant at this reporting precision. I am choosing to go with Model 2 as statistically, it is a marginally better fit, while also noting that Model 1 is equally plausible if interest is credited discretely.",
        )
    if std_log < threshold:
        return (
            "Model 2 (Approx. Constant Log Return)",
            "Log returns are approximately constant at this reporting precision.",
        )
    if std_discrete < threshold:
        return (
            "Model 1 (Approx. Constant Discrete)",
            "Discrete returns are approximately constant at this reporting precision.",
        )
    return (
        "Model 3 (Variable)",
        "Neither discrete nor log returns are approximately constant.",
    )

model_a, model_a_desc = classify_model(std_a_discrete, std_a_log)
model_b, model_b_desc = classify_model(std_b_discrete, std_b_log)

# Aliases used in Q1 summary card
model_a_code = model_a
model_b_code = model_b
```

::: {.callout-important}
## IMPORTANT
Please **refresh** the page if the visualisations don't load and you see large blank spaces in the report.
:::

# Executive Summary {.unnumbered}

<div class="exec-summary-hero">

## Investment Performance Analysis

This report analyzes two investment products over a 12-month period to understand their performance characteristics. By comparing simple and continuously compounded returns, I identified a stark contrast in their behavior: while Product B fluctuates like a market asset, Product A exhibits an unusual, near-perfect stability. The report also examines the mechanics of continuous compounding and uses a two-asset model to demonstrate how estimation error can distort Markowitz portfolio optimization.

This report seeks to answer three questions:

1. How did Products A and B perform over the year under simple and continuously compounded return conventions, and which return model best describes each product?
2. Why is continuous compounding a useful modelling framework in quantitative finance, and what is one practical limitation of relying on it?
3. In a two-asset setting, how do correlation and estimation error affect diversification, the minimum-variance portfolio, and the reliability of Markowitz optimization?

</div>

```{python}
#| label: executive-kpis
#| echo: true

# ============================================================
# EXECUTIVE SUMMARY: KPI Dashboard
# ============================================================

fig_kpi = make_subplots(
    rows=1, cols=4,
    specs=[[{"type": "indicator"}, {"type": "indicator"}, 
            {"type": "indicator"}, {"type": "indicator"}]],
    horizontal_spacing=0.08
)

# KPI 1: Product A Total Return
fig_kpi.add_trace(
    go.Indicator(
        mode="number+delta",
        value=annual_return_a * 100,
        number={"suffix": "%", "font": {"size": 42, "color": COLORS['product_a']}, "valueformat": ".2f"},
        title={"text": "<b>Product A</b><br><span style='font-size:0.75em;color:gray'>Annual Return</span>",
               "font": {"size": 14}},
        delta={"reference": annual_return_b * 100, "relative": False, "suffix": " pp vs B",
               "valueformat": ".2f", "font": {"size": 12}},
        domain={"row": 0, "column": 0}
    ),
    row=1, col=1
)

# KPI 2: Product B Total Return
fig_kpi.add_trace(
    go.Indicator(
        mode="number",
        value=annual_return_b * 100,
        number={"suffix": "%", "font": {"size": 42, "color": COLORS['product_b']}, "valueformat": ".2f"},
        title={"text": "<b>Product B</b><br><span style='font-size:0.75em;color:gray'>Annual Return</span>",
               "font": {"size": 14}},
        domain={"row": 0, "column": 1}
    ),
    row=1, col=2
)

# KPI 3: Product A Sharpe Ratio
fig_kpi.add_trace(
    go.Indicator(
        mode="number",
        value=sharpe_a_annual if not np.isnan(sharpe_a_annual) else 0,
        number={"font": {"size": 42, "color": COLORS['product_a']}, "valueformat": ".2f"},
        title={"text": "<b>Product A</b><br><span style='font-size:0.75em;color:gray'>Sharpe Ratio (Ann.)</span>",
               "font": {"size": 14}},
        domain={"row": 0, "column": 2}
    ),
    row=1, col=3
)

# KPI 4: Product B Sharpe Ratio
fig_kpi.add_trace(
    go.Indicator(
        mode="number",
        value=sharpe_b_annual if not np.isnan(sharpe_b_annual) else 0,
        number={"font": {"size": 42, "color": COLORS['product_b']}, "valueformat": ".2f"},
        title={"text": "<b>Product B</b><br><span style='font-size:0.75em;color:gray'>Sharpe Ratio (Ann.)</span>",
               "font": {"size": 14}},
        domain={"row": 0, "column": 3}
    ),
    row=1, col=4
)

fig_kpi.update_layout(
    height=180,
    margin=dict(l=20, r=20, t=50, b=20),
    paper_bgcolor='rgba(0,0,0,0)',
    font=dict(family="Segoe UI"),
    title=dict(
        text="<b>Key Performance Indicators</b>",
        x=0.5,
        font=dict(size=16, color=COLORS['dark'])
    )
)

fig_kpi.show()
```

::: {.callout-note appearance="simple"}
## At a Glance
**Winner (Raw Return):** `{python} return_winner` (+`{python} return_diff_pct_3sig`) | **Winner (Risk-Adjusted):** `{python} sharpe_winner` | **Lower Volatility:** `{python} vol_winner`

*Note: Product A's Sharpe ratio is inflated because its volatility over this sample is extremely small. That should not be read as “free outperformance.” See [Section 1.6](#sec-q1f) for the model-identification details.*
:::

---

## Key Findings

```{python}
#| label: portfolio-growth-chart
#| echo: true

# ============================================================
# Portfolio Value Growth Over Time
# ============================================================

fig_growth = go.Figure()

# Product A line
fig_growth.add_trace(go.Scatter(
    x=df_portfolio['Month'],
    y=df_portfolio['Product_A'],
    mode='lines+markers',
    name='Product A',
    line=dict(color=COLORS['product_a'], width=3),
    marker=dict(size=8, symbol='circle'),
    hovertemplate="<b>Product A</b><br>Month: %{x}<br>Value: ₹%{y:,.0f}<extra></extra>"
))

# Product B line
fig_growth.add_trace(go.Scatter(
    x=df_portfolio['Month'],
    y=df_portfolio['Product_B'],
    mode='lines+markers',
    name='Product B',
    line=dict(color=COLORS['product_b'], width=3),
    marker=dict(size=8, symbol='diamond'),
    hovertemplate="<b>Product B</b><br>Month: %{x}<br>Value: ₹%{y:,.0f}<extra></extra>"
))

# Add initial investment reference line
fig_growth.add_hline(
    y=V0, 
    line_dash="dot", 
    line_color=COLORS['neutral'],
    annotation_text="Initial Investment: ₹100,000",
    annotation_position="bottom right",
    annotation_font_size=10,
    annotation_font_color=COLORS['neutral']
)

# Add final value annotations
fig_growth.add_annotation(
    x='Dec', y=product_a_values[-1],
    text=f"₹{product_a_values[-1]:,}",
    showarrow=True, arrowhead=2, arrowsize=1, arrowwidth=1,
    arrowcolor=COLORS['product_a'],
    ax=40, ay=-30,
    font=dict(color=COLORS['product_a'], size=11, weight='bold')
)

fig_growth.add_annotation(
    x='Dec', y=product_b_values[-1],
    text=f"₹{product_b_values[-1]:,}",
    showarrow=True, arrowhead=2, arrowsize=1, arrowwidth=1,
    arrowcolor=COLORS['product_b'],
    ax=40, ay=30,
    font=dict(color=COLORS['product_b'], size=11, weight='bold')
)

fig_growth.update_layout(
    title=dict(
        text="<b>Portfolio Value Trajectory</b><br><sup>12-Month Performance Comparison (Initial Investment: ₹100,000)</sup>",
        x=0.5,
        font=dict(size=16, color=COLORS['dark'])
    ),
    xaxis_title="Month",
    yaxis_title="Portfolio Value (₹)",
    height=480,
    margin=dict(l=60, r=40, t=110, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(
        orientation="h",
        yanchor="bottom",
        y=1.05,
        xanchor="center",
        x=0.5,
        bgcolor='rgba(255,255,255,0.8)'
    ),
    hovermode='x unified',
    font=dict(family="Segoe UI")
)

fig_growth.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(0,0,0,0.05)')
fig_growth.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(0,0,0,0.1)', 
                        tickformat=",", tickprefix="₹")

fig_growth.show()
```

### Summary of Critical Insights

| Dimension | Product A | Product B | Interpretation |
|:----------|:---------:|:---------:|:---------------|
| **Annual Return** | `{python} annual_return_a_pct` | `{python} annual_return_b_pct` | Product A outperforms by `{python} return_diff_pct_3sig` |
| **Annualized Volatility** | `{python} vol_a_str` | `{python} vol_b_str` | Product A exhibits remarkably low volatility |
| **Sharpe Ratio (Ann.)** | `{python} sharpe_a_str` | `{python} sharpe_b_str` | Risk-adjusted performance comparison |
| **Underlying Model** | `{python} model_a` | `{python} model_b` | Distinct return-generating processes |

::: {.callout-important}
## Critical Discovery
**Product A is highly stable (but not exactly constant).** Its month-to-month returns barely move, and the remaining variation is consistent with rounding and reporting precision. In contrast, **Product B’s monthly returns vary clearly** across the year.
:::

---

## Report Structure

This report is organized into three major sections aligned with the homework questions:

::: {.callout-note appearance="minimal"}
**[Question 1: Investment Product Analysis](#question-1-investment-product-analysis-10-marks)**  
Detailed return calculations, statistical analysis, model identification, and discussion of return conventions.
:::

::: {.callout-note appearance="minimal"}
**[Question 2: Continuous Compounding Framework](#question-2-continuous-compounding-framework-5-marks)**  
Theoretical exposition on continuous compounding with numerical examples, limitations, and portfolio implications.
:::

::: {.callout-note appearance="minimal"}
**[Question 3: Portfolio Theory & Optimization](#question-3-portfolio-theory-optimization-5-marks)**  
Two-asset portfolio analysis, efficient frontier construction, and discussion of estimation error in Markowitz optimization.
:::

---

## Notation & Conventions {.unnumbered}

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Notation Reference Guide <em>(click to expand)</em></summary>

For clarity and consistency throughout this report, I have adopted the following standard notation:


```{python}
#| label: notation-table
#| echo: false

# ============================================================
# NOTATION TABLE: All Symbols Defined Upfront
# ============================================================

notation_data = {
    'Symbol': [
        '$V_0$', '$V_t$', '$V_T$',
        '$R$', '$R_t$', '$r$',
        '$r_{cc}$', '$g_t$',
        '$\\mu$', '$\\sigma$', '$\\rho$',
        '$w_A, w_B$', '$\\Sigma$', '$t$', '$T$'
    ],
    'Description': [
        'Initial portfolio value', 
        'Portfolio value at time t',
        'Final portfolio value (at time T)',
        'Simple (discrete) return over entire period',
        'Simple return in period t',
        'Generic return variable',
        'Continuously compounded (log) return',
        'Log return in period t',
        'Expected return (mean)',
        'Standard deviation (volatility)',
        'Correlation coefficient',
        'Portfolio weights for assets A and B',
        'Covariance matrix',
        'Time index (e.g., month number)',
        'Total time period (e.g., 12 months)'
    ],
    'Formula/Range': [
        '₹100,000 in this report',
        'Value after t months',
        'Value after 12 months',
        '$R = \\frac{V_T - V_0}{V_0}$',
        '$R_t = \\frac{V_t - V_{t-1}}{V_{t-1}}$',
        'Context-dependent',
        '$r_{cc} = \\ln(V_T/V_0)$',
        '$g_t = \\ln(V_t/V_{t-1})$',
        '$\\mu \\in \\mathbb{R}$',
        '$\\sigma \\geq 0$',
        '$\\rho \\in [-1, 1]$',
        '$w_A + w_B = 1$',
        '$2 \\times 2$ matrix',
        '$t = 1, 2, \\ldots, 12$',
        '$T = 12$ months'
    ],
    'Units': [
        'INR (Rupees)',
        'INR',
        'INR',
        'Decimal (e.g., 0.08151)',
        'Decimal',
        'Decimal',
        'Decimal',
        'Decimal',
        'Decimal or %',
        'Decimal or %',
        'Unitless',
        'Decimal (sum to 1)',
        'Variance units',
        'Months',
        'Months'
    ]
}

notation_df = pd.DataFrame(notation_data)

# Convert LaTeX symbols to Unicode/HTML for proper display in Plotly tables
symbol_display = [
    'V₀', 'Vₜ', 'V_T',
    'R', 'Rₜ', 'r',
    'r_cc', 'gₜ',
    'μ', 'σ', 'ρ',
    'w_A, w_B', 'Σ', 't', 'T'
]

notation_df['Symbol_Display'] = symbol_display

fig_notation = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Symbol</b>', '<b>Description</b>', '<b>Formula/Range</b>', '<b>Units</b>'],
        fill_color=COLORS['purple'],
        font=dict(color='white', size=13, family='Segoe UI'),
        align='left',
        height=40
    ),
    cells=dict(
        values=[notation_df['Symbol_Display'], notation_df['Description'], notation_df['Formula/Range'], notation_df['Units']],
        fill_color=[['#f8f9fa', '#ffffff'] * 8],  # Pattern repeats, works for any row count
        font=dict(size=12, family='Segoe UI'),
        align=['center', 'left', 'left', 'center'],
        height=32
    )
)])

fig_notation.update_layout(
    title=dict(
        text="<b>Notation Reference Guide</b><br><sup>All symbols used in this report with their meanings and conventions</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    height=600,
    margin=dict(l=20, r=20, t=80, b=20),
    paper_bgcolor='rgba(0,0,0,0)'
)

fig_notation.show()
```

::: {.callout-tip}
## Convention Notes
- **Time Indexing**: $t=0$ represents the initial investment; $t=1, 2, \ldots, 12$ represent month-end values
- **Percentage Display**: Returns are shown as percentages (e.g., 8.151%) but calculated as decimals (0.08151)
- **Annualization**: Q1 calculations use monthly data; where appropriate (e.g., Executive Summary, risk-adjusted metrics), I present annualized figures for interpretability. All metrics are clearly labeled.
- **Risk-Free Rate**: When calculating Sharpe ratios, I have assumed $r_f = 4\%$ annually ($\approx 0.33\%$ monthly)
:::

</details>

---

<hr class="section-divider">

# Question: Investment Product Analysis [10 marks]

```{python}
#| label: q1-setup
#| include: false

# ============================================================
# QUESTION 1: Extended Calculations for All Sub-parts
# ============================================================

# --- Model Fitting for RMSE Analysis (Q1f) ---

# Model 1: Constant Discrete Return
# Under Model 1: V_t = V_0 * (1 + c)^t where c is constant discrete return
# Best estimate of c from data: geometric mean return
c_model1_a = (product_a_values[-1] / V0) ** (1/12) - 1  # Geometric mean
c_model1_b = (product_b_values[-1] / V0) ** (1/12) - 1

# Alternative: arithmetic mean of observed discrete returns
c_model1_a_arith = np.mean(monthly_discrete_a)
c_model1_b_arith = np.mean(monthly_discrete_b)

# Model 2: Constant Continuously Compounded Rate
# Under Model 2: V_t = V_0 * e^(g*t) where g is constant CC rate
# Best estimate: g = (1/12) * ln(V_12/V_0) OR mean of log returns
g_model2_a = np.log(product_a_values[-1] / V0) / 12
g_model2_b = np.log(product_b_values[-1] / V0) / 12

# Alternative: mean of observed log returns (equivalent)
g_model2_a_mean = np.mean(monthly_log_a)
g_model2_b_mean = np.mean(monthly_log_b)

# Generate predicted values under each model
predicted_model1_a = [V0 * (1 + c_model1_a) ** t for t in range(13)]
predicted_model1_b = [V0 * (1 + c_model1_b) ** t for t in range(13)]

predicted_model2_a = [V0 * np.exp(g_model2_a * t) for t in range(13)]
predicted_model2_b = [V0 * np.exp(g_model2_b * t) for t in range(13)]

# Calculate RMSE for each model-product combination
def calculate_rmse(actual, predicted):
    """Calculate Root Mean Square Error."""
    errors = np.array(actual) - np.array(predicted)
    return np.sqrt(np.mean(errors ** 2))

def calculate_mae(actual, predicted):
    """Calculate Mean Absolute Error."""
    errors = np.abs(np.array(actual) - np.array(predicted))
    return np.mean(errors)

def calculate_mape(actual, predicted):
    """Calculate Mean Absolute Percentage Error."""
    errors = np.abs((np.array(actual) - np.array(predicted)) / np.array(actual))
    return np.mean(errors) * 100

# RMSE calculations
rmse_model1_a = calculate_rmse(product_a_values, predicted_model1_a)
rmse_model1_b = calculate_rmse(product_b_values, predicted_model1_b)
rmse_model2_a = calculate_rmse(product_a_values, predicted_model2_a)
rmse_model2_b = calculate_rmse(product_b_values, predicted_model2_b)

# MAE calculations
mae_model1_a = calculate_mae(product_a_values, predicted_model1_a)
mae_model1_b = calculate_mae(product_b_values, predicted_model1_b)
mae_model2_a = calculate_mae(product_a_values, predicted_model2_a)
mae_model2_b = calculate_mae(product_b_values, predicted_model2_b)

# MAPE calculations
mape_model1_a = calculate_mape(product_a_values, predicted_model1_a)
mape_model1_b = calculate_mape(product_b_values, predicted_model1_b)
mape_model2_a = calculate_mape(product_a_values, predicted_model2_a)
mape_model2_b = calculate_mape(product_b_values, predicted_model2_b)

# Constant-growth RMSE (Model 1 and Model 2 are algebraically identical when fit to endpoints)
rmse_const_a = rmse_model2_a
rmse_const_b = rmse_model2_b

# Relative RMSE vs average portfolio value
avg_a = np.mean(product_a_values)
avg_b = np.mean(product_b_values)
rmse_const_a_pct = rmse_const_a / avg_a * 100
rmse_const_b_pct = rmse_const_b / avg_b * 100

# Check for near-constant returns (variance analysis)
# Allow small deviations due to integer NAV rounding
threshold_constant = 1e-4  # Tolerance threshold

is_a_constant_discrete = std_a_discrete < threshold_constant
is_a_constant_log = std_a_log < threshold_constant
is_b_constant_discrete = std_b_discrete < threshold_constant
is_b_constant_log = std_b_log < threshold_constant

# Final model classification with proper logic
def classify_model(std_discrete, std_log, threshold=threshold_constant):
    """
    Classify the return model based on standard deviations.
    Model 1: Constant discrete return (std of discrete returns ≈ 0)
    Model 2: Constant CC rate (std of log returns ≈ 0)
    Model 3: Neither (both vary)
    """
    if std_discrete < threshold and std_log < threshold:
        return (
            "Model 2 (deliberately chosen; Model 1 also plausible)",
            "Both conventions describe the same steady growth at this reporting precision. I have chosen to go with Model 2 since its an imperceptibly better fit, while noting that Model 1 is equally plausible from a realistic point of view.",
        )
    if std_log < threshold:
        return "Model 2", "Approximately Constant Continuously Compounded Rate"
    if std_discrete < threshold:
        return "Model 1", "Approximately Constant Discrete Monthly Return"
    return "Model 3", "Variable Returns (Neither Constant)"

model_a_code, model_a_desc = classify_model(std_a_discrete, std_a_log)
model_b_code, model_b_desc = classify_model(std_b_discrete, std_b_log)

# Pre-formatted strings
c_model1_a_pct = f"{c_model1_a:.6%}"
c_model1_b_pct = f"{c_model1_b:.6%}"
g_model2_a_pct = f"{g_model2_a:.6%}"
g_model2_b_pct = f"{g_model2_b:.6%}"

rmse_m1_a_str = f"₹{rmse_model1_a:,.2f}"
rmse_m1_b_str = f"₹{rmse_model1_b:,.2f}"
rmse_m2_a_str = f"₹{rmse_model2_a:,.2f}"
rmse_m2_b_str = f"₹{rmse_model2_b:,.2f}"
rmse_const_a_str = f"₹{rmse_const_a:,.2f}"
rmse_const_b_str = f"₹{rmse_const_b:,.2f}"
rmse_const_a_pct_str = f"{rmse_const_a_pct:.3f}%"
rmse_const_b_pct_str = f"{rmse_const_b_pct:.3f}%"
```

<div class="question-summary">

### Question 1: Answer Summary Card

| Sub-part | Question | Answer |
|:---------|:---------|:-------|
| **(a)** | One-year return for each product | A: **`{python} annual_return_a_pct`** \| B: **`{python} annual_return_b_pct`** |
| **(b)** | Which delivers higher return? | **Product A** (+`{python} return_diff_pct_3sig`) |
| **(c)** | Monthly discrete returns | [See Table 1.3](#tbl-monthly-discrete) |
| **(d)** | Monthly log returns | [See Table 1.4](#tbl-monthly-log) |
| **(e)** | Min, Max, Mean statistics | [See Table 1.5](#tbl-summary-stats) |
| **(f)** | Model identification | A: **`{python} model_a_code`** \| B: **`{python} model_b_code`** |
| **(g)** | Misleading conventions | [See Discussion](#sec-q1g) |

</div>

::: {.callout-warning}
## Note: Product Classification Caveat
**Product A’s returns are unusually smooth.** A truly market-exposed asset (especially equity) would not show month-to-month returns that are almost identical. The simplest explanation is a product with a rule-based interest accrual (plus rounding of the reported month-end value).

This pattern is consistent with a **low-risk fixed-income / cash-like product** (e.g., money market or short-duration bond accrual), or an instrument with scheduled accrual/amortization, rather than a product whose value is driven by daily market pricing. If this were real portfolio data, it would be important to verify the product type because misclassification can distort reported risk.
:::

## One-Year Rate of Return [1 mark] {#sec-q1a}

The **one-year holding period return** (HPR) measures the total percentage gain or loss from holding an investment over the year. For an investment with initial value $V_0$ and final value $V_T$:

$$
R = \frac{V_T - V_0}{V_0} = \frac{V_T}{V_0} - 1
$$

<div class="answer-card">

#### ✓ Answer 1(a): One-Year Rates of Return

| Product | Initial Value (₹) | Final Value (₹) | **One-Year Return** |
|:--------|------------------:|----------------:|:-------------------:|
| **Product A** | 100,000 | 108,151 | **`{python} annual_return_a_pct`** |
| **Product B** | 100,000 | 107,200 | **`{python} annual_return_b_pct`** |

**Calculation Verification:**

- Product A: $R_A = \frac{108,151 - 100,000}{100,000} = \frac{8,151}{100,000} = 0.08151 = 8.151\%$
- Product B: $R_B = \frac{107,200 - 100,000}{100,000} = \frac{7,200}{100,000} = 0.07200 = 7.200\%$

</div>

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Alternative Return Measures <em>(click to expand)</em></summary>

Beyond simple returns, I have also computed **continuously compounded (log) annual returns**:

$$
r_{cc} = \ln\left(\frac{V_T}{V_0}\right)
$$

```{python}
#| label: alternative-returns
#| echo: true

# Create comparison table
alt_returns = pd.DataFrame({
    'Return Measure': ['Simple (Discrete) Return', 'Continuously Compounded Return', 'Difference'],
    'Product A': [f"{annual_return_a:.6%}", f"{annual_cc_return_a:.6%}", f"{(annual_return_a - annual_cc_return_a):.6%}"],
    'Product B': [f"{annual_return_b:.6%}", f"{annual_cc_return_b:.6%}", f"{(annual_return_b - annual_cc_return_b):.6%}"]
})

fig_alt = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Return Measure</b>', '<b>Product A</b>', '<b>Product B</b>'],
        fill_color=COLORS['dark'],
        font=dict(color='white', size=12),
        align='left',
        height=35
    ),
    cells=dict(
        values=[alt_returns[col] for col in alt_returns.columns],
        fill_color=[COLORS['light_bg'], 'white', 'white'],
        font=dict(size=11),
        align='left',
        height=30
    )
)])

fig_alt.update_layout(
    title=dict(text="<b>Simple vs. Continuously Compounded Annual Returns</b>", x=0.5, font=dict(size=14)),
    height=230,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_alt.show()
```

::: {.callout-tip}
## Key Insight
For moderate return levels (~7 to 8%), simple and continuously compounded returns differ by approximately 0.300 to 0.350%. This difference becomes more pronounced at higher return levels due to the nonlinear relationship: $R = e^{r_{cc}} - 1$.
:::

</details>

---

## Higher Return Comparison [1 mark] {#sec-q1b}

<div class="answer-card">

#### ✓ Answer 1(b): Product with Higher Return

**Product A delivers the higher one-year return**, outperforming Product B by:

- **Absolute difference:** `{python} return_diff_pct`
- **Relative outperformance:** `{python} f"{((annual_return_a / annual_return_b) - 1):.2%}"` higher than Product B

</div>

```{python}
#| label: return-comparison-chart
#| echo: true

# Create horizontal bar comparison
fig_compare = go.Figure()

fig_compare.add_trace(go.Bar(
    y=['Product B', 'Product A'],
    x=[annual_return_b * 100, annual_return_a * 100],
    orientation='h',
    marker=dict(
        color=[COLORS['product_b'], COLORS['product_a']],
        line=dict(color='white', width=2)
    ),
    text=[f"{annual_return_b:.2%}", f"{annual_return_a:.2%}"],
    textposition='outside',
    textfont=dict(size=14, weight='bold'),
    hovertemplate="<b>%{y}</b><br>Annual Return: %{x:.4f}%<extra></extra>"
))

# Add difference annotation
fig_compare.add_annotation(
    x=(annual_return_a * 100 + annual_return_b * 100) / 2,
    y=1.3,
    text=f"<b>Δ = +{(annual_return_a - annual_return_b)*100:.2f} pp</b>",
    showarrow=False,
    font=dict(size=12, color=COLORS['positive']),
    bgcolor='rgba(39, 174, 96, 0.1)',
    bordercolor=COLORS['positive'],
    borderwidth=1,
    borderpad=6
)

fig_compare.update_layout(
    title=dict(
        text=f"<b>Annual Return Comparison</b><br><sup>Product A outperforms by {return_diff_pct_3sig}</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis_title="Annual Return (%)",
    height=250,
    margin=dict(l=100, r=80, t=80, b=50),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    showlegend=False
)

fig_compare.update_xaxes(range=[0, 10], showgrid=True, gridcolor='rgba(0,0,0,0.1)')

fig_compare.show()
```

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Risk-Adjusted Performance Metrics <em>(click to expand)</em></summary>

While Product A has a higher raw return, a sophisticated investor would also consider **risk-adjusted returns**. Using the monthly return volatility:

```{python}
#| label: risk-adjusted-table
#| echo: true

# Create risk-adjusted comparison
risk_data = {
    'Metric': ['Annual Return', 'Monthly Volatility (σ)', 'Annualized Volatility', 
               'Sharpe Ratio (Ann.)', 'Return per Unit Risk'],
    'Product A': [
        f"{annual_return_a:.4%}",
        f"{np.std(monthly_discrete_a, ddof=1):.6%}",
        f"{volatility_a_annual:.4%}",
        f"{sharpe_a_annual:.2f}" if not np.isnan(sharpe_a_annual) else "Undefined (σ≈0)",
        "Exceptional (near-zero volatility)"
    ],
    'Product B': [
        f"{annual_return_b:.4%}",
        f"{np.std(monthly_discrete_b, ddof=1):.4%}",
        f"{volatility_b_annual:.4%}",
        f"{sharpe_b_annual:.2f}",
        f"{annual_return_b / volatility_b_annual:.2f}"
    ],
    'Winner': ['Product A', 'Product A', 'Product A', 'Product A', 'Product A']
}

risk_df = pd.DataFrame(risk_data)

fig_risk = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Metric</b>', '<b>Product A</b>', '<b>Product B</b>', '<b>Winner</b>'],
        fill_color=COLORS['dark'],
        font=dict(color='white', size=11),
        align='left',
        height=35
    ),
    cells=dict(
        values=[risk_df[col] for col in risk_df.columns],
        fill_color=[COLORS['light_bg'], 'white', 'white', 'white'],
        font=dict(size=10),
        align='left',
        height=28
    )
)])

fig_risk.update_layout(
    title=dict(text="<b>Risk-Adjusted Performance Analysis</b>", x=0.5, font=dict(size=13)),
    height=270,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_risk.show()
```

::: {.callout-important}
## Critical Observation
Product A has **unusually low volatility**; its monthly returns have very small variance. That pattern is consistent with a product that accrues interest in a rule-based way (with rounding in the reported values). This is verified in the return model in Section 1.6.
:::


</details>

---

## Monthly Discrete Returns [1 mark] {#sec-q1c}

The **monthly discrete (simple) return** measures the percentage change in portfolio value from one month to the next:

$$
r_t = \frac{V_t - V_{t-1}}{V_{t-1}}
$$

<div class="answer-card">

#### ✓ Answer 1(c): Monthly Discrete Returns {#tbl-monthly-discrete}

```{python}
#| label: monthly-discrete-table
#| echo: true

# Create formatted returns table
returns_display = df_returns.copy()
returns_display['A_Discrete_Pct'] = returns_display['A_Discrete'].apply(lambda x: f"{x:.6%}")
returns_display['B_Discrete_Pct'] = returns_display['B_Discrete'].apply(lambda x: f"{x:.6%}")

fig_discrete = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Month</b>', '<b>Product A Return (r<sub>t</sub>)</b>', '<b>Product B Return (r<sub>t</sub>)</b>'],
        fill_color=COLORS['dark'],
        font=dict(color='white', size=12),
        align='center',
        height=40
    ),
    cells=dict(
        values=[
            returns_display['Month'],
            returns_display['A_Discrete_Pct'],
            returns_display['B_Discrete_Pct']
        ],
        fill_color=[COLORS['light_bg'], 
                   'rgba(52, 152, 219, 0.15)',  # Light blue tint
                   'rgba(230, 126, 34, 0.15)'], # Light orange tint
        font=dict(size=11),
        align='center',
        height=28
    )
)])

fig_discrete.update_layout(
    title=dict(
        text="<b>Table 1.3: Monthly Discrete Returns</b><br><sup>r<sub>t</sub> = (V<sub>t</sub> - V<sub>t-1</sub>) / V<sub>t-1</sub></sup>",
        x=0.5,
        font=dict(size=14)
    ),
    height=500,
    margin=dict(l=20, r=20, t=80, b=20)
)

fig_discrete.show()
```

</div>

```{python}
#| label: monthly-returns-chart
#| echo: true

# Create grouped bar chart of monthly returns
fig_monthly = go.Figure()

fig_monthly.add_trace(go.Bar(
    name='Product A',
    x=df_returns['Month'],
    y=df_returns['A_Discrete'] * 100,
    marker_color=COLORS['product_a'],
    text=[f"{r:.3%}" for r in df_returns['A_Discrete']],
    textposition='outside',
    textfont=dict(size=8),
    hovertemplate="<b>Product A</b><br>%{x}: %{y:.4f}%<extra></extra>"
))

fig_monthly.add_trace(go.Bar(
    name='Product B',
    x=df_returns['Month'],
    y=df_returns['B_Discrete'] * 100,
    marker_color=COLORS['product_b'],
    text=[f"{r:.3%}" for r in df_returns['B_Discrete']],
    textposition='outside',
    textfont=dict(size=8),
    hovertemplate="<b>Product B</b><br>%{x}: %{y:.4f}%<extra></extra>"
))

# Add mean lines
fig_monthly.add_hline(y=np.mean(monthly_discrete_a)*100, line_dash="dash", 
                      line_color=COLORS['product_a'], line_width=2,
                      annotation_text=f"A Mean: {np.mean(monthly_discrete_a):.3%}",
                      annotation_position="top left", annotation_font_size=9)

fig_monthly.add_hline(y=np.mean(monthly_discrete_b)*100, line_dash="dot", 
                      line_color=COLORS['product_b'], line_width=2,
                      annotation_text=f"B Mean: {np.mean(monthly_discrete_b):.3%}",
                      annotation_position="bottom left", annotation_font_size=9)

fig_monthly.update_layout(
    title=dict(
        text="<b>Monthly Discrete Returns: Side-by-Side Comparison</b><br><sup>Note the remarkable consistency of Product A vs. variability of Product B</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis_title="Month",
    yaxis_title="Monthly Return (%)",
    barmode='group',
    height=400,
    margin=dict(l=60, r=40, t=80, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5),
    bargap=0.2,
    bargroupgap=0.1
)

fig_monthly.update_yaxes(showgrid=True, gridcolor='rgba(0,0,0,0.1)')

fig_monthly.show()
```

::: {.callout-tip}
## Visual Pattern Recognition
The chart above reveals a striking pattern: **Product A's returns form a nearly flat line** (all bars nearly identical height), while **Product B oscillates between ~0.48% and ~0.70%** monthly. This visual immediately suggests different underlying return-generating mechanisms.
:::

---

## Monthly Log Returns [1 mark] {#sec-q1d}

The **continuously compounded (log) return** is defined as the natural logarithm of the price ratio:

$$
g_t = \ln\left(\frac{V_t}{V_{t-1}}\right)
$$

This is mathematically equivalent to:
$$
g_t = \ln(1 + r_t)
$$

where $r_t$ is the discrete return. Log returns have the important property of **time additivity**: the sum of log returns equals the log return of the entire period.

<div class="answer-card">

#### ✓ Answer 1(d): Monthly Log Returns {#tbl-monthly-log}

```{python}
#| label: monthly-log-table
#| echo: true

# Create formatted log returns table
returns_display['A_Log_Pct'] = returns_display['A_Log'].apply(lambda x: f"{x:.6%}")
returns_display['B_Log_Pct'] = returns_display['B_Log'].apply(lambda x: f"{x:.6%}")

fig_log = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Month</b>', '<b>Product A Log Return (g<sub>t</sub>)</b>', '<b>Product B Log Return (g<sub>t</sub>)</b>'],
        fill_color=COLORS['dark'],
        font=dict(color='white', size=12),
        align='center',
        height=40
    ),
    cells=dict(
        values=[
            returns_display['Month'],
            returns_display['A_Log_Pct'],
            returns_display['B_Log_Pct']
        ],
        fill_color=[COLORS['light_bg'], 
                   'rgba(52, 152, 219, 0.15)',  # Light blue tint
                   'rgba(230, 126, 34, 0.15)'], # Light orange tint
        font=dict(size=11),
        align='center',
        height=28
    )
)])

fig_log.update_layout(
    title=dict(
        text="<b>Table 1.4: Monthly Log (Continuously Compounded) Returns</b><br><sup>g<sub>t</sub> = ln(V<sub>t</sub> / V<sub>t-1</sub>)</sup>",
        x=0.5,
        font=dict(size=14)
    ),
    height=500,
    margin=dict(l=20, r=20, t=80, b=20)
)

fig_log.show()
```

</div>

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Time Additivity Property Verification <em>(click to expand)</em></summary>

A key property of log returns is that they are **additive across time**:

$$
\sum_{t=1}^{T} g_t = \ln\left(\frac{V_T}{V_0}\right)
$$

Let's verify this property numerically:

```{python}
#| label: additivity-verification
#| echo: true

# Verify additivity
sum_log_a = sum(monthly_log_a)
direct_log_a = np.log(product_a_values[-1] / V0)
sum_log_b = sum(monthly_log_b)
direct_log_b = np.log(product_b_values[-1] / V0)

verify_data = {
    'Method': ['Sum of Monthly Log Returns Σg_t', 'Direct Calculation ln(V₁₂/V₀)', 'Difference (Numerical Error)'],
    'Product A': [f"{sum_log_a:.10f}", f"{direct_log_a:.10f}", f"{abs(sum_log_a - direct_log_a):.2e}"],
    'Product B': [f"{sum_log_b:.10f}", f"{direct_log_b:.10f}", f"{abs(sum_log_b - direct_log_b):.2e}"]
}

verify_df = pd.DataFrame(verify_data)

fig_verify = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Method</b>', '<b>Product A</b>', '<b>Product B</b>'],
        fill_color=COLORS['purple'],
        font=dict(color='white', size=11),
        align='left',
        height=35
    ),
    cells=dict(
        values=[verify_df[col] for col in verify_df.columns],
        fill_color=['#faf8fc', 'white', 'white'],
        font=dict(size=10),
        align='left',
        height=30
    )
)])

fig_verify.update_layout(
    title=dict(text="<b>Verification: Time Additivity of Log Returns</b>", x=0.5, font=dict(size=13)),
    height=230,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_verify.show()
```

::: {.callout-note}
## Mathematical Confirmation
The sum of monthly log returns equals the annual log return to within machine precision (~10⁻¹⁵), confirming the time-additivity property. This is **not true for discrete returns**. Discrete returns do not add across time; they compound multiplicatively.
:::

</details>

---

## Summary Statistics [2 marks] {#sec-q1e}

<div class="answer-card">

#### ✓ Answer 1(e): Minimum, Maximum, and Average Returns {#tbl-summary-stats}

```{python}
#| label: summary-stats-mandated
#| echo: true

# Create mandated summary statistics table (Min, Max, Mean as required)
summary_mandated = pd.DataFrame({
    'Return Series': ['Product A - Discrete', 'Product A - Log', 
                      'Product B - Discrete', 'Product B - Log'],
    'Minimum': [f"{min(monthly_discrete_a):.6%}", f"{min(monthly_log_a):.6%}",
                f"{min(monthly_discrete_b):.6%}", f"{min(monthly_log_b):.6%}"],
    'Maximum': [f"{max(monthly_discrete_a):.6%}", f"{max(monthly_log_a):.6%}",
                f"{max(monthly_discrete_b):.6%}", f"{max(monthly_log_b):.6%}"],
    'Average (Mean)': [f"{np.mean(monthly_discrete_a):.6%}", f"{np.mean(monthly_log_a):.6%}",
                       f"{np.mean(monthly_discrete_b):.6%}", f"{np.mean(monthly_log_b):.6%}"]
})

fig_summary = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Return Series</b>', '<b>Minimum</b>', '<b>Maximum</b>', '<b>Average (Mean)</b>'],
        fill_color=COLORS['dark'],
        font=dict(color='white', size=12),
        align='center',
        height=40
    ),
    cells=dict(
        values=[summary_mandated[col] for col in summary_mandated.columns],
        fill_color=[
            COLORS['light_bg'],
            'rgba(231, 76, 60, 0.2)',   # Light red for minimum
            'rgba(39, 174, 96, 0.2)',   # Light green for maximum
            'white'
        ],
        font=dict(size=11),
        align='center',
        height=32
    )
)])

fig_summary.update_layout(
    title=dict(
        text="<b>Table 1.5: Summary Statistics: Minimum, Maximum, and Average Returns</b>",
        x=0.5,
        font=dict(size=14)
    ),
    height=250,
    margin=dict(l=20, r=20, t=70, b=20)
)

fig_summary.show()
```

</div>

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Visual Comparison: Radar Chart <em>(click to expand)</em></summary>

The radar chart below provides an intuitive visual comparison of key metrics between Product A and Product B:

```{python}
#| label: radar-chart-comparison
#| echo: false

import plotly.graph_objects as go
import numpy as np

# Calculate normalized metrics for radar chart (scale 0-1)
# For meaningful comparison, we need to normalize on interpretable scales

# Metrics to compare (we'll use absolute values and normalize)
metrics_A = {
    'Mean Return': np.mean(monthly_discrete_a) * 100,
    'Volatility': np.std(monthly_discrete_a, ddof=1) * 100,
    'Range': (max(monthly_discrete_a) - min(monthly_discrete_a)) * 100,
    'Skewness': abs(stats.skew(monthly_discrete_a)),
    'Kurtosis': abs(stats.kurtosis(monthly_discrete_a))
}

metrics_B = {
    'Mean Return': np.mean(monthly_discrete_b) * 100,
    'Volatility': np.std(monthly_discrete_b, ddof=1) * 100,
    'Range': (max(monthly_discrete_b) - min(monthly_discrete_b)) * 100,
    'Skewness': abs(stats.skew(monthly_discrete_b)),
    'Kurtosis': abs(stats.kurtosis(monthly_discrete_b))
}

# Normalize to 0-100 scale for better visualization
max_vals = {k: max(metrics_A[k], metrics_B[k], 0.001) for k in metrics_A}
norm_A = [metrics_A[k] / max_vals[k] * 100 for k in metrics_A]
norm_B = [metrics_B[k] / max_vals[k] * 100 for k in metrics_B]

categories = list(metrics_A.keys())

fig_radar = go.Figure()

fig_radar.add_trace(go.Scatterpolar(
    r=norm_A + [norm_A[0]],  # Close the loop
    theta=categories + [categories[0]],
    fill='toself',
    fillcolor='rgba(52, 152, 219, 0.3)',
    line=dict(color=COLORS['product_a'], width=3),
    name='Product A',
    hovertemplate='<b>Product A</b><br>%{theta}: %{r:.1f}%<extra></extra>'
))

fig_radar.add_trace(go.Scatterpolar(
    r=norm_B + [norm_B[0]],
    theta=categories + [categories[0]],
    fill='toself',
    fillcolor='rgba(231, 76, 60, 0.3)',
    line=dict(color=COLORS['product_b'], width=3),
    name='Product B',
    hovertemplate='<b>Product B</b><br>%{theta}: %{r:.1f}%<extra></extra>'
))

fig_radar.update_layout(
    polar=dict(
        radialaxis=dict(
            visible=True,
            range=[0, 110],
            showticklabels=True,
            tickfont=dict(size=9)
        ),
        angularaxis=dict(
            tickfont=dict(size=11, weight='bold')
        )
    ),
    title=dict(
        text="<b>Radar Comparison: Product A vs Product B</b><br><sup>Metrics normalized to 100 (higher = more extreme). Product A's near-zero volatility is striking.</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    height=450,
    margin=dict(l=80, r=80, t=100, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    legend=dict(
        orientation='h',
        yanchor='bottom',
        y=-0.15,
        xanchor='center',
        x=0.5
    ),
    showlegend=True
)

fig_radar.show()
```

::: {.callout-note appearance="minimal"}
## Radar Insights
**Product A** (blue) shows a collapsed shape with near-zero volatility, range, skewness, and kurtosis, which is consistent with nearly constant returns at the reporting precision. **Product B** (red) shows a fuller shape indicating meaningful variability.
:::

</details>

::: {.callout-important}
## Critical Observation from Summary Statistics
**Product A's return statistics are virtually identical across all months:**

- Discrete returns range from `{python} f"{min(monthly_discrete_a):.4%}"` to `{python} f"{max(monthly_discrete_a):.4%}"`; the spread is only `{python} f"{(max(monthly_discrete_a) - min(monthly_discrete_a))*100:.3g}"`%
- Log returns show even smaller variation

**Product B shows substantial variation:**

- Discrete returns range from `{python} f"{min(monthly_discrete_b):.4%}"` to `{python} f"{max(monthly_discrete_b):.4%}"`; the spread is `{python} f"{(max(monthly_discrete_b) - min(monthly_discrete_b))*100:.3g}"`%

This difference is the key to model identification in the next section.
:::

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Extended Statistical Analysis <em>(click to expand)</em></summary>

Beyond the mandated statistics, a more detailed distributional analysis provides additional insight:

```{python}
#| label: extended-stats
#| echo: true

# Extended statistics table
extended_stats = pd.DataFrame({
    'Statistic': ['Mean', 'Median', 'Std. Deviation', 'Variance', 'Range (Max-Min)', 
                  'Coeff. of Variation', 'Skewness', 'Kurtosis'],
    'A (Discrete)': [
        f"{np.mean(monthly_discrete_a):.6%}",
        f"{np.median(monthly_discrete_a):.6%}",
        f"{np.std(monthly_discrete_a, ddof=1):.6%}",
        f"{np.var(monthly_discrete_a, ddof=1):.2e}",
        f"{max(monthly_discrete_a) - min(monthly_discrete_a):.6%}",
        f"{np.std(monthly_discrete_a, ddof=1)/np.mean(monthly_discrete_a):.4f}",
        f"{stats.skew(monthly_discrete_a):.4f}",
        f"{stats.kurtosis(monthly_discrete_a):.4f}"
    ],
    'A (Log)': [
        f"{np.mean(monthly_log_a):.6%}",
        f"{np.median(monthly_log_a):.6%}",
        f"{np.std(monthly_log_a, ddof=1):.6%}",
        f"{np.var(monthly_log_a, ddof=1):.2e}",
        f"{max(monthly_log_a) - min(monthly_log_a):.6%}",
        f"{np.std(monthly_log_a, ddof=1)/np.mean(monthly_log_a):.4f}",
        f"{stats.skew(monthly_log_a):.4f}",
        f"{stats.kurtosis(monthly_log_a):.4f}"
    ],
    'B (Discrete)': [
        f"{np.mean(monthly_discrete_b):.6%}",
        f"{np.median(monthly_discrete_b):.6%}",
        f"{np.std(monthly_discrete_b, ddof=1):.4%}",
        f"{np.var(monthly_discrete_b, ddof=1):.2e}",
        f"{max(monthly_discrete_b) - min(monthly_discrete_b):.4%}",
        f"{np.std(monthly_discrete_b, ddof=1)/np.mean(monthly_discrete_b):.4f}",
        f"{stats.skew(monthly_discrete_b):.4f}",
        f"{stats.kurtosis(monthly_discrete_b):.4f}"
    ],
    'B (Log)': [
        f"{np.mean(monthly_log_b):.6%}",
        f"{np.median(monthly_log_b):.6%}",
        f"{np.std(monthly_log_b, ddof=1):.4%}",
        f"{np.var(monthly_log_b, ddof=1):.2e}",
        f"{max(monthly_log_b) - min(monthly_log_b):.4%}",
        f"{np.std(monthly_log_b, ddof=1)/np.mean(monthly_log_b):.4f}",
        f"{stats.skew(monthly_log_b):.4f}",
        f"{stats.kurtosis(monthly_log_b):.4f}"
    ]
})

fig_extended = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Statistic</b>', '<b>A (Discrete)</b>', '<b>A (Log)</b>', 
                '<b>B (Discrete)</b>', '<b>B (Log)</b>'],
        fill_color=COLORS['purple'],
        font=dict(color='white', size=11),
        align='left',
        height=35
    ),
    cells=dict(
        values=[extended_stats[col] for col in extended_stats.columns],
        fill_color=['#faf8fc'] + ['white'] * 4,
        font=dict(size=10),
        align='left',
        height=28
    )
)])

fig_extended.update_layout(
    title=dict(text="<b>Extended Distributional Statistics</b>", x=0.5, font=dict(size=13)),
    height=350,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_extended.show()
```

```{python}
#| label: box-plots
#| echo: true

# Create box plots for return distributions
fig_box = make_subplots(rows=1, cols=2, subplot_titles=("Discrete Returns", "Log Returns"))

# Discrete returns box plots
fig_box.add_trace(go.Box(
    y=monthly_discrete_a,
    name='Product A',
    marker_color=COLORS['product_a'],
    boxmean=True,
    hovertemplate="Value: %{y:.4%}<extra>Product A (Discrete)</extra>"
), row=1, col=1)

fig_box.add_trace(go.Box(
    y=monthly_discrete_b,
    name='Product B',
    marker_color=COLORS['product_b'],
    boxmean=True,
    hovertemplate="Value: %{y:.4%}<extra>Product B (Discrete)</extra>"
), row=1, col=1)

# Log returns box plots
fig_box.add_trace(go.Box(
    y=monthly_log_a,
    name='Product A',
    marker_color=COLORS['product_a'],
    boxmean=True,
    showlegend=False,
    hovertemplate="Value: %{y:.4%}<extra>Product A (Log)</extra>"
), row=1, col=2)

fig_box.add_trace(go.Box(
    y=monthly_log_b,
    name='Product B',
    marker_color=COLORS['product_b'],
    boxmean=True,
    showlegend=False,
    hovertemplate="Value: %{y:.4%}<extra>Product B (Log)</extra>"
), row=1, col=2)

fig_box.update_layout(
    title=dict(
        text="<b>Return Distribution Comparison: Box Plots</b><br><sup>Product A's near-zero variance is strikingly visible</sup>",
        x=0.5,
        font=dict(size=14)
    ),
    height=400,
    margin=dict(l=60, r=40, t=80, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5),
    showlegend=True
)

fig_box.update_yaxes(tickformat=".2%", showgrid=True, gridcolor='rgba(0,0,0,0.1)')

fig_box.show()
```

::: {.callout-note}
## Interpretation
The box plots summarize the distributional differences:

- **Product A**: Boxes are compressed to nearly a single line, indicating near-zero dispersion
- **Product B**: Clear interquartile range and whiskers showing meaningful variability

For Product A, the mean and median are very close; Product B shows a more typical spread.
:::

### Distribution Histograms: A Closer Look

While box plots show summary statistics, histograms reveal the full shape of the return distributions:

```{python}
#| label: distribution-histograms
#| echo: false

from plotly.subplots import make_subplots
import plotly.graph_objects as go
import numpy as np

fig_hist = make_subplots(
    rows=1, cols=2,
    subplot_titles=('<b>Product A: Discrete Returns</b>', '<b>Product B: Discrete Returns</b>'),
    horizontal_spacing=0.12
)

# Product A histogram - need very narrow bins due to near-constant returns
fig_hist.add_trace(go.Histogram(
    x=[r * 100 for r in monthly_discrete_a],
    nbinsx=20,
    marker_color=COLORS['product_a'],
    opacity=0.75,
    name='Product A',
    hovertemplate='Return: %{x:.4f}%<br>Count: %{y}<extra></extra>'
), row=1, col=1)

# Add mean line for Product A
mean_A = np.mean(monthly_discrete_a) * 100
fig_hist.add_vline(
    x=mean_A, 
    line_dash="dash", 
    line_color=COLORS['dark'],
    line_width=2,
    annotation_text=f"Mean: {mean_A:.4f}%",
    annotation_position="top",
    annotation_font_size=10,
    row=1, col=1
)

# Product B histogram - normal spread expected
fig_hist.add_trace(go.Histogram(
    x=[r * 100 for r in monthly_discrete_b],
    nbinsx=10,
    marker_color=COLORS['product_b'],
    opacity=0.75,
    name='Product B',
    hovertemplate='Return: %{x:.2f}%<br>Count: %{y}<extra></extra>'
), row=1, col=2)

# Add mean line for Product B
mean_B = np.mean(monthly_discrete_b) * 100
fig_hist.add_vline(
    x=mean_B, 
    line_dash="dash", 
    line_color=COLORS['dark'],
    line_width=2,
    annotation_text=f"Mean: {mean_B:.2f}%",
    annotation_position="top",
    annotation_font_size=10,
    row=1, col=2
)

# Add normal distribution overlay for Product B
from scipy import stats as sp_stats
x_range_B = np.linspace(min(monthly_discrete_b) * 100, max(monthly_discrete_b) * 100, 100)
y_normal_B = sp_stats.norm.pdf(x_range_B, mean_B, np.std(monthly_discrete_b) * 100) * len(monthly_discrete_b) * (max(monthly_discrete_b) - min(monthly_discrete_b)) * 100 / 10

fig_hist.add_trace(go.Scatter(
    x=x_range_B,
    y=y_normal_B,
    mode='lines',
    line=dict(color=COLORS['warning'], width=2, dash='dot'),
    name='Normal Fit',
    hoverinfo='skip'
), row=1, col=2)

fig_hist.update_layout(
    title=dict(
        text="<b>Return Distributions: Near-Constant vs. Variable</b><br><sup>Product A clusters at single value; Product B shows spread typical of market returns</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    height=400,
    margin=dict(l=60, r=40, t=100, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    showlegend=False,
    bargap=0.05
)

fig_hist.update_xaxes(title_text='Monthly Return (%)', row=1, col=1, showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_hist.update_xaxes(title_text='Monthly Return (%)', row=1, col=2, showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_hist.update_yaxes(title_text='Frequency', row=1, col=1, showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_hist.update_yaxes(title_text='Frequency', row=1, col=2, showgrid=True, gridcolor='rgba(0,0,0,0.1)')

fig_hist.show()
```

::: {.callout-tip}
## Distribution Insights
- **Product A**: All 12 returns cluster at virtually the same value, creating a single spike, which is characteristic of a constant-return process at the reporting precision
- **Product B**: Returns spread across a range, with the orange dotted line showing a normal distribution fit
:::

</details>

---

## Model Identification [2 marks] {#sec-q1f}

This section determines which theoretical model best describes each product's return-generating process.

**The Three Candidate Models:**

| Model | Description | Mathematical Characterization |
|:------|:------------|:------------------------------|
| **Model 1** | Constant discrete monthly return | $r_t = c$ for all $t$ (discrete returns are constant) |
| **Model 2** | Constant continuously compounded rate | $g_t = c$ for all $t$ (log returns are constant) |
| **Model 3** | Variable returns | Neither $r_t$ nor $g_t$ is constant over time |

### Diagnostic Approach

To identify the correct model, I have employed two complementary methods:

1. **Variance Analysis**: Check if standard deviation of returns is near-zero (within tolerance)
2. **Constant-Growth Diagnostic**: Compare a constant-growth path to actual data

### Case Logic and Elimination

I have evaluated all three models explicitly and have used the observed statistics to eliminate inconsistent cases.

**Case A: Model 3 (variable returns).**
This model requires meaningful variation in returns. Product A does not show that. Its discrete returns range from `{python} f"{min(monthly_discrete_a):.4%}"` to `{python} f"{max(monthly_discrete_a):.4%}"`, and its log returns range from `{python} f"{min(monthly_log_a):.4%}"` to `{python} f"{max(monthly_log_a):.4%}"`. The standard deviations are σ(r) ≈ `{python} f"{std_a_discrete:.2e}"` and σ(g) ≈ `{python} f"{std_a_log:.2e}"`, which are far below the tolerance threshold. The constant-growth RMSE is only `{python} rmse_const_a_str`, about `{python} rmse_const_a_pct_str` of the average value. These facts rule out Model 3 for Product A at the reporting precision.

**Case B: Model 1 (constant discrete return).**
This model requires nearly constant simple returns. Product A satisfies this condition, and the spread in its discrete returns is only `{python} f"{(max(monthly_discrete_a) - min(monthly_discrete_a))*100:.3g}"`% across the year.

**Case C: Model 2 (constant continuously compounded rate).**
This model requires nearly constant log returns. Product A also satisfies this condition, with a log-return spread of `{python} f"{(max(monthly_log_a) - min(monthly_log_a))*100:.3g}"`%.

Because both Model 1 and Model 2 pass the constancy tests and the gap between σ(r) and σ(g) is tiny, the data alone do not reveal the provider’s compounding convention. Therefore I have made a conscious decision and henceforward report **Model 2** as best fit since the log-return series is marginally flatter and continuous compounding is the standard analytical convention. An important point to be noted is that **Model 1 is equally plausible** if the product credits interest discretely.

<div class="answer-card">

#### ✓ Answer 1(f): Model Identification Results

| Product | Best-Fit Model | Justification |
|:--------|:---------------|:--------------|
| **Product A** | **`{python} model_a_code`** | Model 3 is eliminated because both σ(r) and σ(g) are near zero and the constant-growth RMSE is only `{python} rmse_const_a_str` ({python} rmse_const_a_pct_str of average value). Between Model 1 and Model 2, the log-return series is marginally flatter, so model 2 best describes A while noting Model 1 is also plausible. |
| **Product B** | **`{python} model_b_code`** | Both return series show substantial variance (std dev > 0.08%) |

</div>

### Visual Proof: Returns Over Time

Before diving into statistical tests, let's visualize the most compelling evidence: **Product A's returns are nearly flat**, while **Product B's returns fluctuate naturally**.

```{python}
#| label: returns-time-series
#| echo: true

# ============================================================
# THE "OBVIOUS CHART": Visual Model Identification
# ============================================================

months_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

fig_returns_ts = go.Figure()

# Product A returns (near-flat line)
fig_returns_ts.add_trace(go.Scatter(
    x=months_labels,
    y=[r * 100 for r in monthly_discrete_a],
    mode='lines+markers',
    name='Product A Returns',
    line=dict(color=COLORS['positive'], width=4),
    marker=dict(size=10, symbol='circle'),
    hovertemplate="<b>Product A</b><br>Month: %{x}<br>Return: %{y:.4f}%<extra></extra>"
))

# Product B returns (fluctuating line)
fig_returns_ts.add_trace(go.Scatter(
    x=months_labels,
    y=[r * 100 for r in monthly_discrete_b],
    mode='lines+markers',
    name='Product B Returns',
    line=dict(color=COLORS['negative'], width=4),
    marker=dict(size=10, symbol='diamond'),
    hovertemplate="<b>Product B</b><br>Month: %{x}<br>Return: %{y:.4f}%<extra></extra>"
))

# Add mean reference line for Product A
mean_a = np.mean(monthly_discrete_a) * 100
fig_returns_ts.add_hline(
    y=mean_a,
    line_dash="dot",
    line_color=COLORS['positive'],
    line_width=2,
    annotation_text=f"Product A Mean: {mean_a:.4f}%",
    annotation_position="right"
)

# Add mean reference line for Product B
mean_b = np.mean(monthly_discrete_b) * 100
fig_returns_ts.add_hline(
    y=mean_b,
    line_dash="dot",
    line_color=COLORS['negative'],
    line_width=2,
    annotation_text=f"Product B Mean: {mean_b:.4f}%",
    annotation_position="left"
)

# Add annotation highlighting the contrast
fig_returns_ts.add_annotation(
    x='Apr', y=0.655,
    text="<b>Product A: Very Stable</b><br>Very low variance so Model 3 is eliminated. Model 2 is reported and Model 1 is also plausible",
    showarrow=True, arrowhead=2,
    ax=-100, ay=-60,
    font=dict(color=COLORS['positive'], size=10),
    bgcolor='rgba(255,255,255,0.95)',
    bordercolor=COLORS['positive'],
    borderwidth=2
)

fig_returns_ts.add_annotation(
    x='Sep', y=0.57,
    text="<b>Product B: Visible Variation</b><br>Variance > 0 → Model 3",
    showarrow=True, arrowhead=2,
    ax=80, ay=50,
    font=dict(color=COLORS['negative'], size=10),
    bgcolor='rgba(255,255,255,0.95)',
    bordercolor=COLORS['negative'],
    borderwidth=2
)

fig_returns_ts.update_layout(
    title=dict(
        text="<b>The 'Obvious' Visual Test: Monthly Discrete Returns Over Time</b><br><sup>Product A is highly stable (approx. constant), Product B shows realistic volatility</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis_title="Month",
    yaxis_title="Monthly Return (%)",
    height=500,
    margin=dict(l=60, r=80, t=120, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(
        orientation="h",
        yanchor="bottom",
        y=1.06,
        xanchor="center",
        x=0.5
    ),
    hovermode='x unified'
)

fig_returns_ts.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(0,0,0,0.05)')
fig_returns_ts.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(0,0,0,0.1)', range=[0.5, 0.75])

fig_returns_ts.show()
```

::: {.callout-tip}
## Visual Insight
This chart makes the model classification **visually clear** without needing statistical tests:
- **Product A**: Returns are nearly identical so Model 3 is eliminated. **Model 2** best describes A while **Model 1** is equally plausible at this precision
- **Product B**: Returns vary from ~0.47% to ~0.70% so **Model 3** is appropriate
:::

---

### Evidence 1: Variance Analysis

If a product follows **Model 1**, its discrete returns should have near-zero variance (within tolerance).  
If a product follows **Model 2**, its log returns should have near-zero variance (within tolerance).

```{python}
#| label: variance-analysis
#| echo: true

# Create variance analysis table
variance_data = {
    'Product': ['Product A', 'Product A', 'Product B', 'Product B'],
    'Return Type': ['Discrete (r_t)', 'Log (g_t)', 'Discrete (r_t)', 'Log (g_t)'],
    'Std. Deviation': [f"{std_a_discrete:.6%}", f"{std_a_log:.6%}", 
                       f"{std_b_discrete:.4%}", f"{std_b_log:.4%}"],
    'Is Constant?': [
        '✓ Approx.' if std_a_discrete < threshold_constant else '✗ No',
        '✓ Approx.' if std_a_log < threshold_constant else '✗ No',
        '✓ Approx.' if std_b_discrete < threshold_constant else '✗ No',
        '✓ Approx.' if std_b_log < threshold_constant else '✗ No'
    ],
    'Implication': [
        (
            'Approx. Model 1 (Model 2 also passes)'
            if (std_a_discrete < threshold_constant and std_a_log < threshold_constant)
            else ('Approx. Model 1' if std_a_discrete < threshold_constant else 'Not Model 1')
        ),
        (
            'Approx. Model 2 (Model 1 also passes)'
            if (std_a_discrete < threshold_constant and std_a_log < threshold_constant)
            else ('Approx. Model 2' if std_a_log < threshold_constant else 'Not Model 2')
        ),
        'Not Model 1' if std_b_discrete >= threshold_constant else 'Approx. Model 1',
        'Not Model 2' if std_b_log >= threshold_constant else 'Approx. Model 2'
    ]
}

var_df = pd.DataFrame(variance_data)

fig_var = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Product</b>', '<b>Return Type</b>', '<b>Std. Deviation</b>', 
                '<b>Is Constant?</b>', '<b>Implication</b>'],
        fill_color=COLORS['dark'],
        font=dict(color='white', size=11),
        align='left',
        height=35
    ),
    cells=dict(
        values=[var_df[col] for col in var_df.columns],
        fill_color=[
            ['rgba(52, 152, 219, 0.15)', 'rgba(52, 152, 219, 0.15)', 
             'rgba(230, 126, 34, 0.15)', 'rgba(230, 126, 34, 0.15)'],
            'white', 'white', 'white', 'white'
        ],
        font=dict(size=10),
        align='left',
        height=30
    )
)])

fig_var.update_layout(
    title=dict(text="<b>Variance Analysis: Testing for Constant Returns</b>", x=0.5, font=dict(size=13)),
    height=230,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_var.show()
```

### Evidence 2: Constant Growth Diagnostic (RMSE vs. Actual)

When parameters are fit from endpoints, **Model 1 and Model 2 generate the same constant-growth path** because:
$$
\ln(1+\hat{c}) = \hat{g}
$$
So an RMSE “tournament” between them is not meaningful. Instead, we test whether a **constant-growth hypothesis** provides a good approximation by comparing the constant-growth path to the actual values.

```{python}
#| label: constant-growth-rmse
#| echo: true

# Create constant-growth diagnostic table
diagnostic_data = {
    'Diagnostic': ['Constant-Growth RMSE (₹)', 'RMSE as % of Avg Value'],
    'Product A': [rmse_const_a_str, rmse_const_a_pct_str],
    'Product B': [rmse_const_b_str, rmse_const_b_pct_str]
}

diagnostic_df = pd.DataFrame(diagnostic_data)

fig_tourn = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Diagnostic</b>', '<b>Product A</b>', '<b>Product B</b>'],
        fill_color=COLORS['dark'],
        font=dict(color='white', size=12),
        align='center',
        height=40
    ),
    cells=dict(
        values=[diagnostic_df[col] for col in diagnostic_df.columns],
        fill_color=[COLORS['light_bg'], 'white', 'white'],
        font=dict(size=11),
        align='center',
        height=35
    )
)])

fig_tourn.update_layout(
    title=dict(
        text="<b>Constant-Growth Diagnostic (RMSE)</b><br><sup>Lower RMSE indicates closer alignment to constant-growth dynamics</sup>",
        x=0.5,
        font=dict(size=14)
    ),
    height=250,
    margin=dict(l=20, r=20, t=80, b=20)
)

fig_tourn.show()
```

**Interpretation.** The constant-growth RMSE measures the average distance between the observed portfolio path and the best-fitting constant-growth path. Because Model 1 and Model 2 are identical once fit to the same endpoints, this diagnostic does not distinguish between the two models; it only tells us how close the data are to *any* constant-growth path. For Product A, the RMSE is `{python} rmse_const_a_str` (about `{python} rmse_const_a_pct_str` of the average value), which indicates very small deviations consistent with rounding. For Product B, the RMSE is `{python} rmse_const_b_str` (about `{python} rmse_const_b_pct_str`), which indicates a materially weaker constant-growth fit and supports the variable-returns classification.

```{python}
#| label: fitted-vs-actual
#| echo: true

# Create fitted vs actual charts
fig_fit = make_subplots(
    rows=1, cols=2,
    subplot_titles=(
        f"<b>Product A</b><br><sup>Constant-Growth Fit (RMSE: {rmse_model2_a:.2f})</sup>",
        f"<b>Product B</b><br><sup>Constant-growth fit shows larger deviation</sup>"
    ),
    horizontal_spacing=0.1
)

months_num = list(range(13))

# Product A: Actual vs Fitted
fig_fit.add_trace(go.Scatter(
    x=months_num, y=product_a_values,
    mode='markers',
    name='Actual Values',
    marker=dict(color=COLORS['dark'], size=10, symbol='circle'),
    hovertemplate="Month %{x}<br>Actual: ₹%{y:,.0f}<extra></extra>"
), row=1, col=1)

fig_fit.add_trace(go.Scatter(
    x=months_num, y=predicted_model1_a,
    mode='lines',
    name='Model 1 Fit',
    line=dict(color=COLORS['warning'], width=2, dash='dash'),
    hovertemplate="Month %{x}<br>Model 1: ₹%{y:,.0f}<extra></extra>"
), row=1, col=1)

fig_fit.add_trace(go.Scatter(
    x=months_num, y=predicted_model2_a,
    mode='lines',
    name='Model 2 Fit',
    line=dict(color=COLORS['positive'], width=3),
    hovertemplate="Month %{x}<br>Model 2: ₹%{y:,.0f}<extra></extra>"
), row=1, col=1)

# Product B: Actual vs Fitted
fig_fit.add_trace(go.Scatter(
    x=months_num, y=product_b_values,
    mode='markers',
    name='Actual Values',
    marker=dict(color=COLORS['dark'], size=10, symbol='circle'),
    showlegend=False,
    hovertemplate="Month %{x}<br>Actual: ₹%{y:,.0f}<extra></extra>"
), row=1, col=2)

fig_fit.add_trace(go.Scatter(
    x=months_num, y=predicted_model1_b,
    mode='lines',
    name='Model 1 Fit',
    line=dict(color=COLORS['warning'], width=2, dash='dash'),
    showlegend=False,
    hovertemplate="Month %{x}<br>Model 1: ₹%{y:,.0f}<extra></extra>"
), row=1, col=2)

fig_fit.add_trace(go.Scatter(
    x=months_num, y=predicted_model2_b,
    mode='lines',
    name='Model 2 Fit',
    line=dict(color=COLORS['positive'], width=3),
    showlegend=False,
    hovertemplate="Month %{x}<br>Model 2: ₹%{y:,.0f}<extra></extra>"
), row=1, col=2)

fig_fit.update_layout(
    title=dict(
        text="<b>Fitted Models vs. Actual Portfolio Values</b>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    height=400,
    margin=dict(l=60, r=40, t=100, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(orientation="h", yanchor="bottom", y=1.08, xanchor="center", x=0.5)
)

fig_fit.update_xaxes(title_text="Month", showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_fit.update_yaxes(title_text="Portfolio Value (₹)", showgrid=True, gridcolor='rgba(0,0,0,0.1)', tickformat=",")

fig_fit.show()
```


#### Residual Bar Chart: Numerical Comparison

```{python}
#| label: residual-analysis
#| echo: true

# Calculate residuals
residuals_m1_a = np.array(product_a_values) - np.array(predicted_model1_a)
residuals_m2_a = np.array(product_a_values) - np.array(predicted_model2_a)
residuals_m1_b = np.array(product_b_values) - np.array(predicted_model1_b)
residuals_m2_b = np.array(product_b_values) - np.array(predicted_model2_b)

# Create residual plot
fig_resid = make_subplots(
    rows=1, cols=2,
    subplot_titles=("<b>Product A Residuals</b>", "<b>Product B Residuals</b>"),
    horizontal_spacing=0.12
)

# Product A residuals
fig_resid.add_trace(go.Bar(
    x=months_num, y=residuals_m1_a,
    name='Model 1 Residuals',
    marker_color=COLORS['warning'],
    opacity=0.7,
    hovertemplate="Month %{x}<br>Residual: ₹%{y:.2f}<extra>Model 1</extra>"
), row=1, col=1)

fig_resid.add_trace(go.Bar(
    x=months_num, y=residuals_m2_a,
    name='Model 2 Residuals',
    marker_color=COLORS['positive'],
    opacity=0.7,
    hovertemplate="Month %{x}<br>Residual: ₹%{y:.2f}<extra>Model 2</extra>"
), row=1, col=1)

# Product B residuals
fig_resid.add_trace(go.Bar(
    x=months_num, y=residuals_m1_b,
    name='Model 1 Residuals',
    marker_color=COLORS['warning'],
    opacity=0.7,
    showlegend=False,
    hovertemplate="Month %{x}<br>Residual: ₹%{y:.2f}<extra>Model 1</extra>"
), row=1, col=2)

fig_resid.add_trace(go.Bar(
    x=months_num, y=residuals_m2_b,
    name='Model 2 Residuals',
    marker_color=COLORS['positive'],
    opacity=0.7,
    showlegend=False,
    hovertemplate="Month %{x}<br>Residual: ₹%{y:.2f}<extra>Model 2</extra>"
), row=1, col=2)

fig_resid.add_hline(y=0, line_color=COLORS['dark'], line_width=1.5, row=1, col=1)
fig_resid.add_hline(y=0, line_color=COLORS['dark'], line_width=1.5, row=1, col=2)

fig_resid.update_layout(
    title=dict(
        text="<b>Residual Analysis: Actual − Predicted Values</b><br><sup>Smaller residuals indicate better model fit</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    height=350,
    margin=dict(l=60, r=40, t=90, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(orientation="h", yanchor="bottom", y=1.05, xanchor="center", x=0.5),
    barmode='group'
)

fig_resid.update_xaxes(title_text="Month", showgrid=False)
fig_resid.update_yaxes(title_text="Residual (₹)", showgrid=True, gridcolor='rgba(0,0,0,0.1)')

fig_resid.show()
```

::: {.callout-important}
## Definitive Conclusion

**Product A → Model 2 (Deliberately Chosen; Model 1 also plausible)**

- **Model 3 is eliminated** because both return series are almost constant: σ(r) ≈ `{python} f"{std_a_discrete:.2e}"` and σ(g) ≈ `{python} f"{std_a_log:.2e}"`, and the constant-growth RMSE is only `{python} rmse_const_a_str` (about `{python} rmse_const_a_pct_str` of the average value).
- **Model 1 and Model 2 both pass** the constancy test. The difference is tiny: σ(r) − σ(g) ≈ `{python} f"{std_gap_a:.2e}"` (about `{python} f"{abs(std_gap_a_pct):.3g}"`%). This is negligible at the precision of integer month-end values.
- **Final choice and rationale:** I have chosen **Model 2** because the log-return series is marginally flatter and continuous compounding is the standard analytical convention. Imp Note: **Model 1** is equally plausible if the provider credits interest discretely.
- **Personal Opinion:** I strongly believe that the tiny deviation we see from a perfect constant return is due to integer rounding effects in the source data rather than genuine variability. (might be wrong though)

**Product B → Model 3 (Variable Returns)**

- Neither discrete nor log returns are constant (both have σ > 0.08%)
- Returns oscillate between ~0.48% and ~0.70% monthly, showing genuine variability
- Neither Model 1 nor Model 2 can capture this non-constant behavior
- This represents a more realistic investment with time-varying returns
:::

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Complete Error Metrics & Statistical Tests <em>(click to expand)</em></summary>

```{python}
#| label: complete-error-metrics
#| echo: true

# Complete error metrics table
error_metrics = pd.DataFrame({
    'Metric': ['RMSE (₹)', 'MAE (₹)', 'MAPE (%)', 'Max Absolute Error (₹)'],
    'A - Model 1': [
        f"{rmse_model1_a:.2f}",
        f"{mae_model1_a:.2f}",
        f"{mape_model1_a:.4f}",
        f"{max(abs(residuals_m1_a)):.2f}"
    ],
    'A - Model 2': [
        f"{rmse_model2_a:.2f}",
        f"{mae_model2_a:.2f}",
        f"{mape_model2_a:.4f}",
        f"{max(abs(residuals_m2_a)):.2f}"
    ],
    'B - Model 1': [
        f"{rmse_model1_b:.2f}",
        f"{mae_model1_b:.2f}",
        f"{mape_model1_b:.4f}",
        f"{max(abs(residuals_m1_b)):.2f}"
    ],
    'B - Model 2': [
        f"{rmse_model2_b:.2f}",
        f"{mae_model2_b:.2f}",
        f"{mape_model2_b:.4f}",
        f"{max(abs(residuals_m2_b)):.2f}"
    ]
})

fig_error = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Error Metric</b>', '<b>A - Model 1</b>', '<b>A - Model 2</b>', 
                '<b>B - Model 1</b>', '<b>B - Model 2</b>'],
        fill_color=COLORS['purple'],
        font=dict(color='white', size=11),
        align='center',
        height=35
    ),
    cells=dict(
        values=[error_metrics[col] for col in error_metrics.columns],
        fill_color=['#faf8fc'] + ['white'] * 4,
        font=dict(size=10),
        align='center',
        height=30
    )
)])

fig_error.update_layout(
    title=dict(text="<b>Complete Error Metrics Comparison</b>", x=0.5, font=dict(size=13)),
    height=230,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_error.show()
```

**Fitted Model Parameters:**

| Parameter | Product A | Product B |
|:----------|:----------|:----------|
| Model 1: Constant monthly discrete rate $\hat{c}$ | `{python} c_model1_a_pct` | `{python} c_model1_b_pct` |
| Model 2: Constant monthly CC rate $\hat{g}$ | `{python} g_model2_a_pct` | `{python} g_model2_b_pct` |

</details>

---

## Misleading Return Conventions [2 marks] {#sec-q1g}

<div class="answer-card">

#### ✓ Answer 1(g): Why Different Return Conventions Can Be Misleading

Comparing portfolios using **different return conventions** (simple/discrete vs. continuously compounded/log) can lead to **misleading conclusions** for the following reasons:

**1. Non-Linear Transformation**

The relationship between simple return $R$ and log return $r$ is non-linear:
$$
r = \ln(1 + R) \quad \text{and} \quad R = e^r - 1
$$

For a **single period**, $r$ is a strictly increasing function of $R$, so rankings do **not** flip. However, the **magnitude** of differences changes, which matters when comparing performance levels or aggregating over time.

**2. Aggregation Inconsistency**

- **Simple returns** aggregate multiplicatively over time: $(1+R_{total}) = \prod_{t=1}^T (1+R_t)$
- **Log returns** aggregate additively: $r_{total} = \sum_{t=1}^T r_t$

Comparing a simple annual return to a sum of log returns (or vice versa) produces mathematically inconsistent comparisons.

**3. Magnitude Dependence**

For small returns, $r \approx R$ (Taylor approximation: $\ln(1+R) \approx R$ for $|R| \ll 1$). But for large returns, the difference becomes substantial:

| Simple Return (R) | Log Return (r) | Difference |
|:-----------------:|:--------------:|:----------:|
| 5% | 4.88% | 0.12% |
| 20% | 18.23% | 1.77% |
| 50% | 40.55% | 9.45% |
| 100% | 69.31% | 30.69% |

</div>

### The Volatility Drag Trap: A Concrete Numerical Example

**The Core Issue:**

Consider two investment portfolios that both average 10% returns per quarter. Despite identical arithmetic means, the portfolio with more volatile (fluctuating) returns accumulates less wealth than the steady one. The mechanism is purely arithmetic:

- **Discrete (simple) returns** average the percentages: (30% - 10% + 25% - 5%) ÷ 4 = 10% average
- **But actual wealth compounds** through multiplication, not addition: ₹100 × 1.30 × 0.90 × 1.25 × 0.95 = ₹139.05
- That 10% average would predict: ₹100 × 1.10 × 1.10 × 1.10 × 1.10 = ₹146.41
- **The difference (₹7.36 per ₹100)** is the compounding penalty created by volatility. In short, volatility kills profits when focusing only on arithmetic averages.

**In finance terms:** This gap between the arithmetic mean (10%) and the geometric mean (actual wealth growth) is called **volatility drag**. Higher return volatility increases the gap and reduces compound growth, even when the arithmetic mean is unchanged.

This leads to a common reporting pitfall: comparing investments using arithmetic average returns without explicitly accounting for compounding.

```{python}
#| label: volatility-drag-main
#| echo: true

# ============================================================
# VOLATILITY DRAG: When Averages Mislead
# ============================================================
# Two portfolios with IDENTICAL arithmetic mean returns but 
# vastly different volatilities, illustrating why comparing means can mislead.

mean_return = 0.10  # 10% arithmetic mean

# Portfolio Low Vol: 10% every period (perfect consistency)
low_vol_returns = np.array([0.10, 0.10, 0.10, 0.10])

# Portfolio High Vol: Same arithmetic mean (10%), high volatility
high_vol_returns = np.array([0.30, -0.10, 0.25, -0.05])  # mean = 10%

low_vol_growth = 100000 * np.cumprod(1 + low_vol_returns)
high_vol_growth = 100000 * np.cumprod(1 + high_vol_returns)

periods = ['Q0', 'Q1', 'Q2', 'Q3', 'Q4']

fig_trap = go.Figure()

fig_trap.add_trace(go.Scatter(
    x=periods,
    y=[100000] + list(low_vol_growth),
    mode='lines+markers',
    name=f'Low-Volatility (σ={np.std(low_vol_returns):.1%})',
    line=dict(color=COLORS['positive'], width=4),
    marker=dict(size=12),
    hovertemplate="<b>Low-Volatility Portfolio</b><br>%{x}: ₹%{y:,.0f}<extra></extra>"
))

fig_trap.add_trace(go.Scatter(
    x=periods,
    y=[100000] + list(high_vol_growth),
    mode='lines+markers',
    name=f'High-Volatility (σ={np.std(high_vol_returns):.1%})',
    line=dict(color=COLORS['negative'], width=4),
    marker=dict(size=12),
    hovertemplate="<b>High-Volatility Portfolio</b><br>%{x}: ₹%{y:,.0f}<extra></extra>"
))

# Add wealth difference annotation
wealth_diff = low_vol_growth[-1] - high_vol_growth[-1]
fig_trap.add_annotation(
    x='Q4', y=(low_vol_growth[-1] + high_vol_growth[-1]) / 2,
    text=f"<b>₹{wealth_diff:,.0f}</b><br>wealth gap<br>due to volatility",
    showarrow=True, arrowhead=2,
    ax=80, ay=0,
    font=dict(color=COLORS['negative'], size=12),
    bgcolor='rgba(255,255,255,0.9)',
    bordercolor=COLORS['negative'],
    borderwidth=1
)

# Add final value annotations
fig_trap.add_annotation(
    x='Q4', y=low_vol_growth[-1],
    text=f"₹{low_vol_growth[-1]:,.0f}",
    showarrow=True, arrowhead=2, ax=50, ay=-30,
    font=dict(color=COLORS['positive'], weight='bold', size=12)
)

fig_trap.add_annotation(
    x='Q4', y=high_vol_growth[-1],
    text=f"₹{high_vol_growth[-1]:,.0f}",
    showarrow=True, arrowhead=2, ax=50, ay=30,
    font=dict(color=COLORS['negative'], weight='bold', size=12)
)

fig_trap.update_layout(
    title=dict(
        text="<b>Volatility Drag Trap: Same Mean, Different Wealth</b><br><sup>Both portfolios have a 10% average quarterly return, yet terminal wealth differs by ₹7,472 due to compounding.</sup>",
        x=0.5,
        font=dict(size=14)
    ),
    xaxis_title="Period",
    yaxis_title="Portfolio Value (₹)",
    height=450,
    margin=dict(l=60, r=80, t=100, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
)

fig_trap.update_yaxes(tickformat=",", tickprefix="₹", range=[95000, 150000])

fig_trap.show()
```

#### Side-by-Side: Arithmetic Mean vs. Geometric Mean

The table below quantifies exactly why "same average return" is misleading:

```{python}
#| label: volatility-drag-comparison
#| echo: false

import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Calculate metrics for both portfolios
low_arith = np.mean(low_vol_returns)
low_geom = (np.prod(1 + low_vol_returns) ** (1/len(low_vol_returns))) - 1
low_std = np.std(low_vol_returns, ddof=0)
low_drag = low_arith - low_geom

high_arith = np.mean(high_vol_returns)
high_geom = (np.prod(1 + high_vol_returns) ** (1/len(high_vol_returns))) - 1
high_std = np.std(high_vol_returns, ddof=0)
high_drag = high_arith - high_geom

# Create side-by-side comparison
fig_compare = make_subplots(
    rows=1, cols=2,
    subplot_titles=('<b>Low-Volatility Portfolio</b>', '<b>High-Volatility Portfolio</b>'),
    specs=[[{'type': 'indicator'}, {'type': 'indicator'}]]
)

# Create comparison table instead
compare_data = {
    'Metric': ['Arithmetic Mean', 'Volatility (σ)', 'Geometric Mean', 
               'Volatility Drag', 'σ²/2 Approximation', 'Final Wealth (₹1L start)'],
    'Low-Volatility': [
        f"{low_arith:.2%}",
        f"{low_std:.2%}",
        f"{low_geom:.2%}",
        f"{low_drag:.4%}",
        f"{low_std**2/2:.4%}",
        f"₹{low_vol_growth[-1]:,.0f}"
    ],
    'High-Volatility': [
        f"{high_arith:.2%}",
        f"{high_std:.2%}",
        f"{high_geom:.2%}",
        f"{high_drag:.2%}",
        f"{high_std**2/2:.2%}",
        f"₹{high_vol_growth[-1]:,.0f}"
    ],
    'Difference': [
        "0.000%",
        f"{(high_std-low_std)*100:+.3g}%",
        f"{(high_geom-low_geom)*100:+.3g}%",
        f"{(high_drag-low_drag)*100:+.3g}%",
        f"{(high_std**2/2-low_std**2/2)*100:+.3g}%",
        f"−₹{low_vol_growth[-1]-high_vol_growth[-1]:,.0f}"
    ]
}

compare_df = pd.DataFrame(compare_data)

# Color-code the difference column
diff_colors = ['white', 'rgba(231,76,60,0.2)', 'rgba(231,76,60,0.2)', 
               'rgba(231,76,60,0.2)', 'rgba(231,76,60,0.2)', 'rgba(231,76,60,0.3)']

fig_comp_table = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Metric</b>', '<b>Low-Volatility</b>', '<b>High-Volatility</b>', '<b>Difference</b>'],
        fill_color=COLORS['dark'],
        font=dict(color='white', size=11),
        align='center',
        height=35
    ),
    cells=dict(
        values=[compare_df[col] for col in compare_df.columns],
        fill_color=[COLORS['light_bg'], 'rgba(39,174,96,0.15)', 
                   'rgba(231,76,60,0.15)', diff_colors],
        font=dict(size=10),
        align='center',
        height=28
    )
)])

fig_comp_table.update_layout(
    title=dict(
        text="<b>Anatomy of Volatility Drag: Same Mean, Different Outcomes</b>",
        x=0.5,
        font=dict(size=13)
    ),
    height=300,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_comp_table.show()
```

::: {.callout-important}
## Reporting Risk: Misreading Average Returns

**The Scenario:** A portfolio manager reports that Fund A and Fund B both delivered a "10% average quarterly return." A decision-maker interprets this as a like-for-like comparison and allocates equally between them.

**The Reality:**
- Fund A (low volatility): Compounds steadily, ending at **₹146,410**
- Fund B (high volatility): Greater variability reduces compound growth, ending at **₹138,938**
- **Difference: ₹7,472**, which is about 7.5% of the initial investment, explained by volatility drag alone.

**The Formula:** Geometric mean ≈ Arithmetic mean - $\frac{\sigma^2}{2}$

This approximation explains why high-volatility portfolios systematically underperform their arithmetic mean would suggest.
:::

::: {.callout-tip}
## Best Practice: Ensuring Fair Comparisons

To avoid misleading portfolio comparisons:

1. **Always use the same return convention** when comparing investments
2. **Prefer log returns** for multi-period analysis (due to time-additivity)
3. **Report both conventions** when returns are large or volatility is high
4. **Consider geometric mean** rather than arithmetic mean for long-term performance
5. **Account for volatility drag** when projecting compound growth

**Example Application**: When Product A and Product B are compared:
- **Same convention** (both using simple returns): Fair comparison → A wins by `{python} return_diff_pct_3sig`
- **Mixed conventions** (A in log, B in simple): Would be misleading and invalid
:::

::: {.callout-note appearance="minimal"}
## Extended Analysis Available

For a detailed 10-year Monte Carlo simulation illustrating how volatility drag accumulates over long horizons, see **[Appendix A: Volatility Drag Simulation](#appendix-volatility-drag)**.
:::
---

<hr class="section-divider">

::: {.callout-note appearance="minimal"}
## Transition: From Empirical Analysis to Theoretical Foundations

Having identified that **Product A is best described by Model 2, with Model 1 also plausible at the reporting precision** in Question 1, lets now look at the continuous-compounding framework and why log returns are convenient for multi-period work.
:::

# Question: Continuous Compounding Framework [5 marks]

```{python}
#| label: q2-compounding-setup
#| include: false

# ============================================================
# QUESTION 2: Continuous Compounding Calculations
# ============================================================

import numpy as np
import pandas as pd

# --- Compounding Frequency Comparison ---
nominal_rate = 0.08  # 8% nominal annual rate
frequencies = {
    'Annual': 1,
    'Semi-Annual': 2,
    'Quarterly': 4,
    'Monthly': 12,
    'Weekly': 52,
    'Daily': 365,
    'Continuous': np.inf
}

def effective_rate(r_nominal, n):
    """Calculate effective annual rate for n compounding periods."""
    if n == np.inf:
        return np.exp(r_nominal) - 1
    else:
        return (1 + r_nominal / n) ** n - 1

effective_rates = {freq: effective_rate(nominal_rate, n) for freq, n in frequencies.items()}

# --- Cash Flow Mismatch Simulation (10-year monthly deposits) ---
monthly_deposit = 1000  # ₹1,000 per month

# Use a single rate definition throughout this simulation:
# 6% effective annual rate, converted to its continuously compounded equivalent.
annual_rate_effective = 0.06  # 6% effective annual rate
annual_rate_cc = np.log(1 + annual_rate_effective)  # continuous annual rate (equivalent)
n_years = 10
n_months = n_years * 12

# Model A: Continuous Compounding Approximation
# Assumes deposits are continuously reinvested
# Using integral approximation: FV = D * (e^(r*T) - 1) / r where D = annual deposit rate
# For monthly deposits, we approximate by treating each deposit as earning CC interest from deposit date

# Model B: Discrete Monthly Compounding (Correct Model)
# Each deposit earns (n_months - deposit_month) months of compound interest

monthly_rate_discrete = (1 + annual_rate_effective) ** (1/12) - 1  # Discrete monthly rate (effective basis)
monthly_rate_cc = annual_rate_cc / 12  # CC monthly rate (consistent basis)

# Correct discrete calculation
discrete_fv = 0
discrete_balances = []
for month in range(1, n_months + 1):
    # Each deposit compounds for remaining months
    periods_remaining = n_months - month + 1
    # Add this month's deposit plus interest on existing balance
    discrete_fv = discrete_fv * (1 + monthly_rate_discrete) + monthly_deposit
    discrete_balances.append(discrete_fv)

# Continuous compounding approximation (continuous cash-flow stream)
# FV(t) = (D/r) * (e^(r t) - 1) where D is annual deposit rate
annual_deposit_rate = monthly_deposit * 12
months_axis = np.arange(1, n_months + 1)
cc_balances = (annual_deposit_rate / annual_rate_cc) * (np.exp(annual_rate_cc * (months_axis / 12)) - 1)

# Calculate divergence over time
years = np.arange(1, n_years + 1)
discrete_yearly = [discrete_balances[y * 12 - 1] for y in years]
cc_yearly = [cc_balances[y * 12 - 1] for y in years]

divergence = np.array(cc_yearly) - np.array(discrete_yearly)
divergence_pct = divergence / np.array(discrete_yearly) * 100

# Final values
final_discrete = discrete_balances[-1]
final_cc = cc_balances[-1]
final_diff = final_cc - final_discrete
final_diff_pct = final_diff / final_discrete * 100

# --- Time Additivity Demonstration ---
# Using Product A data from Q1
sum_log_returns_a = sum(monthly_log_a)
total_log_return_a = np.log(product_a_values[-1] / V0)

# --- Pre-formatted strings ---
nominal_rate_pct = f"{nominal_rate:.0%}"
eff_annual = f"{effective_rates['Annual']:.4%}"
eff_monthly = f"{effective_rates['Monthly']:.4%}"
eff_continuous = f"{effective_rates['Continuous']:.4%}"
eff_diff_pct_3sig = f"{(effective_rates['Continuous'] - effective_rates['Annual'])*100:.3g}%"

final_discrete_str = f"₹{final_discrete:,.2f}"
final_cc_str = f"₹{final_cc:,.2f}"
final_diff_str = f"₹{abs(final_diff):,.2f}"
```

<div class="question-summary">

### Question 2: Answer Summary Card

This essay addresses: **"Why continuous compounding is a useful modelling framework in quant finance"**

| Required Element | Location |
|:-----------------|:---------|
| **One Equation** | [Section 2.1: The Fundamental Equation](#sec-q2-equation) |
| **One Numerical Example** | [Section 2.2: Nominal vs. Effective Returns](#sec-q2-numerical) |
| **One Limitation** | [Section 2.3: Practical Limitations](#sec-q2-limitation) |
| **One Implication** | [Section 2.4: Portfolio-Level Analysis](#sec-q2-implication) |

</div>

::: {.callout-tip}
## One-Page Summary: Why Continuous Compounding Matters

Continuous compounding is best understood as a **continuous-time idealization**: it is not literally how most cash flows occur, but it is often the most useful *model* because it turns multiplicative growth into additive structure.

**1) Core Growth Law (and what the “rate” means)**

**Continuous-time growth:**
$$
V(t) = V_0\,e^{r t}
$$
Here, $r$ is a **continuously compounded** rate. The key modeling advantage is that growth is exponential and smooth.

**Link to simple returns:** If an investment grows from $V_0$ to $V_T$ over $T$ years,
$$
R = \frac{V_T - V_0}{V_0},\quad r_{cc} = \ln\left(\frac{V_T}{V_0}\right)=\ln(1+R)
$$
This is the exact bridge used throughout the report when moving between simple and log-return conventions.

**2) Numerical Meaning: “Nominal” vs “Effective” depends on frequency**

For the report’s example nominal rate of **`{python} nominal_rate_pct`**:

- **Annual compounding effective rate:** `{python} eff_annual` (by definition, this matches the nominal when compounded once)
- **Monthly compounding effective rate:** `{python} eff_monthly` (higher because interest is credited more frequently)
- **Continuous compounding effective rate:** `{python} eff_continuous` (the limiting case as compounding frequency $\to\infty$)

So, at 8% nominal, continuous compounding produces an effective annual rate that is **`{python} eff_diff_pct_3sig`** above annual compounding (consistent with the table and chart in Section 2.2).

**3) Practical Limitation: Cash flows arrive discretely (timing matters)**

Even if prices are modeled continuously, **contributions/dividends/coupons happen at discrete times**. That timing can create small but real differences if discrete cash flows are treated as continuous.

In the report’s 10-year monthly-deposit illustration (₹1,000 per month, 6% effective annual rate expressed on a consistent basis):

- **Correct discrete monthly compounding terminal value:** `{python} final_discrete_str`
- **Continuous-compounding approximation terminal value:** `{python} final_cc_str`
- **Absolute difference:** `{python} final_diff_str` (≈ `{python} f"{abs(final_diff_pct):.2f}%"` of terminal value)

This is why continuous compounding is useful for *pricing and analytics*, but **cash-flow modeling still requires discrete bookkeeping** when accuracy matters.

**4) Portfolio Implication: Log returns make multi-period analysis clean**

Define the period-$t$ log return $g_t = \ln\left(\frac{V_t}{V_{t-1}}\right)$. Then log returns are **time-additive**:
$$
\sum_{t=1}^{T} g_t = \ln\left(\frac{V_T}{V_0}\right)
$$
This property is central to the report’s multi-period reasoning and aligns with the observation in Question 1 that Product A can be described by both Models 1 and 2 (constant discrete vs constant CC rates).

### Bottom Line
Continuous compounding is a deliberately idealized framework that (i) clarifies how **frequency changes effective returns**, (ii) enables **clean multi-period and risk aggregation via log returns**, and (iii) supports continuous-time finance tools, while (iv) requiring care when **discrete cash flows** are material.
:::

---

## The Case for Continuous Compounding in Quantitative Finance

In quantitative finance, **continuous compounding** is a widely used modeling convention because it simplifies multi-period growth and connects naturally to log returns. While no real-world instrument compounds continuously (interest is credited discretely, dividends arrive on specific dates, and markets close overnight), the continuous-time approximation is often accurate enough for analysis and very convenient mathematically.

In the sections below, I have summarized the core equation, provided a numerical example, noted an important practical limitation, and explained the portfolio-level implication that motivates the use of log returns.

---

## The Fundamental Equation of Continuous Compounding {#sec-q2-equation}

<div class="answer-card">

**✓ Required Element 1: The Equation**

The fundamental equation of continuous compounding describes the growth of an investment over time:

$$
\boxed{V(t) = V_0 \cdot e^{r \cdot t}}
$$

Where:

- $V(t)$ = Value of the investment at time $t$
- $V_0$ = Initial investment value
- $r$ = Continuously compounded rate (per unit time)
- $e$ = Euler's number ($\approx 2.71828...$)
- $t$ = Time (typically in years)

</div>

### Derivation from Discrete Compounding

The continuous compounding formula emerges as the limiting case of discrete compounding. Consider an investment with nominal annual rate $r_n$ compounded $n$ times per year:

$$
V(t) = V_0 \left(1 + \frac{r_n}{n}\right)^{n \cdot t}
$$

As the compounding frequency increases without bound:

$$
\lim_{n \to \infty} V_0 \left(1 + \frac{r_n}{n}\right)^{n \cdot t} = V_0 \cdot e^{r_n \cdot t}
$$

This limit converges because $\lim_{n \to \infty} \left(1 + \frac{x}{n}\right)^n = e^x$, a fundamental result in calculus.

```{python}
#| label: convergence-chart
#| echo: true

# Visualize convergence to e^r
r = 0.08
periods = [1, 2, 4, 12, 52, 365, 1000, 10000]
values = [(1 + r/n)**n for n in periods]
continuous_value = np.exp(r)

fig_conv = go.Figure()

fig_conv.add_trace(go.Scatter(
    x=[str(n) for n in periods],
    y=values,
    mode='lines+markers',
    name='(1 + r/n)ⁿ',
    line=dict(color=COLORS['primary_blue'], width=3),
    marker=dict(size=10),
    hovertemplate="n = %{x}<br>Value = %{y:.6f}<extra></extra>"
))

fig_conv.add_hline(
    y=continuous_value,
    line_dash="dash",
    line_color=COLORS['positive'],
    annotation_text=f"e^r = {continuous_value:.6f}",
    annotation_position="top right",
    annotation_font_size=12,
    annotation_font_color=COLORS['positive']
)

fig_conv.update_layout(
    title=dict(
        text="<b>Convergence to Continuous Compounding</b><br><sup>As n → ∞, (1 + r/n)ⁿ → eʳ (shown for r = 8%)</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis_title="Compounding Frequency (n)",
    yaxis_title="Growth Factor (1 year)",
    height=380,
    margin=dict(l=60, r=40, t=80, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    showlegend=True,
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
)

fig_conv.update_xaxes(showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_conv.update_yaxes(showgrid=True, gridcolor='rgba(0,0,0,0.1)', range=[1.08, 1.0835])

fig_conv.show()
```

::: {.callout-note appearance="minimal"}
## Watch the Convergence in Action
The chart below animates how the growth factor approaches $e^r$ as compounding frequency increases from annual to continuous:
:::

```{python}
#| label: convergence-animation
#| echo: false

# Animated convergence: show how (1 + r/n)^n approaches e^r
import plotly.graph_objects as go
import numpy as np

r = 0.08
continuous_limit = np.exp(r)

# Create frames for animation
frames = []
n_values = [1, 2, 4, 12, 52, 365, 1000, 5000, 10000]
cumulative_n = []
cumulative_vals = []

for i, n in enumerate(n_values):
    cumulative_n.append(n)
    cumulative_vals.append((1 + r/n)**n)
    frames.append(go.Frame(
        data=[
            go.Scatter(
                x=cumulative_n.copy(),
                y=cumulative_vals.copy(),
                mode='lines+markers',
                line=dict(color=COLORS['primary_blue'], width=3),
                marker=dict(size=10, color=COLORS['primary_blue']),
                name='(1 + r/n)ⁿ'
            ),
            go.Scatter(
                x=[0.5, max(cumulative_n) * 1.1],
                y=[continuous_limit, continuous_limit],
                mode='lines',
                line=dict(color=COLORS['positive'], width=2, dash='dash'),
                name=f'e^r = {continuous_limit:.6f}'
            )
        ],
        name=str(n),
        layout=go.Layout(
            annotations=[
                dict(
                    x=np.log10(n),
                    y=cumulative_vals[-1],
                    text=f"n={n}: {cumulative_vals[-1]:.6f}",
                    showarrow=True,
                    arrowhead=2,
                    ax=60,
                    ay=-35,
                    font=dict(size=11, color=COLORS['primary_blue']),
                    bgcolor='rgba(255,255,255,0.8)',
                    bordercolor=COLORS['primary_blue'],
                    borderwidth=1,
                    borderpad=3
                ),
                dict(
                    x=3.2,
                    y=continuous_limit + 0.00035,
                    text=f"Gap: {(continuous_limit - cumulative_vals[-1])*100:.3g}%",
                    showarrow=False,
                    font=dict(size=10, color=COLORS['positive']),
                    bgcolor='rgba(255,255,255,0.85)',
                    bordercolor=COLORS['positive'],
                    borderwidth=1,
                    borderpad=3
                )
            ]
        )
    ))

# Initial figure
fig_anim = go.Figure(
    data=[
        go.Scatter(
            x=[1],
            y=[(1 + r/1)**1],
            mode='lines+markers',
            line=dict(color=COLORS['primary_blue'], width=3),
            marker=dict(size=10),
            name='(1 + r/n)ⁿ'
        ),
        go.Scatter(
            x=[0.5, 12000],
            y=[continuous_limit, continuous_limit],
            mode='lines',
            line=dict(color=COLORS['positive'], width=2, dash='dash'),
            name=f'e^r = {continuous_limit:.6f}'
        )
    ],
    frames=frames
)

# Add play/pause buttons
fig_anim.update_layout(
    title=dict(
        text="<b>Racing to e: Animated Convergence</b><br><sup>Click Play to watch (1 + r/n)ⁿ approach its limit as n → ∞</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis=dict(
        title="Compounding Frequency (n)",
        type="log",
        range=[0, 4.2],
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)'
    ),
    yaxis=dict(
        title="Growth Factor (1 year)",
        range=[1.0785, 1.0855],
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)'
    ),
    height=420,
    margin=dict(l=60, r=40, t=80, b=80),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    showlegend=True,
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5),
    updatemenus=[
        dict(
            type="buttons",
            showactive=False,
            y=-0.15,
            x=0.5,
            xanchor="center",
            buttons=[
                dict(
                    label="▶ Play",
                    method="animate",
                    args=[None, {
                        "frame": {"duration": 800, "redraw": True},
                        "fromcurrent": True,
                        "transition": {"duration": 400, "easing": "cubic-in-out"}
                    }]
                ),
                dict(
                    label="⏸ Pause",
                    method="animate",
                    args=[[None], {
                        "frame": {"duration": 0, "redraw": False},
                        "mode": "immediate",
                        "transition": {"duration": 0}
                    }]
                ),
                dict(
                    label="↺ Reset",
                    method="animate",
                    args=[[frames[0].name], {
                        "frame": {"duration": 0, "redraw": True},
                        "mode": "immediate",
                        "transition": {"duration": 0}
                    }]
                )
            ]
        )
    ]
)

fig_anim.show()
```

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Stochastic Extension: Geometric Brownian Motion <em>(click to expand)</em></summary>

In modern quantitative finance, the continuous compounding framework extends naturally to **stochastic processes**. The deterministic equation $V(t) = V_0 e^{rt}$ generalizes to the **Geometric Brownian Motion (GBM)**:

$$
dS_t = \mu S_t \, dt + \sigma S_t \, dW_t
$$

Which has the solution:

$$
S_t = S_0 \exp\left[\left(\mu - \frac{\sigma^2}{2}\right)t + \sigma W_t\right]
$$

This is the foundation of the **Black-Scholes-Merton** option pricing model. The continuous-time framework enables:

1. **Itô calculus** for derivative pricing
2. **Risk-neutral valuation** in complete markets
3. **Closed-form solutions** for vanilla options

Continuous compounding is not only a theoretical device; it also underpins standard continuous-time asset pricing models used in derivative valuation.

</details>

---

## Numerical Example: Nominal vs. Effective Returns {#sec-q2-numerical}

<div class="answer-card">

**✓ Required Element 2: Numerical Example**

Consider a nominal annual interest rate of **`{python} nominal_rate_pct`** compounded at various frequencies:

```{python}
#| label: compounding-comparison
#| echo: true

# Create compounding frequency comparison table
comp_data = {
    'Compounding Frequency': list(frequencies.keys()),
    'Periods per Year (n)': ['1', '2', '4', '12', '52', '365', '∞'],
    'Effective Annual Rate': [f"{effective_rates[freq]:.6%}" for freq in frequencies.keys()],
    'Extra Yield vs Annual': [f"+{(effective_rates[freq] - effective_rates['Annual'])*100:.3g}%" 
                               for freq in frequencies.keys()]
}

comp_df = pd.DataFrame(comp_data)

fig_comp = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Compounding Frequency</b>', '<b>Periods/Year (n)</b>', 
                '<b>Effective Annual Rate</b>', '<b>Extra Yield vs Annual</b>'],
        fill_color=COLORS['dark'],
        font=dict(color='white', size=11),
        align='center',
        height=35
    ),
    cells=dict(
        values=[comp_df[col] for col in comp_df.columns],
        fill_color=[COLORS['light_bg'], 'white',
                   ['white']*6 + ['rgba(39, 174, 96, 0.2)'],  # Highlight continuous
                   'white'],
        font=dict(size=10),
        align='center',
        height=28
    )
)])

fig_comp.update_layout(
    title=dict(
        text=f"<b>Effective Annual Rates for {nominal_rate_pct} Nominal Rate</b>",
        x=0.5,
        font=dict(size=13)
    ),
    height=310,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_comp.show()
```

**Key Observations:**

- **Annual compounding** yields exactly `{python} eff_annual` (the nominal rate)
- **Monthly compounding** yields `{python} eff_monthly`; a gain of `{python} f"{(effective_rates['Monthly'] - effective_rates['Annual'])*100:.3g}"`%
- **Continuous compounding** yields `{python} eff_continuous`; a gain of `{python} eff_diff_pct_3sig` over annual

The relationship between nominal and effective rates under continuous compounding:

$$
r_{effective} = e^{r_{nominal}} - 1
$$

For `{python} nominal_rate_pct` nominal: $r_{eff} = e^{0.08} - 1 = `{python} f"{effective_rates['Continuous']:.6f}"` = `{python} eff_continuous`$

</div>

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Compounding Frequency Visualization <em>(click to expand)</em></summary>

```{python}
#| label: compounding-visual
#| echo: true

# Bar chart of effective rates
fig_eff = go.Figure()

freqs = list(frequencies.keys())
rates = [effective_rates[f] * 100 for f in freqs]
colors = [COLORS['primary_blue']] * 6 + [COLORS['positive']]

fig_eff.add_trace(go.Bar(
    x=freqs,
    y=rates,
    marker_color=colors,
    text=[f"{r:.3f}%" for r in rates],
    textposition='outside',
    textfont=dict(size=10),
    hovertemplate="<b>%{x}</b><br>Effective Rate: %{y:.4f}%<extra></extra>"
))

fig_eff.add_hline(
    y=nominal_rate * 100,
    line_dash="dot",
    line_color=COLORS['warning'],
    annotation_text="Nominal Rate (8%)",
    annotation_position="top left",
    annotation_font_size=10
)

fig_eff.update_layout(
    title=dict(
        text="<b>Effective Annual Rate by Compounding Frequency</b><br><sup>Diminishing marginal gains as frequency increases</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis_title="Compounding Frequency",
    yaxis_title="Effective Annual Rate (%)",
    height=400,
    margin=dict(l=60, r=40, t=80, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    showlegend=False
)

fig_eff.update_yaxes(range=[7.9, 8.5], showgrid=True, gridcolor='rgba(0,0,0,0.1)')

fig_eff.show()
```

::: {.callout-tip}
## Practical Insight
The difference between daily and continuous compounding is about **0.000100%**, which is negligible in most practical settings. This is one reason continuous compounding is used in theoretical models: it improves tractability with minimal loss of numerical accuracy.
:::

</details>

### Real-World Convention Usage by Market

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Market-Specific Convention Heatmap <em>(click to expand)</em></summary>

Different financial markets have adopted different compounding conventions based on their historical development and practical needs:

```{python}
#| label: convention-heatmap
#| echo: false

import plotly.graph_objects as go
import numpy as np

# Define markets and conventions
markets = ['US Treasury', 'Corporate Bonds', 'LIBOR/SOFR', 'FX Forwards', 
           'Equity Options', 'Interest Rate Swaps', 'Mortgages (US)', 'Crypto/DeFi']
conventions = ['Simple', 'Discrete (Annual)', 'Discrete (Semi-Annual)', 
               'Discrete (Quarterly)', 'Discrete (Monthly)', 'Continuous']

# Usage intensity: 0=Never, 1=Rarely, 2=Sometimes, 3=Often, 4=Primary
# Rows: markets, Cols: conventions
usage_matrix = np.array([
    [0, 0, 4, 0, 0, 1],  # US Treasury: semi-annual primary
    [0, 0, 4, 1, 0, 1],  # Corporate Bonds: semi-annual
    [2, 0, 0, 3, 0, 4],  # LIBOR/SOFR: continuous for derivatives
    [4, 0, 0, 0, 0, 2],  # FX Forwards: simple interest
    [0, 0, 0, 0, 0, 4],  # Equity Options: continuous (Black-Scholes)
    [0, 0, 2, 3, 0, 4],  # Interest Rate Swaps: continuous modeling
    [0, 0, 0, 0, 4, 0],  # Mortgages: monthly discrete
    [1, 0, 0, 0, 0, 4],  # Crypto/DeFi: often continuous (yield farming)
])

# Color scale: white to deep blue
text_matrix = np.where(usage_matrix == 0, '',
              np.where(usage_matrix == 1, 'Rare',
              np.where(usage_matrix == 2, 'Sometimes',
              np.where(usage_matrix == 3, 'Often', '✓ Primary'))))

fig_heat = go.Figure(data=go.Heatmap(
    z=usage_matrix,
    x=conventions,
    y=markets,
    text=text_matrix,
    texttemplate='%{text}',
    textfont=dict(size=10),
    colorscale=[
        [0, '#ffffff'],
        [0.25, '#e8f4f8'],
        [0.5, '#a8d5e5'],
        [0.75, '#4a9ebe'],
        [1, '#1a5276']
    ],
    showscale=False,
    hovertemplate='<b>%{y}</b><br>Convention: %{x}<br>Usage: %{text}<extra></extra>'
))

fig_heat.update_layout(
    title=dict(
        text="<b>Compounding Convention Usage Across Financial Markets</b><br><sup>How different markets have standardized their interest calculations</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis=dict(
        title='Compounding Convention',
        tickangle=45,
        side='bottom'
    ),
    yaxis=dict(
        title='Market/Instrument',
        autorange='reversed'
    ),
    height=450,
    margin=dict(l=120, r=40, t=80, b=100),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)'
)

fig_heat.show()
```

::: {.callout-note appearance="minimal"}
## Key Takeaway
**Continuous compounding dominates in derivatives pricing** (options, swaps) where mathematical tractability is paramount, while **discrete conventions persist in cash instruments** (bonds, mortgages) where actual payment schedules matter.
:::

</details>

---

## Limitation: The Discrete Cash Flow Reality {#sec-q2-limitation}

<div class="answer-card">

**✓ Required Element 3: One Limitation**

**Limitation: Continuous compounding models assume smooth, continuous cash flows, but real-world financial transactions occur at discrete points in time.**

Key manifestations of this limitation:

1. **Interest payments** are made monthly, quarterly, or annually, never continuously
2. **Dividends** arrive on specific ex-dates, not as a continuous yield stream
3. **Trading** occurs at discrete times; markets close overnight and on weekends
4. **Cash inflows/outflows** (deposits, withdrawals) happen at specific moments

When these discrete events are significant relative to the time scale of analysis, continuous compounding models can introduce **systematic errors**.

</div>

### Visualizing the Discrete Reality

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Discrete Cash Flow Impact Visualization <em>(click to expand)</em></summary>

The following chart starkly illustrates the difference between the continuous model's assumption (smooth curve) and reality (discrete steps):

```{python}
#| label: discrete-reality-chart
#| echo: false

import plotly.graph_objects as go
import numpy as np

# Simulate quarterly dividend payments over 2 years
quarters = ['Q1 Y1', 'Q2 Y1', 'Q3 Y1', 'Q4 Y1', 'Q1 Y2', 'Q2 Y2', 'Q3 Y2', 'Q4 Y2']
quarterly_div = 2.50  # ₹2.50 per quarter
stock_price = 100

# Discrete reality: dividends arrive as lumps
discrete_cumulative = [quarterly_div * (i+1) for i in range(8)]

# Continuous model: smooth dividend yield
continuous_yield = 0.10  # 10% annual yield
time_points = np.linspace(0, 2, 100)
continuous_income = stock_price * continuous_yield * time_points

# Create figure
fig_discrete = go.Figure()

# Continuous model (smooth curve)
fig_discrete.add_trace(go.Scatter(
    x=time_points,
    y=continuous_income,
    mode='lines',
    name='Continuous Model (Assumed)',
    line=dict(color=COLORS['warning'], width=3, dash='dash'),
    hovertemplate='Time: %{x:.2f} years<br>Income: ₹%{y:.2f}<extra>Continuous</extra>'
))

# Discrete reality (step function)
x_steps = []
y_steps = []
for i, q in enumerate(quarters):
    t_start = i * 0.25
    t_end = (i + 1) * 0.25
    val = discrete_cumulative[i]
    if i == 0:
        x_steps.extend([0, t_start])
        y_steps.extend([0, 0])
    x_steps.extend([t_start, t_end])
    y_steps.extend([val, val])

fig_discrete.add_trace(go.Scatter(
    x=x_steps,
    y=y_steps,
    mode='lines',
    name='Discrete Reality (Actual)',
    line=dict(color=COLORS['positive'], width=3),
    fill='tozeroy',
    fillcolor='rgba(39, 174, 96, 0.2)',
    hovertemplate='Time: %{x:.2f} years<br>Cumulative Dividends: ₹%{y:.2f}<extra>Discrete</extra>'
))

# Add dividend payment markers
for i, q in enumerate(quarters):
    fig_discrete.add_annotation(
        x=(i + 1) * 0.25 - 0.125,
        y=discrete_cumulative[i] + 0.8,
        text=f'₹{quarterly_div}',
        showarrow=False,
        font=dict(size=9, color=COLORS['positive'])
    )

fig_discrete.update_layout(
    title=dict(
        text="<b>The Discrete Reality: Dividends Arrive in Lumps, Not Streams</b><br><sup>Continuous models assume smooth income; reality delivers quarterly payments</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis_title='Time (Years)',
    yaxis_title='Cumulative Dividend Income (₹)',
    height=400,
    margin=dict(l=60, r=40, t=80, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='center', x=0.5),
    hovermode='x unified'
)

fig_discrete.update_xaxes(showgrid=True, gridcolor='rgba(0,0,0,0.1)', 
                          tickvals=[0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],
                          ticktext=quarters)
fig_discrete.update_yaxes(showgrid=True, gridcolor='rgba(0,0,0,0.1)', tickprefix='₹')

fig_discrete.show()
```

### Simulation: The Cash Flow Mismatch Problem

To quantify this limitation, consider an investor making **monthly deposits of ₹1,000** into an account earning **6% effective annually** over **10 years**. This section compares:

- **Model A (Continuous Approximation):** Uses the *equivalent continuously compounded rate* (converted from the same 6% effective annual rate)
- **Model B (Correct Discrete Model):** Uses the *effective annual rate* converted to monthly discrete compounding

```{python}
#| label: cashflow-simulation
#| echo: true

# Create the cash flow mismatch simulation chart
months_range = list(range(1, n_months + 1))

fig_cf = go.Figure()

# Discrete model (correct)
fig_cf.add_trace(go.Scatter(
    x=months_range,
    y=discrete_balances,
    mode='lines',
    name='Discrete Model (Correct)',
    line=dict(color=COLORS['positive'], width=3),
    hovertemplate="Month %{x}<br>Balance: ₹%{y:,.0f}<extra>Discrete</extra>"
))

# Continuous model (approximation)
fig_cf.add_trace(go.Scatter(
    x=months_range,
    y=cc_balances,
    mode='lines',
    name='Continuous Approximation',
    line=dict(color=COLORS['negative'], width=3, dash='dash'),
    hovertemplate="Month %{x}<br>Balance: ₹%{y:,.0f}<extra>Continuous</extra>"
))

# Annotate final values (positioned to avoid line overlap)
fig_cf.add_annotation(
    x=n_months, y=discrete_balances[-1],
    text=f"Discrete: ₹{discrete_balances[-1]:,.0f}",
    showarrow=True, arrowhead=2, ax=-70, ay=-50,
    font=dict(color=COLORS['positive'], size=11, weight='bold'),
    bgcolor='rgba(255,255,255,0.9)',
    bordercolor=COLORS['positive'],
    borderwidth=2,
    borderpad=4
)

fig_cf.add_annotation(
    x=n_months, y=cc_balances[-1],
    text=f"Continuous: ₹{cc_balances[-1]:,.0f}",
    showarrow=True, arrowhead=2, ax=-70, ay=50,
    font=dict(color=COLORS['negative'], size=11, weight='bold'),
    bgcolor='rgba(255,255,255,0.9)',
    bordercolor=COLORS['negative'],
    borderwidth=2,
    borderpad=4
)

fig_cf.update_layout(
    title=dict(
        text="<b>Cash Flow Mismatch: Continuous vs. Discrete Compounding</b><br><sup>₹1,000 monthly deposits at 6% annual rate over 10 years</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis_title="Month",
    yaxis_title="Account Balance (₹)",
    height=450,
    margin=dict(l=70, r=40, t=80, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5),
    hovermode='x unified'
)

fig_cf.update_yaxes(tickformat=",", tickprefix="₹", showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_cf.update_xaxes(showgrid=True, gridcolor='rgba(0,0,0,0.05)')

fig_cf.show()
```

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Year-by-Year Divergence Table <em>(click to expand)</em></summary>

**Why the % difference is constant:** Both models are calibrated to the same **effective annual growth factor** ($1.06^t$), so the continuous cash‑flow approximation is a fixed proportion of the discrete monthly formula. That makes the **percentage gap constant**, while the **rupee gap grows** each year as balances rise.

```{python}
#| label: divergence-table
#| echo: true

# Create divergence summary table
div_data = {
    'Year': [f'Year {y}' for y in years],
    'Discrete Balance': [f"₹{b:,.0f}" for b in discrete_yearly],
    'CC Balance': [f"₹{b:,.0f}" for b in cc_yearly],
    'Difference (₹)': [f"₹{d:,.0f}" for d in divergence],
    'Difference (%)': [f"{p:.3f}%" for p in divergence_pct]
}

div_df = pd.DataFrame(div_data)

fig_div = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Period</b>', '<b>Discrete Model</b>', '<b>CC Model</b>', 
            '<b>Difference (₹)</b>', '<b>Difference (%)</b>'],
        fill_color=COLORS['dark'],
        font=dict(color='white', size=11),
        align='center',
        height=35
    ),
    cells=dict(
        values=[div_df[col] for col in div_df.columns],
        fill_color=[COLORS['light_bg'], 'rgba(39, 174, 96, 0.15)', 
                   'rgba(231, 76, 60, 0.15)', 'white', 'white'],
        font=dict(size=10),
        align='center',
        height=26
    )
)])

fig_div.update_layout(
    title=dict(text="<b>Year-by-Year Divergence Analysis</b>", x=0.5, font=dict(size=13)),
    height=420,
    margin=dict(l=20, r=20, t=50, b=20)
)

fig_div.show()
```

</details>

::: {.callout-warning}
## Quantified Limitation
Over 10 years of monthly deposits, the continuous compounding approximation diverges from the correct discrete model by approximately **`{python} f"₹{abs(final_diff):,.0f}"`** or **`{python} f"{abs(final_diff_pct):.2f}%"`** of the final balance.

While this percentage error appears to be small, the absolute discrepancy becomes material when applied to large principal amounts. For institutional investors managing billions, such approximation errors can translate to millions of dollars.
:::

</details>

### When Does This Limitation Matter?

| Scenario | CC Approximation Error | Recommendation |
|:---------|:----------------------:|:---------------|
| Long-term portfolio modeling | Low (~0.1-0.5%) | CC is acceptable |
| Bond pricing with coupons | Moderate (~0.5-2%) | Use discrete |
| Annuity/pension valuation | High (can exceed 5%) | Must use discrete |
| High-frequency trading | Very High | Use tick-by-tick |
| Derivative pricing (no dividends) | Negligible | CC is ideal |

---

## Portfolio-Level Implication: Time Additivity of Log Returns {#sec-q2-implication}

<div class="answer-card">

**✓ Required Element 4: Portfolio Implication**

**Implication: Under continuous compounding, log returns are additive across time, enabling straightforward multi-period analysis and risk aggregation.**

The mathematical foundation:

$$
\ln\left(\frac{V_T}{V_0}\right) = \sum_{t=1}^{T} \ln\left(\frac{V_t}{V_{t-1}}\right) = \sum_{t=1}^{T} g_t
$$

This means the **total log return equals the sum of periodic log returns**, a property that simple returns do not possess.

</div>

### Why Time Additivity Matters for Portfolios

**1. Multi-Period Performance Attribution**

Log returns allow clean decomposition of performance over time:
$$
r_{total}^{(log)} = r_{Q1}^{(log)} + r_{Q2}^{(log)} + r_{Q3}^{(log)} + r_{Q4}^{(log)}
$$

With discrete returns, the corresponding aggregation is: $1 + r_{total} = (1+r_{Q1})(1+r_{Q2})(1+r_{Q3})(1+r_{Q4})$

**2. Statistical Modeling**

Log returns are more likely to be approximately normally distributed (due to the Central Limit Theorem applied to products). This enables:

- **VaR/CVaR calculations** using analytical formulas
- **Portfolio optimization** with standard covariance matrices
- **Hypothesis testing** and confidence intervals

**3. Risk Aggregation**

For a portfolio with weights $w_i$ and asset log returns $g_i$:
$$
g_{portfolio} \approx \sum_i w_i \cdot g_i \quad \text{(for small returns)}
$$

### Verification Using the Given Data

This property was empirically demonstrated with Product A in Question 1:

```{python}
#| label: additivity-proof
#| echo: true

# Create verification display
proof_data = {
    'Calculation Method': [
        'Sum of 12 monthly log returns: Σg_t',
        'Direct calculation: ln(V₁₂/V₀)',
        'Numerical difference'
    ],
    'Product A Result': [
        f"{sum_log_returns_a:.10f}",
        f"{total_log_return_a:.10f}",
        f"{abs(sum_log_returns_a - total_log_return_a):.2e}"
    ]
}

proof_df = pd.DataFrame(proof_data)

fig_proof = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Calculation Method</b>', '<b>Product A Result</b>'],
        fill_color=COLORS['positive'],
        font=dict(color='white', size=12),
        align='left',
        height=40
    ),
    cells=dict(
        values=[proof_df[col] for col in proof_df.columns],
        fill_color=[COLORS['light_bg'], 'white'],
        font=dict(size=11),
        align='left',
        height=35
    )
)])

fig_proof.update_layout(
    title=dict(text="<b>Empirical Verification: Time Additivity of Log Returns</b>", x=0.5, font=dict(size=13)),
    height=200,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_proof.show()
```

### Waterfall: Building the Annual Return Month by Month

The following waterfall chart visualizes how monthly log returns stack up to form the total annual return, which is a property unique to log returns:

```{python}
#| label: additivity-waterfall
#| echo: false

import plotly.graph_objects as go

# Monthly log returns for Product A (from Q1)
monthly_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Total']

# Measures for waterfall: 'relative' for monthly, 'total' for the final
measures = ['relative'] * 12 + ['total']

# Values (monthly log returns, last value is placeholder - will be calculated)
log_returns_list = list(monthly_log_a)
values = log_returns_list + [0]  # Total is auto-calculated

# Colors based on positive/negative
colors = [COLORS['positive'] if v >= 0 else COLORS['negative'] for v in log_returns_list]
colors.append(COLORS['primary_blue'])  # Total bar color

fig_waterfall = go.Figure(go.Waterfall(
    name='Monthly Log Returns',
    orientation='v',
    measure=measures,
    x=monthly_labels,
    y=values,
    connector=dict(line=dict(color='rgba(0,0,0,0.3)', width=1)),
    increasing=dict(marker=dict(color=COLORS['positive'])),
    decreasing=dict(marker=dict(color=COLORS['negative'])),
    totals=dict(marker=dict(color=COLORS['primary_blue'])),
    text=[f"{v:.2%}" if m != 'total' else f"<b>{sum(log_returns_list):.2%}</b>" 
          for v, m in zip(values[:-1] + [sum(log_returns_list)], measures)],
    textposition='outside',
    textfont=dict(size=9),
    hovertemplate='<b>%{x}</b><br>Log Return: %{y:.4%}<extra></extra>'
))

fig_waterfall.update_layout(
    title=dict(
        text="<b>Time Additivity in Action: Monthly Log Returns Sum to Annual</b><br><sup>Product A: Each month's log return stacks perfectly to give the total</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis_title='Month',
    yaxis_title='Cumulative Log Return',
    height=450,
    margin=dict(l=60, r=40, t=80, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    showlegend=False
)

fig_waterfall.update_yaxes(tickformat='.1%', showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_waterfall.update_xaxes(showgrid=False)

fig_waterfall.show()
```

::: {.callout-important}
## The Power of Additivity
The sum matches to **14 decimal places** (machine precision). This exact additivity is one reason log returns are widely used in continuous-time modeling, portfolio analytics, and risk measurement.

Compare this to discrete returns, where:
$$
(1 + R_{total}) = \prod_{t=1}^{12}(1 + r_t) \neq 1 + \sum_{t=1}^{12} r_t
$$

The multiplicative structure of discrete returns makes multi-period analysis cumbersome and error-prone.
:::

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Cross-Sectional Aggregation Caveat <em>(click to expand)</em></summary>

While log returns are additive **across time**, they are **NOT additive across assets** at a single point in time. This is a common source of confusion.

**The Trap:**
$$
g_{portfolio} \neq w_A \cdot g_A + w_B \cdot g_B \quad \text{(in general)}
$$

**The Truth:**

For portfolio value $V_p = w_A V_A + w_B V_B$:
$$
g_{portfolio} = \ln\left(\frac{V_p(t)}{V_p(t-1)}\right) \neq w_A \ln\left(\frac{V_A(t)}{V_A(t-1)}\right) + w_B \ln\left(\frac{V_B(t)}{V_B(t-1)}\right)
$$

This is because $\ln(x + y) \neq \ln(x) + \ln(y)$.

**The Approximation:**

For small returns, the approximation $g_p \approx \sum_i w_i g_i$ is often used and works well when returns are small (< 5%). But for large returns or high precision work, proper aggregation is essential:

$$
g_p = \ln\left(\sum_i w_i \cdot e^{g_i}\right)
$$

```{python}
#| label: cross-sectional-demo
#| echo: true

# Demonstrate the cross-sectional aggregation issue
# Two assets with different returns
g_A = 0.10  # 10% log return
g_B = -0.05  # -5% log return
w_A, w_B = 0.6, 0.4

# Incorrect (simple weighted sum)
g_p_incorrect = w_A * g_A + w_B * g_B

# Correct (proper aggregation)
g_p_correct = np.log(w_A * np.exp(g_A) + w_B * np.exp(g_B))

# Show example
cross_data = {
    'Calculation': [
        'Asset A log return (g_A)',
        'Asset B log return (g_B)',
        'Portfolio weights',
        'Incorrect: w_A·g_A + w_B·g_B',
        'Correct: ln(w_A·e^g_A + w_B·e^g_B)',
        'Error from approximation'
    ],
    'Value': [
        f"{g_A:.2%}",
        f"{g_B:.2%}",
        f"w_A = {w_A}, w_B = {w_B}",
        f"{g_p_incorrect:.4%}",
        f"{g_p_correct:.4%}",
        f"{(g_p_incorrect - g_p_correct)*100:.3g}%"
    ]
}

cross_df = pd.DataFrame(cross_data)

fig_cross = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Calculation</b>', '<b>Value</b>'],
        fill_color=COLORS['purple'],
        font=dict(color='white', size=11),
        align='left',
        height=35
    ),
    cells=dict(
        values=[cross_df[col] for col in cross_df.columns],
        fill_color=['#faf8fc', 'white'],
        font=dict(size=10),
        align='left',
        height=28
    )
)])

fig_cross.update_layout(
    title=dict(text="<b>Cross-Sectional Aggregation: The Approximation Error</b>", x=0.5, font=dict(size=12)),
    height=280,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_cross.show()
```

For typical monthly returns (small values), the error is usually under 1 basis point. But for annual returns or volatile assets, the error can be significant.

</details>

---

## Summary: The Continuous Compounding Framework

| Aspect | Key Point |
|:-------|:----------|
| **Equation** | $V(t) = V_0 e^{rt}$ (the limiting case of discrete compounding) |
| **Numerical Insight** | 8% nominal yields 8.33% effective under CC (vs. 8.00% annual) |
| **Limitation** | Discrete cash flows cause model mismatch (small but persistent error over long horizons) |
| **Portfolio Implication** | Log returns enable time-additive multi-period analysis |

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Continuous-Time Model Assumptions <em>(click to expand)</em></summary>

::: {.callout-warning}
## Practical Note
$V(t) = V_0 e^{rt}$ assumes a smooth, continuous-time growth process. Real markets are discrete (prices move in steps; cash flows occur at specific times), so the continuous model is an approximation. For the month-to-month horizons in this homework, the approximation is usually very accurate, but it can matter more when cash flows are large or when extremely high precision is required.
:::

</details>

::: {.callout-tip}
## The Bottom Line
Continuous compounding is not a description of reality. No asset truly compounds continuously. Rather, it is a **mathematical idealization** that provides:

1. **Analytical tractability** for derivative pricing and risk modeling
2. **Time-additive returns** for clean multi-period analysis  
3. **Negligible approximation error** for most practical applications

Its limitations matter primarily when discrete cash flows are large relative to the investment horizon, or when extreme precision is required.
:::

---

<hr class="section-divider">

# Question: Portfolio Theory & Optimization [5 marks]

```{python}
#| label: q3-setup
#| include: false

# ============================================================
# QUESTION 3: Portfolio Theory Calculations
# ============================================================

import numpy as np
import pandas as pd
from scipy.optimize import minimize

# --- Given Parameters ---
E_A = 0.10      # Expected return Asset A: 10%
E_B = 0.16      # Expected return Asset B: 16%
sigma_A = 0.12  # Volatility Asset A: 12%
sigma_B = 0.20  # Volatility Asset B: 20%
rho_base = 0.30 # Base correlation: 0.3
rho_high = 0.90 # High correlation: 0.9

# No risk-free asset, no short-selling

# --- Portfolio Variance Formula ---
def portfolio_variance(w_A, sigma_A, sigma_B, rho):
    """
    Calculate portfolio variance for two-asset portfolio.
    w_A = weight in Asset A, w_B = 1 - w_A
    σ_p² = w_A²σ_A² + w_B²σ_B² + 2·w_A·w_B·σ_A·σ_B·ρ
    """
    w_B = 1 - w_A
    var = (w_A**2 * sigma_A**2 + 
           w_B**2 * sigma_B**2 + 
           2 * w_A * w_B * sigma_A * sigma_B * rho)
    return var

def portfolio_std(w_A, sigma_A, sigma_B, rho):
    """Calculate portfolio standard deviation."""
    return np.sqrt(portfolio_variance(w_A, sigma_A, sigma_B, rho))

def portfolio_return(w_A, E_A, E_B):
    """Calculate portfolio expected return."""
    return w_A * E_A + (1 - w_A) * E_B

# --- Minimum Variance Portfolio (MVP) ---
def mvp_weight_A(sigma_A, sigma_B, rho):
    """
    Analytical solution for MVP weight in Asset A.
    w_A* = (σ_B² - σ_A·σ_B·ρ) / (σ_A² + σ_B² - 2·σ_A·σ_B·ρ)
    """
    numerator = sigma_B**2 - sigma_A * sigma_B * rho
    denominator = sigma_A**2 + sigma_B**2 - 2 * sigma_A * sigma_B * rho
    w_A = numerator / denominator
    # Constrain to [0, 1] for no short-selling
    return max(0, min(1, w_A))

# Calculate MVP for both correlations
mvp_wA_base = mvp_weight_A(sigma_A, sigma_B, rho_base)
mvp_wA_high = mvp_weight_A(sigma_A, sigma_B, rho_high)

mvp_return_base = portfolio_return(mvp_wA_base, E_A, E_B)
mvp_std_base = portfolio_std(mvp_wA_base, sigma_A, sigma_B, rho_base)

mvp_return_high = portfolio_return(mvp_wA_high, E_A, E_B)
mvp_std_high = portfolio_std(mvp_wA_high, sigma_A, sigma_B, rho_high)

# --- Generate Efficient Frontier ---
def generate_frontier(sigma_A, sigma_B, E_A, E_B, rho, n_points=100):
    """Generate efficient frontier points."""
    weights_A = np.linspace(0, 1, n_points)
    returns = [portfolio_return(w, E_A, E_B) for w in weights_A]
    stds = [portfolio_std(w, sigma_A, sigma_B, rho) for w in weights_A]
    return weights_A, returns, stds

# Generate frontiers for both correlations
weights_base, returns_base, stds_base = generate_frontier(sigma_A, sigma_B, E_A, E_B, rho_base)
weights_high, returns_high, stds_high = generate_frontier(sigma_A, sigma_B, E_A, E_B, rho_high)

# --- Diversification Benefit Quantification ---
# At equal weights (50/50)
w_equal = 0.5
std_equal_base = portfolio_std(w_equal, sigma_A, sigma_B, rho_base)
std_equal_high = portfolio_std(w_equal, sigma_A, sigma_B, rho_high)

# Weighted average volatility (no diversification benefit)
std_weighted_avg = w_equal * sigma_A + (1 - w_equal) * sigma_B

# Diversification benefit = reduction from weighted average
div_benefit_base = (std_weighted_avg - std_equal_base) / std_weighted_avg
div_benefit_high = (std_weighted_avg - std_equal_high) / std_weighted_avg

# At MVP
div_benefit_mvp_base = (std_weighted_avg - mvp_std_base) / std_weighted_avg
div_benefit_mvp_high = (std_weighted_avg - mvp_std_high) / std_weighted_avg

# --- Perfect Correlation Frontier (ρ = 1) for reference ---
weights_perfect, returns_perfect, stds_perfect = generate_frontier(sigma_A, sigma_B, E_A, E_B, 1.0)

# --- Negative Correlation Frontier (ρ = -0.5) for reference ---
weights_neg, returns_neg, stds_neg = generate_frontier(sigma_A, sigma_B, E_A, E_B, -0.5)

# --- Bootstrap Simulation for Estimation Error (Q3c) ---
np.random.seed(2023014)

# True parameters
true_mu = np.array([E_A, E_B])
true_cov = np.array([
    [sigma_A**2, sigma_A * sigma_B * rho_base],
    [sigma_A * sigma_B * rho_base, sigma_B**2]
])

def optimal_portfolio_weights(mu, cov, allow_short=False):
    """
    Find the minimum-variance portfolio (MVP).
    With no risk-free asset and no short-selling, the objective is to minimize portfolio volatility.
    """
    n = len(mu)
    
    def neg_sharpe(w):
        ret = np.dot(w, mu)
        var = np.dot(w, np.dot(cov, w))
        # Maximize return/std (simplified, no rf)
        return -ret / np.sqrt(var) if var > 0 else 0
    
    def portfolio_vol(w):
        return np.sqrt(np.dot(w, np.dot(cov, w)))
    
    constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]
    
    if not allow_short:
        bounds = [(0, 1) for _ in range(n)]
    else:
        bounds = None
    
    # Find MVP
    result = minimize(portfolio_vol, x0=[0.5, 0.5], method='SLSQP',
                     bounds=bounds, constraints=constraints)
    
    return result.x if result.success else np.array([0.5, 0.5])

# Run bootstrap simulation
n_bootstrap = 1000
sample_size = 60  # 5 years of monthly data
estimated_weights_A = []

for i in range(n_bootstrap):
    # Generate sample returns from true distribution
    sample_returns = np.random.multivariate_normal(true_mu/12, true_cov/12, size=sample_size)
    
    # Estimate parameters from sample
    est_mu = np.mean(sample_returns, axis=0) * 12  # Annualize
    est_cov = np.cov(sample_returns.T) * 12  # Annualize
    
    # Find optimal portfolio with estimated parameters
    opt_w = optimal_portfolio_weights(est_mu, est_cov, allow_short=False)
    estimated_weights_A.append(opt_w[0])

estimated_weights_A = np.array(estimated_weights_A)

# True optimal weight
true_opt_w = optimal_portfolio_weights(true_mu, true_cov, allow_short=False)
true_w_A = true_opt_w[0]

# Bootstrap statistics
bootstrap_mean_wA = np.mean(estimated_weights_A)
bootstrap_std_wA = np.std(estimated_weights_A)
bootstrap_min_wA = np.min(estimated_weights_A)
bootstrap_max_wA = np.max(estimated_weights_A)

# 1/N portfolio comparison
equal_weight = 0.5

# --- Pre-formatted strings ---
mvp_wA_base_pct = f"{mvp_wA_base:.1%}"
mvp_wB_base_pct = f"{1-mvp_wA_base:.1%}"
mvp_wA_high_pct = f"{mvp_wA_high:.1%}"
mvp_wB_high_pct = f"{1-mvp_wA_high:.1%}"

mvp_ret_base_pct = f"{mvp_return_base:.2%}"
mvp_std_base_pct = f"{mvp_std_base:.2%}"
mvp_ret_high_pct = f"{mvp_return_high:.2%}"
mvp_std_high_pct = f"{mvp_std_high:.2%}"

div_benefit_base_pct = f"{div_benefit_base:.1%}"
div_benefit_high_pct = f"{div_benefit_high:.1%}"

true_wA_pct = f"{true_w_A:.1%}"
bootstrap_mean_pct = f"{bootstrap_mean_wA:.1%}"
bootstrap_std_pct = f"{bootstrap_std_wA:.1%}"
```

::: {.callout-important}
## Key Assumptions for Question 3
**Before proceeding, lets explicitly state the assumptions governing this analysis:**

1. **No Risk-Free Asset:** There is no opportunity to borrow or lend at a risk-free rate. All capital must be allocated between risky assets.
2. **No Short-Selling:** Portfolio weights are constrained to $w_A, w_B \geq 0$ (investors cannot take negative positions).
3. **Two-Asset Universe:** The investment opportunity set consists only of Asset A and Asset B.
4. **Returns are Normally Distributed:** Mean-variance optimization is appropriate (implicit assumption).
5. **Parameters are Known:** For parts (a) and (b), we treat $\mu$, $\sigma$, and $\rho$ as given. Part (c) relaxes this assumption.
:::

<div class="question-summary">

### Question 3: Answer Summary Card

**Given:** Two risky assets with $\mathbb{E}[r_A] = 10\%$, $\mathbb{E}[r_B] = 16\%$, $\sigma_A = 12\%$, $\sigma_B = 20\%$, $\rho_{AB} = 0.3$  
**Constraints:** No risk-free asset, no short-selling allowed

| Sub-part | Question | Answer |
|:---------|:---------|:-------|
| **(a)** | Effect of ρ: 0.3 → 0.9 on diversification & MVP | [See Section 3.1](#sec-q3a) |
| **(b)** | Implication for risk-averse investor | [See Section 3.2](#sec-q3b) |
| **(c)** | Limitation from estimation error | [See Section 3.3](#sec-q3c) |

</div>

---

## Effect of Increased Correlation [Parts a] {#sec-q3a}

### The Setup: Two-Asset Portfolio

For a portfolio with weight $w_A$ in Asset A and $w_B = 1 - w_A$ in Asset B:

$$
\mathbb{E}[r_p] = w_A \cdot \mathbb{E}[r_A] + w_B \cdot \mathbb{E}[r_B]
$$

$$
\sigma_p = \sqrt{w_A^2 \sigma_A^2 + w_B^2 \sigma_B^2 + 2 w_A w_B \sigma_A \sigma_B \rho_{AB}}
$$

The **correlation coefficient** $\rho_{AB}$ directly affects portfolio risk but not expected return.

<div class="answer-card">

#### ✓ Answer 3(a): Effect of Correlation Increase (0.3 → 0.9)

**Part (i): Effect on Diversification Benefits**

When correlation increases from 0.3 to 0.9, **diversification benefits decrease substantially**:

- At $\rho = 0.3$: Assets move somewhat independently, allowing significant risk reduction through combination
- At $\rho = 0.9$: Assets move almost in lockstep, limiting the ability to reduce risk through diversification

**Quantified Impact (at 50/50 allocation):**

| Correlation | Portfolio Volatility | Diversification Benefit |
|:-----------:|:--------------------:|:-----------------------:|
| ρ = 0.3 | `{python} f"{std_equal_base:.2%}"` | `{python} div_benefit_base_pct` risk reduction |
| ρ = 0.9 | `{python} f"{std_equal_high:.2%}"` | `{python} div_benefit_high_pct` risk reduction |

The diversification benefit drops from `{python} div_benefit_base_pct` to `{python} div_benefit_high_pct`, a **`{python} f"{(div_benefit_base - div_benefit_high)/div_benefit_base:.0%}"`** reduction in diversification effectiveness.

---

**Part (ii): Effect on Minimum Variance Portfolio Location**

The MVP shifts toward the **lower-volatility asset (Asset A)** as correlation increases:

| Correlation | MVP Weight in A | MVP Weight in B | MVP Volatility |
|:-----------:|:---------------:|:---------------:|:--------------:|
| ρ = 0.3 | `{python} mvp_wA_base_pct` | `{python} mvp_wB_base_pct` | `{python} mvp_std_base_pct` |
| ρ = 0.9 | `{python} mvp_wA_high_pct` | `{python} mvp_wB_high_pct` | `{python} mvp_std_high_pct` |

**Intuition:** When assets are highly correlated, combining them provides little risk reduction. The optimal strategy to minimize variance shifts toward simply holding more of the lower-volatility asset.

</div>

### The Efficient Frontier (Base Case: ρ = 0.3)

Before exploring how correlation affects the frontier, let's first visualize the **base case** from the problem statement:

```{python}
#| label: efficient-frontier-static-base
#| echo: true

# ============================================================
# STATIC EFFICIENT FRONTIER: Base Case (ρ = 0.3)
# ============================================================
# Print-friendly standalone chart showing the base scenario

rho_base = 0.3

# Generate frontier for base case
weights_base, returns_base, stds_base = generate_frontier(sigma_A, sigma_B, E_A, E_B, rho_base)

# Calculate MVP for base case
mvp_w_base = mvp_weight_A(sigma_A, sigma_B, rho_base)
mvp_ret_base = portfolio_return(mvp_w_base, E_A, E_B)
mvp_std_base = portfolio_std(mvp_w_base, sigma_A, sigma_B, rho_base)

# Individual asset positions
asset_a_pos = (sigma_A * 100, E_A * 100)
asset_b_pos = (sigma_B * 100, E_B * 100)

fig_frontier_static = go.Figure()

# Add efficient frontier line
fig_frontier_static.add_trace(go.Scatter(
    x=[s * 100 for s in stds_base],
    y=[r * 100 for r in returns_base],
    mode='lines',
    name='Efficient Frontier',
    line=dict(color=COLORS['primary_blue'], width=5),
    hovertemplate="<b>Efficient Frontier</b><br>σ = %{x:.2f}%<br>E[r] = %{y:.2f}%<extra></extra>"
))

# Add MVP point with star marker
fig_frontier_static.add_trace(go.Scatter(
    x=[mvp_std_base * 100],
    y=[mvp_ret_base * 100],
    mode='markers+text',
    name='Minimum Variance Portfolio',
    marker=dict(color='#F39C12', size=20, symbol='star', line=dict(color='white', width=2)),
    text=['MVP'],
    textposition='top center',
    textfont=dict(size=12, color='#F39C12', family='Segoe UI', weight='bold'),
    hovertemplate="<b>MVP</b><br>σ = %{x:.2f}%<br>E[r] = %{y:.2f}%<br>" +
                  f"Weight A: {mvp_w_base:.1%}<br>Weight B: {(1-mvp_w_base):.1%}<extra></extra>"
))

# Add Asset A point
fig_frontier_static.add_trace(go.Scatter(
    x=[asset_a_pos[0]],
    y=[asset_a_pos[1]],
    mode='markers+text',
    name='Asset A',
    marker=dict(color=COLORS['positive'], size=16, symbol='circle', line=dict(color='white', width=2)),
    text=['Asset A'],
    textposition='bottom center',
    textfont=dict(size=11, color=COLORS['positive'], weight='bold'),
    hovertemplate="<b>Asset A</b><br>σ = %{x:.2f}%<br>E[r] = %{y:.2f}%<extra></extra>"
))

# Add Asset B point
fig_frontier_static.add_trace(go.Scatter(
    x=[asset_b_pos[0]],
    y=[asset_b_pos[1]],
    mode='markers+text',
    name='Asset B',
    marker=dict(color=COLORS['negative'], size=16, symbol='square', line=dict(color='white', width=2)),
    text=['Asset B'],
    textposition='top center',
    textfont=dict(size=11, color=COLORS['negative'], weight='bold'),
    hovertemplate="<b>Asset B</b><br>σ = %{x:.2f}%<br>E[r] = %{y:.2f}%<extra></extra>"
))

# Add annotation for efficient portion
fig_frontier_static.add_annotation(
    x=mvp_std_base * 100 + 2,
    y=mvp_ret_base * 100 + 1.5,
    text="<b>Efficient Portion</b><br>(above MVP)",
    showarrow=True,
    arrowhead=2,
    ax=60, ay=-40,
    font=dict(size=10, color=COLORS['dark']),
    bgcolor='rgba(255,255,255,0.9)',
    bordercolor=COLORS['primary_blue'],
    borderwidth=1
)

# Add shaded region for inefficient area (below MVP)
fig_frontier_static.add_shape(
    type="rect",
    x0=0, x1=25,
    y0=0, y1=mvp_ret_base * 100,
    fillcolor="rgba(231, 76, 60, 0.08)",
    line=dict(width=0),
    layer="below"
)

fig_frontier_static.update_layout(
    title=dict(
        text=f"<b>Efficient Frontier: Base Case (ρ = {rho_base})</b><br><sup>Two-asset portfolio with Asset A (E[r]=10%, σ=12%) and Asset B (E[r]=16%, σ=20%)</sup>",
        x=0.5,
        font=dict(size=15, color=COLORS['dark'])
    ),
    xaxis_title="Portfolio Risk (σ, %)",
    yaxis_title="Expected Return (E[r], %)",
    height=550,
    margin=dict(l=70, r=40, t=100, b=70),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(
        orientation="v",
        yanchor="top",
        y=0.98,
        xanchor="right",
        x=0.98,
        bgcolor='rgba(255,255,255,0.9)',
        bordercolor=COLORS['neutral'],
        borderwidth=1
    ),
    hovermode='closest'
)

fig_frontier_static.update_xaxes(
    showgrid=True, 
    gridwidth=1, 
    gridcolor='rgba(0,0,0,0.1)',
    range=[10, 21]
)
fig_frontier_static.update_yaxes(
    showgrid=True, 
    gridwidth=1, 
    gridcolor='rgba(0,0,0,0.1)',
    range=[9, 17]
)

fig_frontier_static.show()
```

### How Diversification Benefit Decays with Correlation

The following chart visualizes the core insight of Question 3(a): as correlation increases, the diversification benefit shrinks:

```{python}
#| label: diversification-decay-chart
#| echo: false

import plotly.graph_objects as go
import numpy as np

# Calculate diversification benefit across correlation spectrum
correlations = np.linspace(-1, 1, 21)
div_benefits = []
mvp_volatilities = []
mvp_weights_A = []

for rho in correlations:
    # Calculate MVP for this correlation
    mvp_w = mvp_weight_A(sigma_A, sigma_B, rho)
    mvp_w = np.clip(mvp_w, 0, 1)  # No short selling
    mvp_vol = portfolio_std(mvp_w, sigma_A, sigma_B, rho)
    
    # Weighted average volatility (no diversification benchmark)
    weighted_avg_vol = mvp_w * sigma_A + (1 - mvp_w) * sigma_B
    
    # Diversification benefit = reduction from weighted average
    div_ben = (weighted_avg_vol - mvp_vol) / weighted_avg_vol * 100
    
    div_benefits.append(div_ben)
    mvp_volatilities.append(mvp_vol * 100)
    mvp_weights_A.append(mvp_w * 100)

fig_decay = make_subplots(
    rows=1, cols=2,
    subplot_titles=('<b>Diversification Benefit (%)</b>', '<b>MVP Volatility (%)</b>'),
    horizontal_spacing=0.12
)

# Diversification benefit curve
fig_decay.add_trace(go.Scatter(
    x=correlations,
    y=div_benefits,
    mode='lines+markers',
    line=dict(color=COLORS['positive'], width=3),
    marker=dict(size=6),
    name='Diversification Benefit',
    fill='tozeroy',
    fillcolor='rgba(39, 174, 96, 0.2)',
    hovertemplate='ρ = %{x:.2f}<br>Benefit: %{y:.1f}%<extra></extra>'
), row=1, col=1)

# Mark key correlation values
for rho_mark, label in [(0.3, 'ρ=0.3 (base)'), (0.9, 'ρ=0.9 (high)')]:
    idx = int((rho_mark + 1) / 2 * 20)
    fig_decay.add_trace(go.Scatter(
        x=[correlations[idx]],
        y=[div_benefits[idx]],
        mode='markers+text',
        marker=dict(size=14, color=COLORS['warning'], symbol='star', line=dict(color='white', width=2)),
        text=[label],
        textposition='top center',
        textfont=dict(size=9),
        name=label,
        showlegend=False,
        hovertemplate=f'{label}<br>Benefit: {div_benefits[idx]:.1f}%<extra></extra>'
    ), row=1, col=1)

# MVP Volatility curve
fig_decay.add_trace(go.Scatter(
    x=correlations,
    y=mvp_volatilities,
    mode='lines+markers',
    line=dict(color=COLORS['negative'], width=3),
    marker=dict(size=6),
    name='MVP Volatility',
    fill='tozeroy',
    fillcolor='rgba(231, 76, 60, 0.2)',
    hovertemplate='ρ = %{x:.2f}<br>MVP σ: %{y:.2f}%<extra></extra>'
), row=1, col=2)

# Add horizontal lines for individual asset volatilities
fig_decay.add_hline(y=sigma_A * 100, line_dash='dot', line_color=COLORS['positive'],
                    annotation_text=f'σ_A = {sigma_A:.0%}', annotation_position='right',
                    row=1, col=2)
fig_decay.add_hline(y=sigma_B * 100, line_dash='dot', line_color=COLORS['warning'],
                    annotation_text=f'σ_B = {sigma_B:.0%}', annotation_position='right',
                    row=1, col=2)

fig_decay.update_layout(
    title=dict(
        text="<b>The Decay of Diversification: Correlation's Impact</b><br><sup>As ρ increases from -1 to +1, diversification benefits shrink and MVP volatility rises</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    height=400,
    margin=dict(l=60, r=60, t=100, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    showlegend=False
)

fig_decay.update_xaxes(title_text='Correlation (ρ)', showgrid=True, gridcolor='rgba(0,0,0,0.1)', row=1, col=1)
fig_decay.update_xaxes(title_text='Correlation (ρ)', showgrid=True, gridcolor='rgba(0,0,0,0.1)', row=1, col=2)
fig_decay.update_yaxes(title_text='Risk Reduction (%)', showgrid=True, gridcolor='rgba(0,0,0,0.1)', row=1, col=1)
fig_decay.update_yaxes(title_text='Portfolio σ (%)', showgrid=True, gridcolor='rgba(0,0,0,0.1)', range=[0, 22], row=1, col=2)

fig_decay.show()
```

::: {.callout-important}
## Key Insight: Correlation Reduces Diversification Benefits
- At **ρ = -1** (perfect negative correlation): Diversification benefit is **maximized**; risk can theoretically be eliminated entirely
- At **ρ = 0** (uncorrelated): Substantial benefit remains (~40%+ risk reduction possible)
- At **ρ = +1** (perfect positive correlation): **Zero diversification benefit**; no portfolio can beat holding the lower-volatility asset alone

The problem asks about ρ = 0.3 → 0.9: This represents a move from "moderate diversification" to "almost no diversification."
:::

::: {.callout-tip}
## Key Observations from Base Case

1. **MVP Location**: At ρ=0.3, the MVP requires approximately **82.6%** in Asset A, achieving σ ≈ **10.2%** and E[r] ≈ **11.0%**
2. **Diversification Benefit**: The MVP risk (10.2%) is **substantially lower** than either individual asset (12% and 20%)
3. **Efficient vs Inefficient**: Portfolios below the MVP (shaded red region) are **dominated**: higher risk for the same return
4. **Individual Assets**: Asset A and Asset B are the **endpoints** of the efficient frontier (feasible boundary). Blended portfolios between them can offer better risk-return tradeoffs, but the endpoints remain part of the frontier.
:::

---

### Efficient Frontier Visualization: Interactive Correlation Explorer

The visualization below shows how the efficient frontier changes as correlation varies. The **slider** parameterizes correlation from -1 to +1, illustrating how the frontier narrows as assets become more correlated.

```{python}
#| label: efficient-frontier-interactive
#| echo: true

# ============================================================
# INTERACTIVE EFFICIENT FRONTIER WITH CORRELATION SLIDER
# ============================================================

# Generate frontiers for continuous correlation range
corr_values = np.linspace(-1.0, 1.0, 21)  # 21 values from -1 to 1 (step of 0.1)

# Helper function to interpolate colors from green (low corr) to red (high corr)
def get_color_for_correlation(rho):
    """Green for negative correlation, yellow for 0, red for positive"""
    # Normalize rho from [-1, 1] to [0, 1]
    normalized = (rho + 1) / 2
    if normalized < 0.5:
        # Green to Yellow: interpolate from (39, 174, 96) to (241, 196, 15)
        t = normalized * 2  # 0 to 1
        r = int(39 + (241 - 39) * t)
        g = int(174 + (196 - 174) * t)
        b = int(96 - (96 - 15) * t)
    else:
        # Yellow to Red: interpolate from (241, 196, 15) to (231, 76, 60)
        t = (normalized - 0.5) * 2  # 0 to 1
        r = int(241 - (241 - 231) * t)
        g = int(196 - (196 - 76) * t)
        b = int(15 + (60 - 15) * t)
    return f'rgb({r},{g},{b})'

# Pre-compute all frontiers
all_frontiers = {}
all_mvps = {}
for rho in corr_values:
    weights, returns, stds = generate_frontier(sigma_A, sigma_B, E_A, E_B, rho)
    all_frontiers[rho] = (weights, returns, stds)
    mvp_w = mvp_weight_A(sigma_A, sigma_B, rho)
    mvp_ret = portfolio_return(mvp_w, E_A, E_B)
    mvp_std_val = portfolio_std(mvp_w, sigma_A, sigma_B, rho)
    all_mvps[rho] = (mvp_w, mvp_ret, mvp_std_val)

# Create figure with slider
fig_frontier = go.Figure()

# Add all frontier traces (visibility controlled by slider)
for i, rho in enumerate(corr_values):
    weights, returns, stds = all_frontiers[rho]
    # Get color based on correlation
    color_hex = get_color_for_correlation(rho)
    
    visible = (abs(rho - 0.3) < 0.01)  # Default to base case (rho = 0.3)
    fig_frontier.add_trace(go.Scatter(
        x=[s * 100 for s in stds],
        y=[r * 100 for r in returns],
        mode='lines',
        name=f'ρ = {rho:.2f}',
        line=dict(color=color_hex, width=4),
        visible=visible,
        hovertemplate=f"<b>ρ = {rho:.2f}</b><br>σ = %{{x:.2f}}%<br>E[r] = %{{y:.2f}}%<extra></extra>"
    ))

# Add MVP markers for each correlation
for i, rho in enumerate(corr_values):
    mvp_w, mvp_ret, mvp_std_val = all_mvps[rho]
    mvp_w_B = 1 - mvp_w
    color_hex = get_color_for_correlation(rho)
    
    visible = (abs(rho - 0.3) < 0.01)
    fig_frontier.add_trace(go.Scatter(
        x=[mvp_std_val * 100],
        y=[mvp_ret * 100],
        mode='markers',
        name=f'MVP (ρ={rho:.2f})',
        marker=dict(color=color_hex, size=14, symbol='diamond', 
                   line=dict(color='white', width=2)),
        visible=visible,
        hovertemplate=f"<b>MVP at ρ={rho:.2f}</b><br>w_A = {mvp_w:.1%} | w_B = {mvp_w_B:.1%}<br>σ = %{{x:.2f}}%<br>E[r] = %{{y:.2f}}%<extra></extra>"
    ))

# Add Asset A marker (always visible)
fig_frontier.add_trace(go.Scatter(
    x=[sigma_A * 100],
    y=[E_A * 100],
    mode='markers+text',
    name='Asset A',
    marker=dict(color=COLORS['positive'], size=18, symbol='circle',
               line=dict(color='white', width=2)),
    text=['A'],
    textposition='bottom left',
    textfont=dict(size=16, weight='bold', color=COLORS['positive']),
    hovertemplate="<b>Asset A</b><br>σ = 12%<br>E[r] = 10%<extra></extra>"
))

# Add Asset B marker (always visible)
fig_frontier.add_trace(go.Scatter(
    x=[sigma_B * 100],
    y=[E_B * 100],
    mode='markers+text',
    name='Asset B',
    marker=dict(color=COLORS['warning'], size=18, symbol='circle',
               line=dict(color='white', width=2)),
    text=['B'],
    textposition='top right',
    textfont=dict(size=16, weight='bold', color=COLORS['warning']),
    hovertemplate="<b>Asset B</b><br>σ = 20%<br>E[r] = 16%<extra></extra>"
))

# Create slider steps
steps = []
for i, rho in enumerate(corr_values):
    # Each correlation has 2 traces (frontier + MVP), plus 2 asset markers at end
    n_corr = len(corr_values)
    visibility = [False] * (n_corr * 2) + [True, True]  # Assets always visible
    visibility[i] = True  # Show this frontier
    visibility[n_corr + i] = True  # Show this MVP
    
    mvp_w, mvp_ret, mvp_std_val = all_mvps[rho]
    mvp_w_B = 1 - mvp_w  # Calculate weight in Asset B
    # Calculate diversification benefit at 50/50 portfolio
    port_50_50_std = portfolio_std(0.5, sigma_A, sigma_B, rho)
    std_weighted_avg_local = 0.5 * sigma_A + 0.5 * sigma_B
    div_benefit = (std_weighted_avg_local - port_50_50_std) / std_weighted_avg_local * 100
    
    # Special handling for perfect negative correlation (near-zero volatility)
    if mvp_std_val < 0.001:
        mvp_risk_display = "σ ≈ 0%"
    else:
        mvp_risk_display = f"σ = {mvp_std_val:.2%}"
    
    steps.append(dict(
        method="update",
        args=[{"visible": visibility},
              {"title": f"<b>Efficient Frontier at ρ = {rho:.2f}</b><br><sup>MVP: w_A = {mvp_w:.1%} | w_B = {mvp_w_B:.1%} | {mvp_risk_display} | E[r] = {mvp_ret:.2%} | Diversification Benefit (50/50): {div_benefit:.1f}%</sup>"}],
        label=f"{rho:.1f}"
    ))

# Find index for rho = 0.3 (default)
default_idx = np.argmin(np.abs(corr_values - 0.3))
mvp_w_default, mvp_ret_default, mvp_std_default = all_mvps[corr_values[default_idx]]
mvp_w_B_default = 1 - mvp_w_default
port_50_50_std_default = portfolio_std(0.5, sigma_A, sigma_B, 0.3)
div_benefit_default = (std_weighted_avg - port_50_50_std_default) / std_weighted_avg * 100

fig_frontier.update_layout(
    title=dict(
        text=f"<b>Efficient Frontier at ρ = 0.30</b><br><sup>MVP: w_A = {mvp_w_default:.1%} | w_B = {mvp_w_B_default:.1%} | σ = {mvp_std_default:.2%} | E[r] = {mvp_ret_default:.2%} | Diversification Benefit (50/50): {div_benefit_default:.1f}%</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    sliders=[dict(
        active=default_idx,
        yanchor="top",
        y=-0.08,
        xanchor="left",
        currentvalue=dict(
            prefix="<b>Correlation ρ = </b>",
            visible=True,
            xanchor="center",
            font=dict(size=15, color=COLORS['dark'], family='Arial Black')
        ),
        pad=dict(b=10, t=60),
        len=0.88,
        x=0.06,
        steps=steps,
        bgcolor='rgba(255,255,255,0.95)',
        bordercolor=COLORS['dark'],
        borderwidth=2,
        tickcolor=COLORS['dark'],
        ticklen=8,
        tickwidth=2
    )],
    xaxis_title="<b>Portfolio Risk (σ) %</b>",
    yaxis_title="<b>Expected Return E[r] %</b>",
    height=600,
    margin=dict(l=70, r=50, t=120, b=120),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(
        orientation="h",
        yanchor="bottom",
        y=-0.28,
        xanchor="center",
        x=0.5,
        bgcolor='rgba(255,255,255,0.95)',
        bordercolor=COLORS['dark'],
        borderwidth=1,
        font=dict(size=11)
    ),
    hovermode='closest'
)

# Expand x-axis range to accommodate negative correlations (frontier can go very low)
fig_frontier.update_xaxes(range=[0, 22], showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_frontier.update_yaxes(range=[9, 17], showgrid=True, gridcolor='rgba(0,0,0,0.1)')

fig_frontier.show()
```

::: {.callout-tip}
## Interactive Exploration
The slider (bottom) varies correlation continuously from ρ = -1.0 to ρ = 1.0. The plot illustrates how:

- **At ρ = -1.0 to -0.5:** The frontier bows dramatically leftward, achieving very low risk through diversification
- **At ρ = 0.3 (base):** Moderate curvature provides meaningful risk reduction  
- **At ρ = 0.9:** The frontier is nearly linear; diversification benefits almost vanish
- **At ρ = 1.0:** Perfect straight line; zero diversification benefit, and the portfolio is just a weighted average
:::

### Short-Selling: The Unconstrained Frontier (Revamped)

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">How Shorting Changes the Efficient Frontier <em>(click to expand)</em></summary>

Short-selling removes the $0\le w \le 1$ constraint and allows **negative weights** (short positions) and **leverage** (weights above 100%). This section shows, visually and quantitatively, how that expands the efficient frontier and shifts the **Global MVP** (the minimum-variance portfolio over all real weights).

**How to read the chart below**

- **Solid thick line** = long-only frontier (used in Question 3)
- **Red dashed line** = short-only region (weights outside $[0,1]$)
- **Grey dotted line** = full unconstrained frontier (for shape context)
- **Gold star marker** = **Global MVP** (best possible risk) with shorting allowed
- **◆ Diamond** = Long-only MVP (best possible risk without shorting)

```{python}
#| label: efficient-frontier-short-selling
#| echo: true

# ============================================================
# SHORT-SELLING FRONTIER (CLEAR DIFFERENCE VISUAL)
# ============================================================

def frontier_data(sigma_A, sigma_B, E_A, E_B, rho, w_min=-1.0, w_max=2.0, n=200):
    weights_A = np.linspace(w_min, w_max, n)
    out = []
    for w_A in weights_A:
        w_B = 1 - w_A
        port_ret = w_A * E_A + w_B * E_B
        port_var = (w_A**2 * sigma_A**2 + w_B**2 * sigma_B**2 +
                    2 * w_A * w_B * sigma_A * sigma_B * rho)
        port_std = np.sqrt(max(port_var, 0))
        out.append((w_A, w_B, port_ret, port_std))
    return out

def mvp_weight_A_unconstrained(sigma_A, sigma_B, rho):
    """Unconstrained MVP weight in Asset A (can be <0 or >1)."""
    numerator = sigma_B**2 - sigma_A * sigma_B * rho
    denominator = sigma_A**2 + sigma_B**2 - 2 * sigma_A * sigma_B * rho
    return numerator / denominator

def stats_for_weight(w_A, rho):
    w_B = 1 - w_A
    port_ret = w_A * E_A + w_B * E_B
    port_std = portfolio_std(w_A, sigma_A, sigma_B, rho)
    return w_B, port_ret, port_std

def segment_xy(data, condition):
    xs, ys = [], []
    for w_A, w_B, ret, std in data:
        if condition(w_A):
            xs.append(std * 100)
            ys.append(ret * 100)
        else:
            if xs and xs[-1] is not None:
                xs.append(None)
                ys.append(None)
    return xs, ys

corr_values_short = np.linspace(-1.0, 1.0, 21)

all_ext = {}
all_lo = {}
all_mvp_global = {}
all_mvp_long = {}

for rho in corr_values_short:
    ext = frontier_data(sigma_A, sigma_B, E_A, E_B, rho, w_min=-1.0, w_max=2.0, n=200)
    lo = frontier_data(sigma_A, sigma_B, E_A, E_B, rho, w_min=0.0, w_max=1.0, n=120)
    all_ext[rho] = ext
    all_lo[rho] = lo

    # Global MVP (unconstrained)
    wA_star = mvp_weight_A_unconstrained(sigma_A, sigma_B, rho)
    wB_star, ret_star, std_star = stats_for_weight(wA_star, rho)
    all_mvp_global[rho] = (wA_star, wB_star, ret_star, std_star)

    # Long-only MVP
    wA_clipped = np.clip(wA_star, 0, 1)
    wB_clip, ret_clip, std_clip = stats_for_weight(wA_clipped, rho)
    all_mvp_long[rho] = (wA_clipped, wB_clip, ret_clip, std_clip)

fig_short = go.Figure()

trace_per_rho = 5
default_rho = 0.9
default_idx = int(np.argmin(np.abs(corr_values_short - default_rho)))
default_rho = corr_values_short[default_idx]

for i, rho in enumerate(corr_values_short):
    ext = all_ext[rho]
    lo = all_lo[rho]
    visible = (i == default_idx)

    # 1) Full unconstrained frontier (shape context)
    fig_short.add_trace(go.Scatter(
        x=[d[3] * 100 for d in ext],
        y=[d[2] * 100 for d in ext],
        mode='lines',
        line=dict(color='rgba(100,100,100,0.4)', width=2, dash='dot'),
        name='Unconstrained Frontier',
        visible=visible,
        showlegend=False
    ))

    # 2) Long-only frontier (solid thick)
    fig_short.add_trace(go.Scatter(
        x=[d[3] * 100 for d in lo],
        y=[d[2] * 100 for d in lo],
        mode='lines',
        line=dict(color=COLORS['primary_blue'], width=5),
        name='Long-Only Frontier',
        visible=visible,
        showlegend=False
    ))

    # 3) Short-only segments (red dashed)
    xs_short, ys_short = segment_xy(ext, lambda w: (w < 0) or (w > 1))
    fig_short.add_trace(go.Scatter(
        x=xs_short,
        y=ys_short,
        mode='lines',
        line=dict(color='#E74C3C', width=4, dash='dash'),
        name='Shorting Region',
        visible=visible,
        showlegend=False
    ))

    # 4) Global MVP (star)
    wA_star, wB_star, ret_star, std_star = all_mvp_global[rho]
    requires_short = (wA_star < 0) or (wA_star > 1)
    fig_short.add_trace(go.Scatter(
        x=[std_star * 100],
        y=[ret_star * 100],
        mode='markers',
        marker=dict(
            color='#F1C40F' if not requires_short else '#E74C3C',
            size=22,
            symbol='star',
            line=dict(color='black', width=2)
        ),
        name='Global MVP (Best Possible)',
        visible=visible,
        showlegend=False,
        hovertemplate=f"<b>Global MVP</b><br>ρ = {rho:.2f}<br>w_A = {wA_star:.1%} | w_B = {wB_star:.1%}<br>σ = %{{x:.2f}}%<br>E[r] = %{{y:.2f}}%<extra></extra>"
    ))

    # 5) Long-only MVP (diamond)
    wA_lo, wB_lo, ret_lo, std_lo = all_mvp_long[rho]
    fig_short.add_trace(go.Scatter(
        x=[std_lo * 100],
        y=[ret_lo * 100],
        mode='markers',
        marker=dict(color=COLORS['primary_blue'], size=14, symbol='diamond', line=dict(color='white', width=2)),
        name='Long-Only MVP',
        visible=visible,
        showlegend=False,
        hovertemplate=f"<b>◆ Long-Only MVP</b><br>ρ = {rho:.2f}<br>w_A = {wA_lo:.1%} | w_B = {wB_lo:.1%}<br>σ = %{{x:.2f}}%<br>E[r] = %{{y:.2f}}%<extra></extra>"
    ))

# Asset markers (always visible)
fig_short.add_trace(go.Scatter(
    x=[sigma_A * 100], y=[E_A * 100],
    mode='markers+text', name='Asset A',
    marker=dict(color=COLORS['positive'], size=18, symbol='circle', line=dict(color='white', width=2)),
    text=['A'], textposition='middle right',
    textfont=dict(size=14, weight='bold', color=COLORS['positive']),
    hovertemplate="<b>Asset A (100% Long)</b><br>σ = 12%<br>E[r] = 10%<extra></extra>"
))

fig_short.add_trace(go.Scatter(
    x=[sigma_B * 100], y=[E_B * 100],
    mode='markers+text', name='Asset B',
    marker=dict(color=COLORS['warning'], size=18, symbol='circle', line=dict(color='white', width=2)),
    text=['B'], textposition='middle right',
    textfont=dict(size=14, weight='bold', color=COLORS['warning']),
    hovertemplate="<b>Asset B (100% Long)</b><br>σ = 20%<br>E[r] = 16%<extra></extra>"
))

# Slider
steps_short = []
legend_annotation = dict(
    x=0.02, y=0.98, xref="paper", yref="paper",
    text=(
        "<b>Legend:</b><br>"
        "━━━ Long-only frontier<br>"
        "<span style='color:#E74C3C'>▭</span> Short-only region (red dashed)<br>"
        "⋯⋯ Unconstrained frontier shape<br>"
        "Global MVP (best possible)<br>"
        "◆ Long-only MVP"
    ),
    showarrow=False,
    font=dict(size=10, color=COLORS['dark']),
    bgcolor='rgba(255,255,255,0.95)',
    bordercolor=COLORS['dark'],
    borderwidth=1,
    borderpad=8,
    align='left'
)
for i, rho in enumerate(corr_values_short):
    visibility = [False] * (len(corr_values_short) * trace_per_rho) + [True, True]
    base = i * trace_per_rho
    for k in range(trace_per_rho):
        visibility[base + k] = True

    wA_star, wB_star, ret_star, std_star = all_mvp_global[rho]
    wA_lo, wB_lo, ret_lo, std_lo = all_mvp_long[rho]
    requires_short = (wA_star < 0) or (wA_star > 1)
    short_note = "Shorting required" if requires_short else "Long-only feasible"
    title = (
        f"<b>Short-Selling vs Long-Only (ρ = {rho:.2f})</b><br>"
        f"<sup>Global MVP: w_A = {wA_star:.1%}, w_B = {wB_star:.1%} | σ = {std_star:.2%} | E[r] = {ret_star:.2%} | {short_note}</sup>"
    )

    summary_annotation = dict(
        x=0.98, y=0.98, xref="paper", yref="paper",
        text=(
            f"<b>Global MVP Summary</b><br>"
            f"w_A = {wA_star:.1%}<br>"
            f"w_B = {wB_star:.1%}<br>"
            f"σ = {std_star:.2%}<br>"
            f"E[r] = {ret_star:.2%}"
        ),
        showarrow=False,
        font=dict(size=10, color=COLORS['dark']),
        bgcolor='rgba(255,255,255,0.95)',
        bordercolor=COLORS['dark'],
        borderwidth=1,
        borderpad=8,
        align='left'
    )

    steps_short.append(dict(
        method="update",
        args=[{"visible": visibility}, {"title": {"text": title}, "annotations": [legend_annotation, summary_annotation]}],
        label=f"{rho:.1f}"
    ))

default_wA, default_wB, default_ret, default_std = all_mvp_global[default_rho]
default_short = "Shorting required" if (default_wA < 0 or default_wA > 1) else "Long-only feasible"

fig_short.update_layout(
    title=dict(
        text=(
            f"<b>Short-Selling vs Long-Only (ρ = {default_rho:.2f})</b><br>"
            f"<sup>Global MVP: w_A = {default_wA:.1%}, w_B = {default_wB:.1%} | σ = {default_std:.2%} | E[r] = {default_ret:.2%} | {default_short}</sup>"
        ),
        x=0.5,
        font=dict(size=13, color=COLORS['dark'])
    ),
    sliders=[dict(
        active=default_idx,
        yanchor="top",
        y=-0.08,
        xanchor="left",
        currentvalue=dict(
            prefix="<b>Correlation ρ = </b>",
            visible=True,
            xanchor="center",
            font=dict(size=15, color=COLORS['dark'], family='Arial Black')
        ),
        pad=dict(b=10, t=60),
        len=0.88,
        x=0.06,
        steps=steps_short,
        bgcolor='rgba(255,255,255,0.95)',
        bordercolor=COLORS['dark'],
        borderwidth=2,
        tickcolor=COLORS['dark'],
        ticklen=8,
        tickwidth=2
    )],
    xaxis_title="<b>Portfolio Risk (σ) %</b>",
    yaxis_title="<b>Expected Return E[r] %</b>",
    height=650,
    margin=dict(l=70, r=50, t=140, b=120),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    hovermode='closest',
    annotations=[
        legend_annotation,
        dict(
            x=0.98, y=0.98, xref="paper", yref="paper",
            text=(
                f"<b>Global MVP Summary</b><br>"
                f"w_A = {default_wA:.1%}<br>"
                f"w_B = {default_wB:.1%}<br>"
                f"σ = {default_std:.2%}<br>"
                f"E[r] = {default_ret:.2%}"
            ),
            showarrow=False,
            font=dict(size=10, color=COLORS['dark']),
            bgcolor='rgba(255,255,255,0.95)',
            bordercolor=COLORS['dark'],
            borderwidth=1,
            borderpad=8,
            align='left'
        )
    ]
)

fig_short.update_xaxes(range=[0, 40], showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_short.update_yaxes(range=[2, 24], showgrid=True, gridcolor='rgba(0,0,0,0.1)')

fig_short.show()
```

**What to notice (quick checklist)**

1. If the **gold star** is outside the blue segment, shorting is required to reach the true MVP.
2. The **red dashed region** highlights portfolios that are impossible without shorting.
3. When correlation is high (e.g., ρ ≈ 0.9), the global MVP often shifts into the shorting region.

</details>

### Correlation Sensitivity Heatmap: Portfolio Risk Landscape

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Correlation Sensitivity Heatmap <em>(click to expand)</em></summary>

The heatmap below provides a comprehensive view of how portfolio risk varies with **both** the weight in Asset A and the correlation level:

```{python}
#| label: correlation-sensitivity-heatmap
#| echo: false

import plotly.graph_objects as go
import numpy as np

# Create grid of weights and correlations
weights_A = np.linspace(0, 1, 51)
correlations = np.linspace(-0.5, 1.0, 31)

# Calculate portfolio volatility for each combination
z_matrix = np.zeros((len(correlations), len(weights_A)))

for i, rho in enumerate(correlations):
    for j, w_A in enumerate(weights_A):
        vol = portfolio_std(w_A, sigma_A, sigma_B, rho) * 100
        z_matrix[i, j] = vol

fig_heatmap = go.Figure(data=go.Heatmap(
    z=z_matrix,
    x=weights_A * 100,
    y=correlations,
    xgap=0,
    ygap=0,
    colorscale=[
        [0, '#2ecc71'],      # Low risk = green
        [0.3, '#f1c40f'],    # Medium = yellow
        [0.6, '#e67e22'],    # Higher = orange
        [1, '#c0392b']       # High risk = red
    ],
    colorbar=dict(
        title='Portfolio σ (%)',
        titleside='right',
        ticksuffix='%'
    ),
    hovertemplate='Weight A: %{x:.0f}%<br>Correlation: %{y:.1f}<br>Portfolio σ: %{z:.2f}%<extra></extra>'
))

# Add contour lines for key volatility levels
fig_heatmap.add_trace(go.Contour(
    z=z_matrix,
    x=weights_A * 100,
    y=correlations,
    contours=dict(
        coloring='none',
        showlabels=True,
        labelfont=dict(size=10, color='white'),
        start=10,
        end=20,
        size=2
    ),
    line=dict(width=1.5, color='rgba(255,255,255,0.7)'),
    showscale=False,
    hoverinfo='skip'
))

# Mark MVP locations for key correlations
mvp_points = []
for rho in [0.0, 0.3, 0.6, 0.9]:
    mvp_w = np.clip(mvp_weight_A(sigma_A, sigma_B, rho), 0, 1)
    mvp_vol = portfolio_std(mvp_w, sigma_A, sigma_B, rho) * 100
    mvp_points.append((mvp_w * 100, rho, mvp_vol))

fig_heatmap.add_trace(go.Scatter(
    x=[p[0] for p in mvp_points],
    y=[p[1] for p in mvp_points],
    mode='markers+text',
    marker=dict(size=12, color='white', symbol='diamond', line=dict(color='black', width=2)),
    text=['MVP' if i == 1 else '' for i in range(len(mvp_points))],
    textposition='top center',
    textfont=dict(size=10, color='white', weight='bold'),
    name='MVP Path',
    hovertemplate='<b>MVP</b><br>w_A: %{x:.0f}%<br>ρ: %{y:.1f}<br>σ: %{customdata:.1f}%<extra></extra>',
    customdata=[p[2] for p in mvp_points],
    showlegend=False
))

# Connect MVP points with a line
fig_heatmap.add_trace(go.Scatter(
    x=[p[0] for p in mvp_points],
    y=[p[1] for p in mvp_points],
    mode='lines',
    line=dict(color='white', width=2, dash='dash'),
    name='MVP Path',
    showlegend=False,
    hoverinfo='skip'
))

fig_heatmap.update_layout(
    title=dict(
        text="<b>Portfolio Risk Landscape: Weight vs. Correlation</b><br><sup>Green = low risk, Red = high risk. White diamonds trace the MVP as correlation changes.</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis=dict(
        title='Weight in Asset A (%)',
        tickmode='array',
        tickvals=list(range(0, 101, 20)),
        ticktext=['0%', '20%', '40%', '60%', '80%', '100%'],
        range=[-1, 101]
    ),
    yaxis=dict(
        title='Correlation (ρ)',
        tickmode='array',
        tickvals=[-0.5, 0, 0.3, 0.5, 0.9, 1.0],
        ticktext=['-0.5', '0', '0.3', '0.5', '0.9', '1.0'],
        range=[-0.5, 1.0]
    ),
    height=500,
    margin=dict(l=70, r=100, t=100, b=70),
    paper_bgcolor='rgba(0,0,0,0)'
)

fig_heatmap.show()
```

::: {.callout-tip}
## Reading the Heatmap
- **Horizontal movement**: Changing portfolio weights at fixed correlation
- **Vertical movement**: Seeing how same weights perform under different correlation regimes
- **White diamond path**: Shows how the optimal (MVP) weight shifts toward Asset A as correlation increases
- **Green valley**: The "sweet spot" of low portfolio volatility; note how it narrows as correlation rises
:::


<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Mathematics of MVP Shift <em>(click to expand)</em></summary>

The analytical formula for the **Minimum Variance Portfolio** weight in Asset A is:

$$
w_A^{*} = \frac{\sigma_B^2 - \sigma_A \sigma_B \rho}{\sigma_A^2 + \sigma_B^2 - 2\sigma_A \sigma_B \rho}
$$

Let's trace through the calculation:

```{python}
#| label: mvp-derivation
#| echo: true

# Show MVP derivation
derivation_data = {
    'Parameter': ['σ_A²', 'σ_B²', 'σ_A·σ_B', 'ρ', 'Numerator: σ_B² - σ_A·σ_B·ρ', 
                  'Denominator: σ_A² + σ_B² - 2·σ_A·σ_B·ρ', 'w_A* (unconstrained)', 'w_A* (constrained to [0,1])'],
    'ρ = 0.3': [
        f"{sigma_A**2:.4f}",
        f"{sigma_B**2:.4f}",
        f"{sigma_A * sigma_B:.4f}",
        "0.30",
        f"{sigma_B**2 - sigma_A * sigma_B * rho_base:.4f}",
        f"{sigma_A**2 + sigma_B**2 - 2 * sigma_A * sigma_B * rho_base:.4f}",
        f"{(sigma_B**2 - sigma_A * sigma_B * rho_base) / (sigma_A**2 + sigma_B**2 - 2 * sigma_A * sigma_B * rho_base):.4f}",
        f"{mvp_wA_base:.4f} ({mvp_wA_base:.1%})"
    ],
    'ρ = 0.9': [
        f"{sigma_A**2:.4f}",
        f"{sigma_B**2:.4f}",
        f"{sigma_A * sigma_B:.4f}",
        "0.90",
        f"{sigma_B**2 - sigma_A * sigma_B * rho_high:.4f}",
        f"{sigma_A**2 + sigma_B**2 - 2 * sigma_A * sigma_B * rho_high:.4f}",
        f"{(sigma_B**2 - sigma_A * sigma_B * rho_high) / (sigma_A**2 + sigma_B**2 - 2 * sigma_A * sigma_B * rho_high):.4f}",
        f"{mvp_wA_high:.4f} ({mvp_wA_high:.1%})"
    ]
}

deriv_df = pd.DataFrame(derivation_data)

fig_deriv = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Parameter</b>', '<b>ρ = 0.3</b>', '<b>ρ = 0.9</b>'],
        fill_color=COLORS['purple'],
        font=dict(color='white', size=11),
        align='left',
        height=35
    ),
    cells=dict(
        values=[deriv_df[col] for col in deriv_df.columns],
        fill_color=['#faf8fc', 'white', 'white'],
        font=dict(size=10),
        align='left',
        height=28
    )
)])

fig_deriv.update_layout(
    title=dict(text="<b>MVP Weight Calculation: Step-by-Step</b>", x=0.5, font=dict(size=13)),
    height=350,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_deriv.show()
```

**Interpretation:**

- When $\rho$ is low (0.3), the numerator is larger relative to the denominator, resulting in $w_A^* \approx 71\%$
- When $\rho$ is high (0.9), both numerator and denominator shrink, but the ratio shifts toward higher $w_A^* \approx 83\%$
- The MVP gravitates toward the lower-volatility asset as diversification benefits diminish

</details>

</details>

### 3D Portfolio Explorer

Interactive 3D surface showing portfolio risk, return, and efficiency across all weight allocations and correlations. Use the dropdown to switch views. Drag to rotate, scroll to zoom, hover for details.

```{python}
#| label: ultimate-3d-portfolio-explorer
#| echo: true

# ============================================================
# ULTIMATE 3D PORTFOLIO EXPLORER (ENHANCED)
# ============================================================

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np

# Create high-resolution grid
n_weights = 101
n_corr = 51
weights_grid = np.linspace(0, 1, n_weights)
corr_grid = np.linspace(-0.5, 1.0, n_corr)
W, RHO = np.meshgrid(weights_grid, corr_grid)

# Calculate risk and return surfaces
def calc_portfolio_std_grid(w_A, rho):
    """Vectorized portfolio volatility calculation."""
    w_B = 1 - w_A
    var = (w_A**2 * sigma_A**2 + 
           w_B**2 * sigma_B**2 + 
           2 * w_A * w_B * sigma_A * sigma_B * rho)
    return np.sqrt(np.maximum(var, 0)) * 100

def calc_portfolio_return_grid(w_A):
    """Vectorized portfolio return calculation."""
    return (w_A * E_A + (1 - w_A) * E_B) * 100

# Compute surfaces
Z_risk = calc_portfolio_std_grid(W, RHO)
Z_return = calc_portfolio_return_grid(W)

# Calculate Sharpe-like ratio surface (return / risk)
Z_sharpe = np.where(Z_risk > 0.1, Z_return / Z_risk, np.nan)

# Find MVP path across correlations
mvp_path = []
for rho in corr_grid:
    w_mvp = np.clip(mvp_weight_A(sigma_A, sigma_B, rho), 0, 1)
    std_mvp = portfolio_std(w_mvp, sigma_A, sigma_B, rho) * 100
    ret_mvp = portfolio_return(w_mvp, E_A, E_B) * 100
    mvp_path.append((w_mvp * 100, rho, std_mvp, ret_mvp))

mvp_weights = [p[0] for p in mvp_path]
mvp_corrs = [p[1] for p in mvp_path]
mvp_risks = [p[2] for p in mvp_path]
mvp_returns = [p[3] for p in mvp_path]

# ============================================================
# FIGURE 1: Interactive 3D Surface with Dropdown
# ============================================================

fig_3d = go.Figure()

# --- RISK SURFACE ---
fig_3d.add_trace(go.Surface(
    x=weights_grid * 100,
    y=corr_grid,
    z=Z_risk,
    colorscale=[
        [0, '#1a9850'],      # Deep green (low risk = GOOD)
        [0.2, '#66bd63'],    # Medium green
        [0.4, '#a6d96a'],    # Light green
        [0.5, '#fee08b'],    # Yellow (medium)
        [0.7, '#fdae61'],    # Orange
        [0.85, '#f46d43'],   # Dark orange
        [1, '#d73027']       # Red (high risk = BAD)
    ],
    colorbar=dict(
        title=dict(text='<b>Risk (σ)</b>', side='right', font=dict(size=13)),
        x=1.05,
        len=0.7,
        thickness=25,
        tickformat='.0f',
        ticksuffix='%',
        tickfont=dict(size=11),
        outlinewidth=1,
        outlinecolor='#333'
    ),
    opacity=0.94,
    lighting=dict(ambient=0.7, diffuse=0.8, specular=0.3, roughness=0.5),
    contours=dict(
        z=dict(show=True, usecolormap=True, project=dict(z=True), width=2, highlightwidth=3),
        x=dict(show=True, color='rgba(0,0,0,0.12)', width=1),
        y=dict(show=True, color='rgba(0,0,0,0.12)', width=1)
    ),
    hovertemplate=(
        'Asset A: %{x:.0f}%<br>'
        'Asset B: ' + '%{text}' + '<br>'
        'Correlation: %{y:.2f}<br>'
        'Risk: %{z:.1f}%<extra></extra>'
    ),
    text=[[f'{100-w:.0f}%' for w in weights_grid * 100] for _ in corr_grid],
    name='Risk Surface',
    visible=True
))

# --- RETURN SURFACE (High return = Green, Low return = Red) ---
fig_3d.add_trace(go.Surface(
    x=weights_grid * 100,
    y=corr_grid,
    z=Z_return,
    colorscale=[
        [0, '#d73027'],      # Red (low return = BAD)
        [0.15, '#f46d43'],   # Dark orange
        [0.3, '#fdae61'],    # Orange
        [0.45, '#fee08b'],   # Yellow
        [0.6, '#a6d96a'],    # Light green
        [0.8, '#66bd63'],    # Medium green
        [1, '#1a9850']       # Deep green (high return = GOOD)
    ],
    colorbar=dict(
        title=dict(text='<b>Return E[r]</b>', side='right', font=dict(size=13)),
        x=1.05,
        len=0.7,
        thickness=25,
        tickformat='.0f',
        ticksuffix='%',
        tickfont=dict(size=11),
        outlinewidth=1,
        outlinecolor='#333'
    ),
    opacity=0.94,
    lighting=dict(ambient=0.7, diffuse=0.8, specular=0.3, roughness=0.5),
    contours=dict(
        z=dict(show=True, usecolormap=True, project=dict(z=True), width=2)
    ),
    hovertemplate=(
        'Asset A: %{x:.0f}%<br>'
        'Asset B: ' + '%{text}' + '<br>'
        'Correlation: %{y:.2f}<br>'
        'Return: %{z:.1f}%<extra></extra>'
    ),
    text=[[f'{100-w:.0f}%' for w in weights_grid * 100] for _ in corr_grid],
    name='Return Surface',
    visible=False
))

# --- SHARPE SURFACE ---
fig_3d.add_trace(go.Surface(
    x=weights_grid * 100,
    y=corr_grid,
    z=Z_sharpe,
    colorscale='Viridis',
    colorbar=dict(
        title=dict(text='<b>Efficiency</b><br>(E[r]/σ)', side='right', font=dict(size=12)),
        x=1.05,
        len=0.7,
        thickness=25,
        tickformat='.2f',
        tickfont=dict(size=11),
        outlinewidth=1,
        outlinecolor='#333'
    ),
    opacity=0.94,
    lighting=dict(ambient=0.7, diffuse=0.8, specular=0.3, roughness=0.5),
    contours=dict(
        z=dict(show=True, usecolormap=True, project=dict(z=True), width=2)
    ),
    hovertemplate=(
        'Asset A: %{x:.0f}%<br>'
        'Asset B: ' + '%{text}' + '<br>'
        'Correlation: %{y:.2f}<br>'
        'Efficiency: %{z:.2f}<extra></extra>'
    ),
    text=[[f'{100-w:.0f}%' for w in weights_grid * 100] for _ in corr_grid],
    name='Efficiency Surface',
    visible=False
))

# --- MVP PATH (3D Line for Risk surface) ---
fig_3d.add_trace(go.Scatter3d(
    x=mvp_weights,
    y=mvp_corrs,
    z=mvp_risks,
    mode='lines+markers',
    line=dict(color='#FFD700', width=12),
    marker=dict(size=5, color='#FFD700', symbol='diamond', line=dict(color='#333', width=1)),
    name='MVP Path',
    hovertemplate=(
        'MVP: Asset A %{x:.1f}%, Asset B %{customdata:.1f}%<br>'
        'Correlation: %{y:.2f}<br>'
        'Min Risk: %{z:.1f}%<extra></extra>'
    ),
    customdata=[100 - w for w in mvp_weights],
    visible=True
))

# MVP path for return surface (at return levels)
fig_3d.add_trace(go.Scatter3d(
    x=mvp_weights,
    y=mvp_corrs,
    z=mvp_returns,
    mode='lines+markers',
    line=dict(color='#FFD700', width=12),
    marker=dict(size=5, color='#FFD700', symbol='diamond', line=dict(color='#333', width=1)),
    name='MVP Path',
    hovertemplate=(
        'MVP: Asset A %{x:.1f}%, Asset B %{customdata:.1f}%<br>'
        'Correlation: %{y:.2f}<br>'
        'Return at MVP: %{z:.1f}%<extra></extra>'
    ),
    customdata=[100 - w for w in mvp_weights],
    visible=False
))

# MVP path for Sharpe surface
sharpe_at_mvp = [r/s if s > 0.1 else np.nan for r, s in zip(mvp_returns, mvp_risks)]
fig_3d.add_trace(go.Scatter3d(
    x=mvp_weights,
    y=mvp_corrs,
    z=sharpe_at_mvp,
    mode='lines+markers',
    line=dict(color='gold', width=8),
    marker=dict(size=4, color='gold', symbol='diamond'),
    name='MVP Path',
    hovertemplate='MVP: Weight A %{x:.1f}%, ρ=%{y:.2f}, Efficiency=%{z:.3f}<extra></extra>',
    visible=False
))

# Asset markers (spheres at corners)
# Asset A: w_A=100%, shown at all correlations (pick middle)
fig_3d.add_trace(go.Scatter3d(
    x=[100], y=[0.3], z=[sigma_A * 100],
    mode='markers+text',
    marker=dict(size=10, color=COLORS['positive'], symbol='circle'),
    text=['Asset A'],
    textposition='top center',
    name='Asset A',
    visible=True,
    showlegend=False
))

# Asset B: w_A=0%
# Asset B marker for Risk surface
fig_3d.add_trace(go.Scatter3d(
    x=[0], y=[0.3], z=[sigma_B * 100],
    mode='markers+text',
    marker=dict(size=14, color=COLORS['warning'], symbol='circle', 
                line=dict(color='#333', width=2)),
    text=['<b>ASSET B</b><br>100% Bonds<br>σ=20%, E[r]=16%'],
    textposition='top center',
    textfont=dict(size=10, color=COLORS['warning']),
    name='Asset B (Bonds)',
    hovertemplate='Asset B: 100% Bonds<br>Risk: 20%, Return: 16%<extra></extra>',
    visible=True,
    showlegend=False
))

# Duplicate asset markers for return surface
fig_3d.add_trace(go.Scatter3d(
    x=[100], y=[0.3], z=[E_A * 100],
    mode='markers+text',
    marker=dict(size=14, color=COLORS['positive'], symbol='circle',
                line=dict(color='#333', width=2)),
    text=['<b>ASSET A</b><br>100% Stocks<br>E[r]=10%'],
    textposition='top center',
    textfont=dict(size=10, color=COLORS['positive']),
    hovertemplate='Asset A: 100% Stocks<br>Risk: 12%, Return: 10%<extra></extra>',
    visible=False,
    showlegend=False
))

fig_3d.add_trace(go.Scatter3d(
    x=[0], y=[0.3], z=[E_B * 100],
    mode='markers+text',
    marker=dict(size=14, color=COLORS['warning'], symbol='circle',
                line=dict(color='#333', width=2)),
    text=['<b>ASSET B</b><br>100% Bonds<br>E[r]=16%'],
    textposition='top center',
    textfont=dict(size=10, color=COLORS['warning']),
    hovertemplate='Asset B: 100% Bonds<br>Risk: 20%, Return: 16%<extra></extra>',
    visible=False,
    showlegend=False
))

# Asset markers for Sharpe surface
fig_3d.add_trace(go.Scatter3d(
    x=[100], y=[0.3], z=[E_A / sigma_A],
    mode='markers+text',
    marker=dict(size=14, color=COLORS['positive'], symbol='circle',
                line=dict(color='#333', width=2)),
    text=['<b>ASSET A</b><br>100% Stocks<br>Sharpe=0.83'],
    textposition='top center',
    textfont=dict(size=10, color=COLORS['positive']),
    hovertemplate='Asset A: 100% Stocks<br>Return/Risk: 0.83<extra></extra>',
    visible=False,
    showlegend=False
))

fig_3d.add_trace(go.Scatter3d(
    x=[0], y=[0.3], z=[E_B / sigma_B],
    mode='markers+text',
    marker=dict(size=14, color=COLORS['warning'], symbol='circle',
                line=dict(color='#333', width=2)),
    text=['<b>ASSET B</b><br>100% Bonds<br>Sharpe=0.80'],
    textposition='top center',
    textfont=dict(size=10, color=COLORS['warning']),
    hovertemplate='Asset B: 100% Bonds<br>Return/Risk: 0.80<extra></extra>',
    visible=False,
    showlegend=False
))

# Create dropdown menu
fig_3d.update_layout(
    updatemenus=[
        dict(
            active=0,
            buttons=[
                dict(
                    label='Risk (σ%)',
                    method='update',
                    args=[{'visible': [True, False, False, True, False, False, True, True, False, False, False, False]},
                          {'scene': {
                              'xaxis': {'title': {'text': 'Asset A Weight (%)', 'font': {'size': 12}}, 'ticktext': ['0% (100% B)', '25%', '50%', '75%', '100% (0% B)']},
                              'yaxis': {'title': {'text': 'Correlation (ρ)', 'font': {'size': 12}}},
                              'zaxis': {'title': {'text': 'Risk (σ%)', 'font': {'size': 12}}, 'ticksuffix': '%'}
                          }}]
                ),
                dict(
                    label='Return (E[r]%)',
                    method='update',
                    args=[{'visible': [False, True, False, False, True, False, False, False, True, True, False, False]},
                          {'scene': {
                              'xaxis': {'title': {'text': 'Asset A Weight (%)', 'font': {'size': 12}}, 'ticktext': ['0% (100% B)', '25%', '50%', '75%', '100% (0% B)']},
                              'yaxis': {'title': {'text': 'Correlation (ρ)', 'font': {'size': 12}}},
                              'zaxis': {'title': {'text': 'Expected Return (%)', 'font': {'size': 12}}, 'ticksuffix': '%'}
                          }}]
                ),
                dict(
                    label='Efficiency (E[r]/σ)',
                    method='update',
                    args=[{'visible': [False, False, True, False, False, True, False, False, False, False, True, True]},
                          {'scene': {
                              'xaxis': {'title': {'text': 'Asset A Weight (%)', 'font': {'size': 12}}, 'ticktext': ['0% (100% B)', '25%', '50%', '75%', '100% (0% B)']},
                              'yaxis': {'title': {'text': 'Correlation (ρ)', 'font': {'size': 12}}},
                              'zaxis': {'title': {'text': 'Efficiency (E[r]/σ)', 'font': {'size': 12}}}
                          }}]
                )
            ],
            direction='down',
            showactive=True,
            x=0.02,
            xanchor='left',
            y=0.98,
            yanchor='top',
            bgcolor='rgba(255,255,255,0.98)',
            bordercolor=COLORS['dark'],
            borderwidth=1,
            font=dict(size=12)
        )
    ],
    scene=dict(
        xaxis=dict(
            title=dict(
                text='Asset A Weight (%)',
                font=dict(size=12, color=COLORS['dark'])
            ),
            tickvals=[0, 25, 50, 75, 100],
            ticktext=['0% (100% B)', '25%', '50%', '75%', '100% (0% B)'],
            tickfont=dict(size=9),
            backgroundcolor='rgba(248,249,250,0.95)',
            gridcolor='rgba(0,0,0,0.15)',
            showbackground=True,
            gridwidth=1
        ),
        yaxis=dict(
            title=dict(
                text='Correlation (ρ)',
                font=dict(size=12, color=COLORS['dark'])
            ),
            tickvals=[-0.5, 0, 0.3, 0.6, 0.9],
            ticktext=['-0.5', '0', '0.3', '0.6', '0.9'],
            tickfont=dict(size=9),
            backgroundcolor='rgba(248,249,250,0.95)',
            gridcolor='rgba(0,0,0,0.15)',
            showbackground=True,
            gridwidth=1
        ),
        zaxis=dict(
            title=dict(
                text='Risk (σ%)',
                font=dict(size=12, color=COLORS['dark'])
            ),
            backgroundcolor='rgba(248,249,250,0.95)',
            gridcolor='rgba(0,0,0,0.15)',
            showbackground=True,
            range=[0, 22],
            gridwidth=1,
            ticksuffix='%'
        ),
        camera=dict(
            eye=dict(x=1.9, y=-1.9, z=1.3),
            center=dict(x=0, y=0, z=-0.1)
        ),
        aspectratio=dict(x=1.3, y=1.1, z=0.9)
    ),
    title=dict(
        text='3D Portfolio Explorer',
        x=0.5,
        font=dict(size=16, color=COLORS['dark'])
    ),
    height=700,
    margin=dict(l=0, r=0, t=60, b=0),
    paper_bgcolor='rgba(0,0,0,0)',
    showlegend=True,
    legend=dict(
        x=0.85,
        y=0.98,
        bgcolor='rgba(255,255,255,0.95)',
        bordercolor=COLORS['dark'],
        borderwidth=1,
        font=dict(size=11)
    )
)

fig_3d.show()
```

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Multi-Panel Contour Dashboard <em>(click to expand)</em></summary>

Three contour panels showing portfolio risk, return, and efficiency across all weight allocations and correlations.

```{python}
#| label: multi-panel-dashboard
#| echo: true

# ============================================================
# MULTI-PANEL CONTOUR DASHBOARD
# ============================================================

from plotly.subplots import make_subplots
import plotly.graph_objects as go

fig_dash = make_subplots(
    rows=3, cols=1,
    subplot_titles=(
        'Risk (σ%)',
        'Return (E[r]%)',
        'Efficiency (E[r]/σ)'
    ),
    vertical_spacing=0.12
)

# Common colorbar settings
common_colorbar = dict(len=0.25, thickness=18, tickformat='.1f', tickfont=dict(size=10))

# --- RISK CONTOUR ---
fig_dash.add_trace(go.Contour(
    x=weights_grid * 100,
    y=corr_grid,
    z=Z_risk,
    colorscale=[
        [0, '#1a9850'], [0.2, '#66bd63'], [0.4, '#a6d96a'], 
        [0.5, '#fee08b'], [0.6, '#fdae61'], [0.8, '#f46d43'], [1, '#d73027']
    ],
    contours=dict(showlabels=True, labelfont=dict(size=10, color='black')),
    colorbar=dict(**common_colorbar, title=dict(text='Risk<br>σ (%)', font=dict(size=11)), y=0.83),
    hovertemplate=(
        'Asset A: %{x:.0f}%<br>'
        'Asset B: %{customdata:.0f}%<br>'
        'Correlation: %{y:.2f}<br>'
        'Risk: %{z:.2f}%<extra></extra>'
    ),
    customdata=100 - weights_grid * 100
), row=1, col=1)

# Add MVP path to risk panel
fig_dash.add_trace(go.Scatter(
    x=mvp_weights, y=mvp_corrs,
    mode='lines+markers',
    line=dict(color='#333', width=4, dash='dot'),
    marker=dict(size=8, color='#FFD700', symbol='diamond', line=dict(color='#333', width=1.5)),
    name='MVP Path',
    hovertemplate=(
        'MVP at ρ=%{y:.2f}<br>'
        'Optimal A: %{x:.1f}%<extra></extra>'
    ),
    showlegend=False
), row=1, col=1)

# --- RETURN CONTOUR ---
fig_dash.add_trace(go.Contour(
    x=weights_grid * 100,
    y=corr_grid,
    z=Z_return,
    colorscale='RdYlBu_r',
    contours=dict(showlabels=True, labelfont=dict(size=10, color='black')),
    colorbar=dict(**common_colorbar, title=dict(text='Return<br>E[r] (%)', font=dict(size=11)), y=0.5),
    hovertemplate=(
        'Asset A: %{x:.0f}%<br>'
        'Asset B: %{customdata:.0f}%<br>'
        'Correlation: %{y:.2f}<br>'
        'Return: %{z:.2f}%<extra></extra>'
    ),
    customdata=100 - weights_grid * 100
), row=2, col=1)

# Note: Return doesn't depend on correlation, so horizontal contours
fig_dash.add_annotation(
    x=50, y=0.75,
    text='Note: Return depends<br>only on weights',
    showarrow=False,
    font=dict(size=9, color='rgba(0,0,0,0.7)'),
    bgcolor='rgba(255,255,255,0.85)',
    bordercolor='rgba(0,0,0,0.3)',
    borderwidth=1,
    borderpad=4,
    xref='x2', yref='y2'
)

# --- SHARPE CONTOUR ---
fig_dash.add_trace(go.Contour(
    x=weights_grid * 100,
    y=corr_grid,
    z=Z_sharpe,
    colorscale='Viridis',
    contours=dict(showlabels=True, labelfont=dict(size=10, color='white')),
    colorbar=dict(**common_colorbar, title=dict(text='Efficiency<br>E[r]/σ', font=dict(size=11)), y=0.17),
    hovertemplate=(
        'Asset A: %{x:.0f}%<br>'
        'Asset B: %{customdata:.0f}%<br>'
        'Correlation: %{y:.2f}<br>'
        'Efficiency: %{z:.3f}<extra></extra>'
    ),
    customdata=100 - weights_grid * 100
), row=3, col=1)

# Add MVP path to Sharpe panel
fig_dash.add_trace(go.Scatter(
    x=mvp_weights, y=mvp_corrs,
    mode='lines+markers',
    line=dict(color='white', width=4, dash='dot'),
    marker=dict(size=8, color='#FFD700', symbol='diamond', line=dict(color='#333', width=1.5)),
    showlegend=False,
    hovertemplate=(
        'MVP at ρ=%{y:.2f}<br>'
        'Optimal A: %{x:.1f}%<extra></extra>'
    )
), row=3, col=1)

# Mark key correlations with labeled lines
for panel in [1, 2, 3]:
    for rho_mark, label, color in [(0.3, 'ρ=0.3 (Base Case)', 'rgba(52, 73, 94, 0.8)'), 
                                    (0.9, 'ρ=0.9 (High Corr)', 'rgba(192, 57, 43, 0.8)')]:
        fig_dash.add_hline(
            y=rho_mark, line_dash='dash', line_color=color, line_width=2,
            row=panel, col=1
        )

# Add labels for the horizontal lines (on first panel)
fig_dash.add_annotation(x=95, y=0.32, text='ρ=0.3', showarrow=False, 
                        font=dict(size=9, color='#34495e'), xref='x', yref='y')
fig_dash.add_annotation(x=95, y=0.92, text='ρ=0.9', showarrow=False, 
                        font=dict(size=9, color='#c0392b'), xref='x', yref='y')

# Update axes for all panels with proper labels
for i in range(1, 4):
    fig_dash.update_xaxes(
        title_text='Weight in Asset A (%)' if i == 3 else '',
        title_font=dict(size=10),
        tickvals=[0, 25, 50, 75, 100],
        ticktext=['0%', '25%', '50%', '75%', '100%'],
        tickfont=dict(size=9),
        gridcolor='rgba(0,0,0,0.1)',
        row=i, col=1
    )
    fig_dash.update_yaxes(
        title_text='Correlation (ρ)',
        title_font=dict(size=10),
        tickvals=[-0.5, 0, 0.3, 0.6, 0.9],
        ticktext=['-0.5', '0', '0.3', '0.6', '0.9'],
        tickfont=dict(size=9),
        gridcolor='rgba(0,0,0,0.1)',
        row=i, col=1
    )

fig_dash.update_layout(
    title=dict(
        text='Portfolio Analysis Dashboard',
        x=0.5,
        font=dict(size=15, color=COLORS['dark'])
    ),
    height=1200,
    margin=dict(l=80, r=30, t=80, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)'
)

fig_dash.show()
```

</details>

---

### Portfolio Positioning: Risk-Return Scatter

<details markdown="1">
<summary style="font-size: 1.1em; font-weight: bold; cursor: pointer; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #9b59b6; margin-bottom: 10px;">Risk-Return Scatter Plot <em>(click to expand)</em></summary>

The following bubble chart positions key portfolios in risk-return space, making it easy to compare their characteristics at a glance:

```{python}
#| label: risk-return-scatter
#| echo: false

import plotly.graph_objects as go

# Compute equal weight portfolio values
equal_weight_return = portfolio_return(0.5, E_A, E_B)
equal_weight_std = portfolio_std(0.5, sigma_A, sigma_B, rho_base)

# Define key portfolios to plot (for ρ = 0.3 base case)
portfolios = {
    'Asset A (100%)': {
        'return': E_A * 100,
        'risk': sigma_A * 100,
        'weight_A': 100,
        'color': COLORS['positive'],
        'symbol': 'circle'
    },
    'Asset B (100%)': {
        'return': E_B * 100,
        'risk': sigma_B * 100,
        'weight_A': 0,
        'color': COLORS['warning'],
        'symbol': 'circle'
    },
    'MVP (ρ=0.3)': {
        'return': mvp_return_base * 100,
        'risk': mvp_std_base * 100,
        'weight_A': mvp_wA_base * 100,
        'color': COLORS['primary_blue'],
        'symbol': 'diamond'
    },
    'MVP (ρ=0.9)': {
        'return': mvp_return_high * 100,
        'risk': mvp_std_high * 100,
        'weight_A': mvp_wA_high * 100,
        'color': COLORS['negative'],
        'symbol': 'diamond'
    },
    'Equal Weight (50/50)': {
        'return': equal_weight_return * 100,
        'risk': equal_weight_std * 100,
        'weight_A': 50,
        'color': COLORS['purple'],
        'symbol': 'square'
    }
}

fig_scatter = go.Figure()

# Add each portfolio as a bubble
for name, props in portfolios.items():
    fig_scatter.add_trace(go.Scatter(
        x=[props['risk']],
        y=[props['return']],
        mode='markers+text',
        name=name,
        marker=dict(
            size=10 + 0.2 * props['weight_A'],
            color=props['color'],
            symbol=props['symbol'],
            line=dict(color='white', width=2),
            opacity=0.85
        ),
        text=[name.split('(')[0].strip()],
        textposition='top center',
        textfont=dict(size=10, weight='bold'),
        hovertemplate=f"<b>{name}</b><br>E[r]: %{{y:.2f}}%<br>σ: %{{x:.2f}}%<br>Weight A: {props['weight_A']:.0f}%<extra></extra>"
    ))

# Add efficient frontier as background
fig_scatter.add_trace(go.Scatter(
    x=[s * 100 for s in stds_base],
    y=[r * 100 for r in returns_base],
    mode='lines',
    name='Efficient Frontier (ρ=0.3)',
    line=dict(color='rgba(52, 152, 219, 0.4)', width=8),
    hoverinfo='skip'
))

# Add quadrant labels
fig_scatter.add_annotation(x=11, y=15, text="High Return<br>Low Risk", 
                           font=dict(size=10, color='#27ae60'), showarrow=False)
fig_scatter.add_annotation(x=19, y=15, text="High Return<br>High Risk",
                           font=dict(size=10, color='#f39c12'), showarrow=False)
fig_scatter.add_annotation(x=11, y=10.5, text="Low Return<br>Low Risk",
                           font=dict(size=10, color='#3498db'), showarrow=False)
fig_scatter.add_annotation(x=19, y=10.5, text="Low Return<br>High Risk",
                           font=dict(size=10, color='#e74c3c'), showarrow=False)

fig_scatter.update_layout(
    title=dict(
        text="Portfolio Positioning: Risk vs. Return",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis_title="Portfolio Risk (σ) %",
    yaxis_title="Expected Return E[r] %",
    height=450,
    margin=dict(l=60, r=40, t=60, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    showlegend=False,
    hovermode='closest'
)

fig_scatter.update_xaxes(range=[9, 21], showgrid=True, gridcolor='rgba(0,0,0,0.15)')
fig_scatter.update_yaxes(range=[9.5, 16.5], showgrid=True, gridcolor='rgba(0,0,0,0.15)')

# Add quadrant dividers
fig_scatter.add_hline(y=13, line_dash='dot', line_color='rgba(0,0,0,0.2)')
fig_scatter.add_vline(x=15, line_dash='dot', line_color='rgba(0,0,0,0.2)')

fig_scatter.show()
```

</details>

---

## Implication for Risk-Averse Investors [Part b] {#sec-q3b}

<div class="answer-card">

#### ✓ Answer 3(b): Implication for Risk-Averse Investor

When correlation increases from 0.3 to 0.9, **a risk-averse investor faces a deteriorated risk-return tradeoff** and must make one of the following uncomfortable choices:

1. **Accept higher risk** for the same expected return, or
2. **Accept lower expected return** to maintain the same risk level, or
3. **Seek alternative assets** with lower correlation to improve diversification

**Specific Impact:**

At the Minimum Variance Portfolio:

| Scenario | MVP Return | MVP Volatility | Sharpe-like Ratio (E[r]/σ) |
|:---------|:----------:|:--------------:|:--------------------------:|
| ρ = 0.3 | `{python} mvp_ret_base_pct` | `{python} mvp_std_base_pct` | `{python} f"{mvp_return_base/mvp_std_base:.2f}"` |
| ρ = 0.9 | `{python} mvp_ret_high_pct` | `{python} mvp_std_high_pct` | `{python} f"{mvp_return_high/mvp_std_high:.2f}"` |

The risk-return efficiency ratio **decreases** as correlation rises, making it harder to achieve attractive risk-adjusted returns.

</div>

```{python}
#| label: risk-averse-visualization
#| echo: true

# Create visualization showing the impact on a risk-averse investor
fig_risk = make_subplots(
    rows=1, cols=2,
    subplot_titles=(
        "<b>Same Target Return: Higher Risk Required</b>",
        "<b>Same Target Risk: Lower Return Achieved</b>"
    ),
    horizontal_spacing=0.12
)

# Subplot 1: Same return, different risk
target_return = 0.12  # 12% target return

# Find weight that achieves target return (return is linear in weight)
# E[r] = w_A * E_A + (1-w_A) * E_B => w_A = (E[r] - E_B) / (E_A - E_B)
w_for_target_return = (target_return - E_B) / (E_A - E_B)
std_at_target_base = portfolio_std(w_for_target_return, sigma_A, sigma_B, rho_base)
std_at_target_high = portfolio_std(w_for_target_return, sigma_A, sigma_B, rho_high)

fig_risk.add_trace(go.Bar(
    x=['ρ = 0.3', 'ρ = 0.9'],
    y=[std_at_target_base * 100, std_at_target_high * 100],
    marker_color=[COLORS['primary_blue'], COLORS['negative']],
    text=[f"{std_at_target_base:.2%}", f"{std_at_target_high:.2%}"],
    textposition='outside',
    textfont=dict(size=12),
    hovertemplate="Correlation: %{x}<br>Risk: %{y:.2f}%<extra></extra>"
), row=1, col=1)

fig_risk.add_annotation(
    x=0.5, y=max(std_at_target_base, std_at_target_high) * 100 + 1,
    text=f"Target Return: {target_return:.0%}",
    showarrow=False,
    font=dict(size=10, color=COLORS['dark']),
    xref='x1', yref='y1'
)

# Subplot 2: Same risk, different return
target_risk = 0.13  # 13% target volatility

# Find weight that achieves target risk (need to solve quadratic)
def find_weight_for_risk(target_std, sigma_A, sigma_B, rho):
    """Find weight in A that achieves target volatility."""
    # σ² = w²σ_A² + (1-w)²σ_B² + 2w(1-w)σ_Aσ_Bρ = target²
    # Solve quadratic in w
    a = sigma_A**2 + sigma_B**2 - 2*sigma_A*sigma_B*rho
    b = 2*sigma_B**2*(-1) + 2*sigma_A*sigma_B*rho
    b = -2*sigma_B**2 + 2*sigma_A*sigma_B*rho
    c = sigma_B**2 - target_std**2
    
    # Quadratic formula
    discriminant = b**2 - 4*a*c
    if discriminant < 0:
        return None
    w1 = (-b + np.sqrt(discriminant)) / (2*a)
    w2 = (-b - np.sqrt(discriminant)) / (2*a)
    
    # Return valid weight in [0,1]
    for w in [w1, w2]:
        if 0 <= w <= 1:
            return w
    return None

w_for_risk_base = find_weight_for_risk(target_risk, sigma_A, sigma_B, rho_base)
w_for_risk_high = find_weight_for_risk(target_risk, sigma_A, sigma_B, rho_high)

ret_at_risk_base = portfolio_return(w_for_risk_base, E_A, E_B) if w_for_risk_base else None
ret_at_risk_high = portfolio_return(w_for_risk_high, E_A, E_B) if w_for_risk_high else None

if ret_at_risk_base and ret_at_risk_high:
    fig_risk.add_trace(go.Bar(
        x=['ρ = 0.3', 'ρ = 0.9'],
        y=[ret_at_risk_base * 100, ret_at_risk_high * 100],
        marker_color=[COLORS['primary_blue'], COLORS['negative']],
        text=[f"{ret_at_risk_base:.2%}", f"{ret_at_risk_high:.2%}"],
        textposition='outside',
        textfont=dict(size=12),
        hovertemplate="Correlation: %{x}<br>Return: %{y:.2f}%<extra></extra>"
    ), row=1, col=2)
    
    fig_risk.add_annotation(
        x=0.5, y=max(ret_at_risk_base, ret_at_risk_high) * 100 + 0.5,
        text=f"Target Risk: {target_risk:.0%}",
        showarrow=False,
        font=dict(size=10, color=COLORS['dark']),
        xref='x2', yref='y2'
    )

fig_risk.update_layout(
    title=dict(
        text="<b>Impact on Risk-Averse Investor: The Correlation Penalty</b>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    height=400,
    margin=dict(l=60, r=40, t=100, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    showlegend=False
)

fig_risk.update_yaxes(title_text="Portfolio Risk (%)", row=1, col=1)
fig_risk.update_yaxes(title_text="Expected Return (%)", row=1, col=2)

fig_risk.show()
```

::: {.callout-warning}
## The Correlation Trap
During market crises, correlations between assets tend to **spike toward 1** precisely when diversification is most needed. This phenomenon, sometimes called "correlation breakdown", means that portfolios designed under normal conditions may offer far less protection during turbulent markets than expected.

**Practical Implication:** Risk-averse investors should:

1. Stress-test portfolios assuming higher correlations
2. Consider assets with *structural* low correlation (e.g., different asset classes)
3. Use dynamic allocation strategies that respond to changing correlation regimes
:::

---

## Limitation: Estimation Error in Markowitz Optimization [Part c] {#sec-q3c}

<div class="answer-card">

#### ✓ Answer 3(c): Limitation from Estimation Error

**Limitation: Markowitz mean-variance optimization is highly sensitive to estimation errors in expected returns and covariances, leading to unstable and often extreme portfolio weights.**

Key manifestations:

1. **Extreme positions:** Small changes in estimated $\mu$ or $\Sigma$ can cause large swings in optimal weights
2. **Error maximization:** The optimizer tends to overweight assets with *overestimated* returns and underweight those with *underestimated* returns
3. **Out-of-sample degradation:** Portfolios that look optimal in-sample often underperform simple alternatives (like 1/N) out-of-sample
4. **Garbage in, garbage out:** Optimization amplifies estimation errors rather than averaging them out

</div>

### Monte Carlo Simulation: The Bootstrap Experiment

::: {.callout-note}
## Simulation Overview

**Research Question**: How much do optimal portfolio weights change when estimated from 5 years of historical data versus known population parameters?

**The Method (Monte Carlo Bootstrap Simulation)**: 

For demonstration, I ran a **Monte Carlo simulation** (repeated random sampling to quantify uncertainty) with the following steps:

1. **Set a random seed** of 2023014 so the simulation is reproducible
2. **Assume population parameters for the simulation:** Assets have expected returns of 10% and 16%, with volatilities of 12% and 20%, and correlation of 0.3
3. **Generate synthetic samples:** Randomly generate 60 months (5 years) of returns that follow these assumed parameters
4. **Estimate from limited data:** Calculate sample means and covariances from each 60-month sample (as would happen with finite historical data)
5. **Optimize using estimated inputs:** Use these estimates to compute the minimum-variance portfolio implied by the sample
6. **Repeat 1,000 times:** Each iteration represents a different possible 5-year history

This **Monte Carlo approach** lets us quantify how much error can arise even when the model is correctly specified; in this setup, the primary limitation is finite sample size.

**The Finding**: Even with perfect model specification, the MVP weight estimate has a **standard deviation of ~7 percentage points** due to estimation error alone. This means a quant using 5 years of data could arrive at significantly different "optimal" portfolios depending on which historical window they analyze. The "Ghost Frontier" visualization below shows how the efficient frontier blurs when data is noisy.
:::

To demonstrate this limitation empirically through Monte Carlo simulation, I ran the following experiment:

**Bootstrap Simulation Process:**

1. **True parameters:** $\mu_A = 10\%$, $\mu_B = 16\%$, $\sigma_A = 12\%$, $\sigma_B = 20\%$, $\rho = 0.3$ (the assumed "ground truth")
2. **Generate random samples:** Draw 60 months (5 years) of returns from a multivariate normal distribution with these true parameters
3. **Estimate from limited data:** Compute sample mean and covariance from each 60-month sample (mimicking a real analyst's situation)
4. **Optimize with noisy estimates:** Calculate MVP using the estimated (not true) parameters
5. **Repeat with Monte Carlo:** Run 1,000 independent iterations, each with a different random 60-month sample
6. **Random seed set:** Ensures reproducibility of all random number generation

```{python}
#| label: bootstrap-histogram
#| echo: true

# Create histogram of estimated optimal weights
fig_bootstrap = go.Figure()

fig_bootstrap.add_trace(go.Histogram(
    x=estimated_weights_A * 100,
    nbinsx=40,
    marker_color=COLORS['primary_blue'],
    opacity=0.7,
    name='Estimated Weights',
    hovertemplate="Weight in A: %{x:.1f}%<br>Count: %{y}<extra></extra>"
))

# Add true optimal weight line
fig_bootstrap.add_vline(
    x=true_w_A * 100,
    line_dash="solid",
    line_color=COLORS['positive'],
    line_width=3,
    annotation_text=f"True Optimal: {true_w_A:.1%}",
    annotation_position="top",
    annotation_font_size=11,
    annotation_font_color=COLORS['positive']
)

# Add mean estimated weight line
fig_bootstrap.add_vline(
    x=bootstrap_mean_wA * 100,
    line_dash="dash",
    line_color=COLORS['negative'],
    line_width=2,
    annotation_text=f"Mean Estimated: {bootstrap_mean_wA:.1%}",
    annotation_position="top left",
    annotation_font_size=11,
    annotation_font_color=COLORS['negative']
)

# Add 1/N weight for reference
fig_bootstrap.add_vline(
    x=50,
    line_dash="dot",
    line_color=COLORS['warning'],
    line_width=2,
    annotation_text="1/N (50%)",
    annotation_position="bottom",
    annotation_font_size=10,
    annotation_font_color=COLORS['warning']
)

fig_bootstrap.update_layout(
    title=dict(
        text="<b>Bootstrap Distribution of Estimated Optimal Weights</b><br><sup>1,000 iterations with 60-month samples from true distribution</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis_title="Weight in Asset A (%)",
    yaxis_title="Frequency",
    height=420,
    margin=dict(l=60, r=40, t=80, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    showlegend=False,
    bargap=0.05
)

fig_bootstrap.update_xaxes(range=[0, 100], showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_bootstrap.update_yaxes(showgrid=True, gridcolor='rgba(0,0,0,0.1)')

fig_bootstrap.show()
```

```{python}
#| label: bootstrap-stats
#| echo: true

# Create bootstrap statistics table
bootstrap_stats = {
    'Statistic': ['True Optimal Weight in A', 'Mean Estimated Weight', 'Std Dev of Estimates',
                  'Minimum Estimate', 'Maximum Estimate', 'Range of Estimates',
                  '% of estimates within ±10pp of true'],
    'Value': [
        f"{true_w_A:.1%}",
        f"{bootstrap_mean_wA:.1%}",
        f"{bootstrap_std_wA:.1%}",
        f"{bootstrap_min_wA:.1%}",
        f"{bootstrap_max_wA:.1%}",
        f"{(bootstrap_max_wA - bootstrap_min_wA):.1%}",
        f"{np.mean(np.abs(estimated_weights_A - true_w_A) < 0.10):.1%}"
    ]
}

bootstrap_df = pd.DataFrame(bootstrap_stats)

fig_boot_stats = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Statistic</b>', '<b>Value</b>'],
        fill_color=COLORS['dark'],
        font=dict(color='white', size=12),
        align='left',
        height=35
    ),
    cells=dict(
        values=[bootstrap_df[col] for col in bootstrap_df.columns],
        fill_color=[COLORS['light_bg'], 'white'],
        font=dict(size=11),
        align='left',
        height=30
    )
)])

fig_boot_stats.update_layout(
    title=dict(text="<b>Bootstrap Simulation Results</b>", x=0.5, font=dict(size=13)),
    height=310,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_boot_stats.show()
```

::: {.callout-important}
## The Estimation Error Problem: Quantified

Even with **5 years of monthly data** (a substantial sample by practical standards):

- Estimated optimal weights range from **`{python} f"{bootstrap_min_wA:.0%}"`** to **`{python} f"{bootstrap_max_wA:.0%}"`**, a spread of `{python} f"{(bootstrap_max_wA - bootstrap_min_wA):.0%}"`.
- The standard deviation of estimated weights is **`{python} bootstrap_std_pct`**
- Only **`{python} f"{np.mean(np.abs(estimated_weights_A - true_w_A) < 0.10):.0%}"`** of estimates fall within 10 percentage points of the true optimal

This level of variability means that two analysts using different 5-year windows could arrive at **materially different "optimal" portfolios**, even when the true parameters remain constant.
:::

### The "Ghost Frontier": Visualizing Estimation Uncertainty

The histogram above shows weight variability, but **the most informative visualization** of estimation error is the "Ghost Frontier", which overlays multiple efficient frontiers computed from bootstrap samples. This reveals how the frontier itself shifts when estimated from finite data.

```{python}
#| label: ghost-frontier
#| echo: true

# ============================================================
# GHOST FRONTIER: Estimation Uncertainty Visualization
# ============================================================
# This spaghetti plot shows 100 efficient frontiers computed from 
# bootstrap samples, revealing the inherent uncertainty in 
# Markowitz optimization when parameters are estimated from data.

np.random.seed(2023014)

# Number of ghost frontiers to plot
n_ghost = 100

# Storage for ghost frontier data
ghost_frontiers = []

# Generate ghost frontiers from bootstrap samples
for i in range(n_ghost):
    # Generate sample returns from true distribution
    sample_returns = np.random.multivariate_normal(true_mu/12, true_cov/12, size=60)
    
    # Estimate parameters from sample
    est_mu = np.mean(sample_returns, axis=0) * 12  # Annualize
    est_cov = np.cov(sample_returns.T) * 12  # Annualize
    
    # Extract estimated parameters
    est_sigma_A = np.sqrt(est_cov[0, 0])
    est_sigma_B = np.sqrt(est_cov[1, 1])
    est_rho = est_cov[0, 1] / (est_sigma_A * est_sigma_B) if est_sigma_A > 0 and est_sigma_B > 0 else 0
    
    # Generate frontier with estimated parameters
    weights = np.linspace(0, 1, 50)
    est_returns = [w * est_mu[0] + (1-w) * est_mu[1] for w in weights]
    est_stds = [np.sqrt(w**2 * est_cov[0,0] + (1-w)**2 * est_cov[1,1] + 
                        2*w*(1-w)*est_cov[0,1]) for w in weights]
    
    ghost_frontiers.append((est_returns, est_stds))

# Create the Ghost Frontier visualization
fig_ghost = go.Figure()

# Add all ghost frontiers with low opacity (the "ghosts")
for i, (est_rets, est_stds) in enumerate(ghost_frontiers):
    fig_ghost.add_trace(go.Scatter(
        x=[s * 100 for s in est_stds],
        y=[r * 100 for r in est_rets],
        mode='lines',
        line=dict(color='rgba(52, 152, 219, 0.08)', width=1.5),
        showlegend=(i == 0),
        name='Estimated Frontiers (n=100)',
        hoverinfo='skip'
    ))

# Add the TRUE efficient frontier (bold, distinct)
true_stds = [portfolio_std(w, sigma_A, sigma_B, rho_base) for w in weights_base]
true_rets = [portfolio_return(w, E_A, E_B) for w in weights_base]

fig_ghost.add_trace(go.Scatter(
    x=[s * 100 for s in true_stds],
    y=[r * 100 for r in true_rets],
    mode='lines',
    line=dict(color='#E74C3C', width=4),
    name='TRUE Frontier (Known Parameters)',
    hovertemplate="<b>True Frontier</b><br>σ = %{x:.2f}%<br>E[r] = %{y:.2f}%<extra></extra>"
))

# Add Asset markers
fig_ghost.add_trace(go.Scatter(
    x=[sigma_A * 100, sigma_B * 100],
    y=[E_A * 100, E_B * 100],
    mode='markers+text',
    marker=dict(color=[COLORS['positive'], COLORS['warning']], size=16, symbol='circle',
               line=dict(color='white', width=2)),
    text=['A', 'B'],
    textposition=['bottom left', 'top right'],
    textfont=dict(size=14, weight='bold'),
    name='Assets',
    hovertemplate="<b>Asset %{text}</b><br>σ = %{x:.1f}%<br>E[r] = %{y:.1f}%<extra></extra>"
))

# Add annotation explaining the visualization
fig_ghost.add_annotation(
    x=18, y=11,
    text="<b>Each faint blue line</b> is an efficient<br>frontier estimated from a 5-year<br>sample. The <b>red line</b> is the true<br>frontier (known only in simulation).",
    showarrow=False,
    font=dict(size=11, color=COLORS['dark']),
    align='left',
    bgcolor='rgba(255,255,255,0.9)',
    bordercolor='rgba(0,0,0,0.1)',
    borderwidth=1,
    borderpad=8
)

fig_ghost.update_layout(
    title=dict(
        text="<b>The 'Ghost Frontier': How Estimation Error Blurs Portfolio Optimization</b><br><sup>100 efficient frontiers estimated from 60-month bootstrap samples vs. the true frontier</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    xaxis_title="Portfolio Risk (σ) %",
    yaxis_title="Expected Return E[r] %",
    height=520,
    margin=dict(l=60, r=40, t=100, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(
        orientation="h",
        yanchor="bottom",
        y=-0.18,
        xanchor="center",
        x=0.5,
        bgcolor='rgba(255,255,255,0.9)'
    ),
    hovermode='closest'
)

fig_ghost.update_xaxes(range=[5, 28], showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_ghost.update_yaxes(range=[6, 20], showgrid=True, gridcolor='rgba(0,0,0,0.1)')

fig_ghost.show()
```

::: {.callout-warning}
## The Ghost Frontier
The visualization above provides a clear illustration of **why Markowitz optimization can be unstable in practice**:

1. **The cloud of blue lines** represents 100 different "optimal" frontiers, each one computed from a different 5-year sample.
2. **The red line** shows the true frontier (which is unknown in practice).
3. **The spread is substantial**; estimated frontiers can shift left, right, up, or down.

This helps explain why practitioners sometimes summarize mean-variance optimization as "garbage in, garbage out". The mathematical structure of mean-variance optimization is highly sensitive to the **uncertainty in estimating its inputs**.
:::


:::: {.callout-note collapse="true"}
## Why Estimation Error Is So Damaging

### The "Error Maximization" Property

Michaud (1989) famously characterized mean-variance optimization as an "error maximizer" because:

1. **Optimizer seeks extremes:** MVO finds portfolios at corners/edges of the feasible region
2. **Errors compound:** Small errors in $\mu$ or $\Sigma$ are amplified, not averaged
3. **Inverse problem:** Small covariance matrices are nearly singular, making inversion unstable

### Mathematical Insight

The optimal portfolio weights depend on $\Sigma^{-1}\mu$. Estimation errors in both components interact:

$$
\hat{w}^* = \frac{\hat{\Sigma}^{-1} \hat{\mu}}{\mathbf{1}'\hat{\Sigma}^{-1} \hat{\mu}}
$$

- Error in $\hat{\mu}$: Expected returns are notoriously hard to estimate. A 2% annual estimation error is common.
- Error in $\hat{\Sigma}$: With $n$ assets, the covariance matrix has $n(n+1)/2$ parameters to estimate.
- Combined effect: Errors in both numerator and denominator create highly unstable ratios.

### Practical Solutions

Several approaches have been developed to address this limitation:

| Approach | Description and Mechanism |
|:---------|:--------------------------|
| **Shrinkage Estimators** | Instead of trusting the sample covariance matrix completely, blend it with a simpler, more stable estimate. Think of it as pulling extreme estimates back toward something sensible. Example: Ledoit-Wolf shrinkage. |
| **Bayesian Methods** | Start with reasonable "prior beliefs" about expected returns (e.g., from market equilibrium), then adjust them modestly based on observed data rather than replacing them entirely. Example: Black-Litterman model. |
| **Resampled Efficiency** | Generate many plausible datasets from the original sample, optimize on each, then average the results. This smooths out the instability caused by any single noisy estimate. |
| **Robust Optimization** | Instead of finding the "best" portfolio for a single point estimate, find one that performs reasonably well across a range of plausible parameter values (accounting for estimation uncertainty). |
| **1/N Portfolio** | Simply put equal amounts in each asset and ignore all the optimization. Sounds crude, but often outperforms optimized portfolios when estimation error is high. |

```{python}
#| label: solution-comparison
#| echo: true

# Compare optimization approaches
approaches = {
    'Approach': ['Mean-Variance (Sample)', 'Shrinkage Covariance', '1/N Equal Weight', 'MVP Only'],
    'Uses μ estimates': ['Yes', 'Yes', 'No', 'No'],
    'Uses Σ estimates': ['Yes', 'Yes (shrunk)', 'No', 'Yes'],
    'Estimation sensitivity': ['Very High', 'Moderate', 'None', 'Moderate'],
    'Out-of-sample performance': ['Often poor', 'Better', 'Surprisingly good', 'Good']
}

app_df = pd.DataFrame(approaches)

fig_approaches = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>' + col + '</b>' for col in app_df.columns],
        fill_color=COLORS['purple'],
        font=dict(color='white', size=10),
        align='left',
        height=35
    ),
    cells=dict(
        values=[app_df[col] for col in app_df.columns],
        fill_color=['#faf8fc'] + ['white'] * 3,
        font=dict(size=10),
        align='left',
        height=28
    )
)])

fig_approaches.update_layout(
    title=dict(text="<b>Alternative Portfolio Construction Approaches</b>", x=0.5, font=dict(size=12)),
    height=230,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_approaches.show()
```

::: {.callout-tip}
## Practical Takeaway
DeMiguel, Garlappi, and Uppal (2009) showed that the simple **1/N portfolio** often outperforms sophisticated optimization strategies out-of-sample, especially when:

- Sample sizes are small
- Number of assets is large
- True parameters are unstable over time

The message is clear: **simplicity and robustness often beat theoretical optimality** in the presence of estimation uncertainty.
:::

::::

---

## Summary: Portfolio Theory Insights

| Concept | Key Finding |
|:--------|:------------|
| **Correlation & Diversification** | Higher ρ → flatter frontier → reduced risk reduction potential |
| **MVP Shift** | MVP weight shifts toward lower-volatility asset as ρ increases |
| **Risk-Averse Investor** | Must accept worse risk-return tradeoffs when correlations rise |
| **Estimation Error** | Optimal weights are highly sensitive to parameter estimation errors |
| **Practical Wisdom** | Simple strategies (1/N) often outperform "optimal" ones out-of-sample |

::: {.callout-warning}
## Practical Note: Correlations can rise in stress
This analysis treats correlation as fixed. In practice, correlations often rise during market stress, which reduces diversification benefits. A reasonable way to handle this is to stress-test the portfolio with higher correlations (e.g., ρ near 0.8 to 0.9) and check how much the risk reduction deteriorates.
:::

---

<hr class="section-divider">

# Conclusion {.unnumbered}

This report brings together three core domains of quantitative portfolio management:

::: {.callout-note appearance="simple"}
## Summary of Findings

**Question 1:** Identified that Product A is **best reported as Model 2, with Model 1 also plausible at the reporting precision** because both return series are nearly constant and the constant-growth RMSE is very low. Product B exhibits **variable returns** and is classified as Model 3.

**Question 2:** Explained **continuous compounding** and why log returns add nicely over time, while noting that real-world cash flows and discrete pricing can create small (but real) mismatches.

**Question 3:** Showed how **higher correlation reduces diversification benefits** and shifts the MVP toward the lower-volatility asset. The bootstrap simulation showed how sensitive “optimal” weights can be to estimation error.
:::

### Key Takeaways for Practice

1. **Return conventions matter:** Always be explicit about whether simple or log returns are being used, especially for multi-period analysis.

2. **Continuous compounding is a useful approximation:** It provides analytical tractability at minimal cost in accuracy for most applications.

3. **Correlation is destiny for diversification:** Portfolio construction should stress-test against correlation increases, particularly during market stress.

4. **Beware of false precision:** Sophisticated optimization techniques may underperform simple heuristics when parameter uncertainty is high.

---

*Report generated on {{< meta date >}}*

---

<hr class="section-divider">

# Appendices {.unnumbered}

The following appendices contain supplementary technical details, extended derivations, and additional analyses that support the main findings but are not essential for understanding the core answers.

<details id="appendix-volatility-drag" markdown="1">
<summary style="font-size: 1.3em; font-weight: bold; cursor: pointer; padding: 12px 15px; background-color: #fef2f2; border-left: 5px solid #e74c3c; margin-bottom: 15px; border-radius: 4px;">Appendix A: 10-Year Volatility Drag Simulation (Q1g Extended) <em>(click to expand)</em></summary>

## A.1 Extended Monte Carlo Analysis: Volatility Drag at Scale

The 4-quarter example in Q1(g) showed the volatility drag phenomenon in miniature. This appendix presents a **Monte Carlo simulation illustrating how volatility drag can accumulate over a 10-year horizon**.

### Simulation Setup

```{python}
#| label: appendix-volatility-drag-monte-carlo
#| echo: true

# ============================================================
# 10-YEAR MONTE CARLO: Volatility Drag Simulation
# ============================================================
# Simulate 1,000 paths over 10 years for two portfolios with 
# IDENTICAL expected returns but different volatilities.
# Uses Geometric Brownian Motion (GBM) for realistic modeling.

np.random.seed(2023014)

# Simulation parameters
n_paths_app = 1000
n_years_app = 10
n_months_app = n_years_app * 12
dt_app = 1/12  # Monthly time step

# Both portfolios have same expected return (10% annually)
mu_app = 0.10  # 10% expected annual return

# Different volatilities
sigma_low_app = 0.05   # 5% annual volatility (very stable, like bonds)
sigma_high_app = 0.25  # 25% annual volatility (like equities)

# Initial investment
V0_sim_app = 100000

# Generate paths using GBM: dS = μSdt + σSdW
# Discrete: S(t+dt) = S(t) * exp((μ - σ²/2)dt + σ√dt * Z)

# Generate random shocks (same for fair comparison of drift)
Z_app = np.random.standard_normal((n_paths_app, n_months_app))

# Low volatility paths
low_vol_paths_app = np.zeros((n_paths_app, n_months_app + 1))
low_vol_paths_app[:, 0] = V0_sim_app
for t in range(n_months_app):
    drift = (mu_app - 0.5 * sigma_low_app**2) * dt_app
    diffusion = sigma_low_app * np.sqrt(dt_app) * Z_app[:, t]
    low_vol_paths_app[:, t+1] = low_vol_paths_app[:, t] * np.exp(drift + diffusion)

# High volatility paths
high_vol_paths_app = np.zeros((n_paths_app, n_months_app + 1))
high_vol_paths_app[:, 0] = V0_sim_app
for t in range(n_months_app):
    drift = (mu_app - 0.5 * sigma_high_app**2) * dt_app
    diffusion = sigma_high_app * np.sqrt(dt_app) * Z_app[:, t]
    high_vol_paths_app[:, t+1] = high_vol_paths_app[:, t] * np.exp(drift + diffusion)

# Calculate percentiles for fan chart
percentiles_app = [5, 10, 25, 50, 75, 90, 95]
months_axis_app = np.arange(n_months_app + 1)
years_axis_app = months_axis_app / 12

low_vol_pcts_app = np.percentile(low_vol_paths_app, percentiles_app, axis=0)
high_vol_pcts_app = np.percentile(high_vol_paths_app, percentiles_app, axis=0)

# Select representative sample paths for density visualization
np.random.seed(2023015)
sample_paths_app = 200
sample_indices_app = np.random.choice(n_paths_app, sample_paths_app, replace=False)
```

### Visualization: Fan Chart with Uncertainty Bands

```{python}
#| label: appendix-mc-fan-chart
#| echo: true

# Create enhanced subplot layout with histogram
fig_mc_app = make_subplots(
    rows=1, cols=2,
    column_widths=[0.72, 0.28],
    subplot_titles=(
        "<b>Monte Carlo Wealth Trajectories</b><br><sup>1,000 simulated paths per portfolio; 200 shown as faint paths + percentile bands</sup>",
        "<b>Final Distribution</b>"
    ),
    horizontal_spacing=0.06
)

# ========== LEFT PANEL: Fan chart with sample paths ==========

# Low volatility - layered fan (more granular percentile bands)
# 5th-95th percentile band (lightest)
fig_mc_app.add_trace(go.Scatter(
    x=np.concatenate([years_axis_app, years_axis_app[::-1]]),
    y=np.concatenate([low_vol_pcts_app[0], low_vol_pcts_app[6][::-1]]),
    fill='toself',
    fillcolor='rgba(39, 174, 96, 0.08)',
    line=dict(color='rgba(0,0,0,0)'),
    name='Low Vol (5th-95th)',
    showlegend=False,
    hoverinfo='skip'
), row=1, col=1)

# 10th-90th percentile band
fig_mc_app.add_trace(go.Scatter(
    x=np.concatenate([years_axis_app, years_axis_app[::-1]]),
    y=np.concatenate([low_vol_pcts_app[1], low_vol_pcts_app[5][::-1]]),
    fill='toself',
    fillcolor='rgba(39, 174, 96, 0.15)',
    line=dict(color='rgba(0,0,0,0)'),
    showlegend=False,
    hoverinfo='skip'
), row=1, col=1)

# 25th-75th percentile band
fig_mc_app.add_trace(go.Scatter(
    x=np.concatenate([years_axis_app, years_axis_app[::-1]]),
    y=np.concatenate([low_vol_pcts_app[2], low_vol_pcts_app[4][::-1]]),
    fill='toself',
    fillcolor='rgba(39, 174, 96, 0.25)',
    line=dict(color='rgba(0,0,0,0)'),
    showlegend=False,
    hoverinfo='skip'
), row=1, col=1)

# Add sample paths for low volatility (ghost lines)
for idx in sample_indices_app:
    fig_mc_app.add_trace(go.Scatter(
        x=years_axis_app,
        y=low_vol_paths_app[idx],
        mode='lines',
        line=dict(color='rgba(39, 174, 96, 0.08)', width=0.5),
        showlegend=False,
        hoverinfo='skip'
    ), row=1, col=1)

# Median line (bold)
fig_mc_app.add_trace(go.Scatter(
    x=years_axis_app,
    y=low_vol_pcts_app[3],
    mode='lines',
    name=f'Low Volatility (σ={sigma_low_app:.0%})',
    line=dict(color=COLORS['positive'], width=4),
    hovertemplate="Year %{x:.1f}<br>Median: ₹%{y:,.0f}<extra>Low Vol</extra>"
), row=1, col=1)

# High volatility - layered fan
# 5th-95th percentile band (lightest)
fig_mc_app.add_trace(go.Scatter(
    x=np.concatenate([years_axis_app, years_axis_app[::-1]]),
    y=np.concatenate([high_vol_pcts_app[0], high_vol_pcts_app[6][::-1]]),
    fill='toself',
    fillcolor='rgba(231, 76, 60, 0.08)',
    line=dict(color='rgba(0,0,0,0)'),
    showlegend=False,
    hoverinfo='skip'
), row=1, col=1)

# 10th-90th percentile band
fig_mc_app.add_trace(go.Scatter(
    x=np.concatenate([years_axis_app, years_axis_app[::-1]]),
    y=np.concatenate([high_vol_pcts_app[1], high_vol_pcts_app[5][::-1]]),
    fill='toself',
    fillcolor='rgba(231, 76, 60, 0.15)',
    line=dict(color='rgba(0,0,0,0)'),
    showlegend=False,
    hoverinfo='skip'
), row=1, col=1)

# 25th-75th percentile band  
fig_mc_app.add_trace(go.Scatter(
    x=np.concatenate([years_axis_app, years_axis_app[::-1]]),
    y=np.concatenate([high_vol_pcts_app[2], high_vol_pcts_app[4][::-1]]),
    fill='toself',
    fillcolor='rgba(231, 76, 60, 0.25)',
    line=dict(color='rgba(0,0,0,0)'),
    showlegend=False,
    hoverinfo='skip'
), row=1, col=1)

# Add sample paths for high volatility (ghost lines)
for idx in sample_indices_app:
    fig_mc_app.add_trace(go.Scatter(
        x=years_axis_app,
        y=high_vol_paths_app[idx],
        mode='lines',
        line=dict(color='rgba(231, 76, 60, 0.08)', width=0.5),
        showlegend=False,
        hoverinfo='skip'
    ), row=1, col=1)

# Median line (bold)
fig_mc_app.add_trace(go.Scatter(
    x=years_axis_app,
    y=high_vol_pcts_app[3],
    mode='lines',
    name=f'High Volatility (σ={sigma_high_app:.0%})',
    line=dict(color=COLORS['negative'], width=4),
    hovertemplate="Year %{x:.1f}<br>Median: ₹%{y:,.0f}<extra>High Vol</extra>"
), row=1, col=1)

# Add theoretical "no volatility drag" line for reference
no_drag_line_app = V0_sim_app * np.exp(mu_app * years_axis_app)
fig_mc_app.add_trace(go.Scatter(
    x=years_axis_app,
    y=no_drag_line_app,
    mode='lines',
    name='Expected (μ=10%, no drag)',
    line=dict(color=COLORS['neutral'], width=2, dash='dash'),
    hovertemplate="Year %{x:.1f}<br>Expected: ₹%{y:,.0f}<extra>No Drag</extra>"
), row=1, col=1)

# ========== RIGHT PANEL: Final wealth histogram ==========
low_final_vals_app = low_vol_paths_app[:, -1]
high_final_vals_app = high_vol_paths_app[:, -1]

fig_mc_app.add_trace(go.Histogram(
    y=low_final_vals_app,
    nbinsy=35,
    orientation='h',
    name='Low Vol Final',
    marker_color='rgba(39, 174, 96, 0.6)',
    showlegend=False,
    hovertemplate="₹%{y:,.0f}<br>Count: %{x}<extra></extra>"
), row=1, col=2)

fig_mc_app.add_trace(go.Histogram(
    y=high_final_vals_app,
    nbinsy=35,
    orientation='h',
    name='High Vol Final',
    marker_color='rgba(231, 76, 60, 0.6)',
    showlegend=False,
    hovertemplate="₹%{y:,.0f}<br>Count: %{x}<extra></extra>"
), row=1, col=2)

# Calculate metrics for annotations
median_low_final_app = low_vol_pcts_app[3, -1]
median_high_final_app = high_vol_pcts_app[3, -1]
expected_final_app = no_drag_line_app[-1]
vol_drag_amount_app = median_low_final_app - median_high_final_app

# Final layout adjustments
fig_mc_app.update_layout(
    height=550,
    margin=dict(l=60, r=20, t=100, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(
        orientation="h",
        yanchor="bottom",
        y=1.02,
        xanchor="center",
        x=0.35
    )
)

fig_mc_app.update_xaxes(title_text="Year", row=1, col=1, showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_mc_app.update_yaxes(title_text="Portfolio Value (₹)", row=1, col=1, 
                        tickformat=",", tickprefix="₹", showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_mc_app.update_xaxes(title_text="Frequency", row=1, col=2, showgrid=True, gridcolor='rgba(0,0,0,0.1)')
fig_mc_app.update_yaxes(title_text="", row=1, col=2, tickformat=",", tickprefix="₹", showgrid=False)

fig_mc_app.show()
```

### Key Findings from Monte Carlo Simulation

::: {.callout-important}
## Critical Insights from 10-Year Simulation

1. **Volatility drag is devastating over time**: Despite identical 10% expected returns, the high-volatility portfolio's median outcome is **₹`{python} f"{int(vol_drag_amount_app/1000)}k"`** lower after 10 years
1. **Volatility drag is material over time**: Despite identical 10% expected returns, the high-volatility portfolio's median outcome is **₹`{python} f"{int(vol_drag_amount_app/1000)}k"`** lower after 10 years

2. **The formula works**: Theoretical volatility drag = $\\frac{\\sigma^2}{2}$ predicts:
   - Low vol: $0.10 - \\frac{0.05^2}{2} = 9.875\\%$ geometric return
   - High vol: $0.10 - \\frac{0.25^2}{2} = 6.875\\%$ geometric return
    - Difference: ~3% annually compounds to a large wealth gap

3. **Downside risk amplified**: The 5th percentile outcome (worst 5%) is **far worse** for high volatility, showing tail risk

4. **Why log returns matter**: This simulation illustrates why log returns are often preferred; they correctly capture the compounding penalty from volatility
:::

### Implications for Portfolio Management

This extended analysis demonstrates several critical lessons:

1. **Arithmetic mean misleads**: Reporting that both portfolios "earned 10% on average" hides the compounding reality
2. **Volatility is not free**: Even when not constraining downside, variance reduces terminal wealth through path dependency
3. **Sharpe ratio superiority**: A portfolio with lower return but much lower volatility can dominate a high-return/high-volatility alternative over long horizons
4. **Kelly criterion relevance**: Optimal bet sizing must account for volatility drag, not just expected return

</details>

<details id="appendix-residual-ribbon" markdown="1">
<summary style="font-size: 1.3em; font-weight: bold; cursor: pointer; padding: 12px 15px; background-color: #eef6ff; border-left: 5px solid #3498db; margin-bottom: 15px; border-radius: 4px;">Appendix B: Residual Ribbon Analysis (Model Fit Visualization) <em>(click to expand)</em></summary>

<div class="appendix-content">

**B.1 What Are Residuals?**

One clean way to compare models is to examine the **residuals** (actual − predicted). A good fit produces residuals that are small and roughly centered around zero.

**B.2 Residual Ribbon Chart: Visual Model Comparison**

The following visualization shows the actual values with **residual ribbons**, the shaded area between the model fit and actual data points. A tighter ribbon means a better fit.

```{python}
#| label: residual-ribbon-chart
#| echo: true

# ============================================================
# RESIDUAL RIBBON CHART: Visual Model Comparison
# ============================================================
# This visualization shows the "residual ribbon", the area between
# fitted values and actual values. Tighter ribbons = better fits.

fig_ribbon = make_subplots(
    rows=2, cols=2,
    subplot_titles=(
        "<b>Product A: Model 1 Fit</b>",
        "<b>Product A: Model 2 Fit</b>",
        "<b>Product B: Model 1 Fit</b>",
        "<b>Product B: Model 2 Fit</b>"
    ),
    vertical_spacing=0.15,
    horizontal_spacing=0.08
)

months_labels = ['0', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

# Function to add ribbon trace
def add_ribbon_subplot(fig, actual, predicted, row, col, color, model_name, rmse):
    # Upper bound (max of actual, predicted at each point)
    upper = np.maximum(actual, predicted)
    lower = np.minimum(actual, predicted)
    
    # Add the ribbon (shaded area between actual and predicted)
    fig.add_trace(go.Scatter(
        x=list(range(13)) + list(range(12, -1, -1)),
        y=list(upper) + list(lower[::-1]),
        fill='toself',
        fillcolor=f'rgba({color[0]}, {color[1]}, {color[2]}, 0.3)',
        line=dict(color='rgba(0,0,0,0)'),
        showlegend=False,
        hoverinfo='skip'
    ), row=row, col=col)
    
    # Add actual values
    fig.add_trace(go.Scatter(
        x=list(range(13)),
        y=actual,
        mode='markers',
        marker=dict(color='#2C3E50', size=8, symbol='circle'),
        name='Actual',
        showlegend=(row==1 and col==1),
        hovertemplate="Actual: ₹%{y:,.0f}<extra></extra>"
    ), row=row, col=col)
    
    # Add predicted line
    fig.add_trace(go.Scatter(
        x=list(range(13)),
        y=predicted,
        mode='lines',
        line=dict(color=f'rgb({color[0]}, {color[1]}, {color[2]})', width=2),
        name=model_name,
        showlegend=(row==1 and col==1),
        hovertemplate=f"{model_name}: ₹%{{y:,.0f}}<extra></extra>"
    ), row=row, col=col)
    
    # Add RMSE annotation
    fig.add_annotation(
        xref=f'x{(row-1)*2 + col}', yref=f'y{(row-1)*2 + col}',
        x=6, y=min(actual) - (max(actual) - min(actual)) * 0.15,
        text=f"RMSE: ₹{rmse:.2f}",
        showarrow=False,
        font=dict(size=10, color=f'rgb({color[0]}, {color[1]}, {color[2]})', weight='bold'),
        bgcolor='rgba(255,255,255,0.8)'
    )

# Product A - Model 1 (row=1, col=1) - Orange
add_ribbon_subplot(fig_ribbon, product_a_values, predicted_model1_a, 1, 1, 
                   (230, 126, 34), 'Model 1', rmse_model1_a)

# Product A - Model 2 (row=1, col=2) - Green  
add_ribbon_subplot(fig_ribbon, product_a_values, predicted_model2_a, 1, 2,
                   (39, 174, 96), 'Model 2', rmse_model2_a)

# Product B - Model 1 (row=2, col=1) - Orange
add_ribbon_subplot(fig_ribbon, product_b_values, predicted_model1_b, 2, 1,
                   (230, 126, 34), 'Model 1', rmse_model1_b)

# Product B - Model 2 (row=2, col=2) - Green
add_ribbon_subplot(fig_ribbon, product_b_values, predicted_model2_b, 2, 2,
                   (39, 174, 96), 'Model 2', rmse_model2_b)

fig_ribbon.update_layout(
    title=dict(
        text="<b>Residual Ribbon Analysis: Model Fit Quality</b><br><sup>Shaded area = deviation from actual values. Smaller ribbons indicate better fits.</sup>",
        x=0.5,
        font=dict(size=14, color=COLORS['dark'])
    ),
    height=550,
    margin=dict(l=60, r=40, t=100, b=60),
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(248,249,250,1)',
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
)

fig_ribbon.update_xaxes(showgrid=True, gridcolor='rgba(0,0,0,0.1)', title_text="Month")
fig_ribbon.update_yaxes(showgrid=True, gridcolor='rgba(0,0,0,0.1)', tickformat=",")

fig_ribbon.show()
```

::: {.callout-tip}
## Visual Insight: The Ribbon Tells the Story
**For Product A (top row):** The constant-growth ribbon is **very tight**; deviations are small relative to the ₹100,000 scale. This indicates strong (though not perfect) alignment with constant-growth dynamics.

**For Product B (bottom row):** Both ribbons are visible and similar in size, confirming that neither model captures the variable return pattern well. This is consistent with the Model 3 (Variable Returns) classification.
:::

**B.3 Key Takeaway: Why Residual Analysis Matters**

The residual ribbon chart provides an intuitive visual confirmation of what the numerical error metrics already told us:

1. **Product A exhibits near-constant growth**: Both Model 1 and Model 2 produce very tight ribbons, but if we are being pedantic, model-2 better describes A due to it being a marginally better fit.

2. **Product B shows variable returns**: Neither model fits well, as evidenced by wider, more visible ribbons. The deviations are not random noise but reflect genuine variation in monthly returns.

3. **Visual validation complements numerical metrics**: While RMSE provides a quantitative measure of fit quality, the ribbon chart makes the difference immediately obvious to the eye.

This type of residual analysis is standard practice in model validation. A good model should produce residuals that are:
- Small in magnitude (tight ribbon)
- Randomly distributed (no systematic pattern)
- Homoscedastic (constant variance over time)

For Product A, we observe all three properties. For Product B, the systematic deviations indicate that constant-growth models are inappropriate.

</div>
</details>

<details markdown="1">
<summary style="font-size: 1.3em; font-weight: bold; cursor: pointer; padding: 12px 15px; background-color: #ecf0f1; border-left: 5px solid #2c3e50; margin-bottom: 15px; border-radius: 4px;">Appendix C: Raw Data & Complete Calculations <em>(click to expand)</em></summary>

#### C.1 Product Data Summary {.unnumbered}

```{python}
#| label: appendix-raw-data
#| echo: true

# Create comprehensive raw data table
raw_data = {
    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],
    'Product A Value': [f"₹{product_a_values[i]:,.0f}" for i in range(1, 13)],
    'Product B Value': [f"₹{product_b_values[i]:,.0f}" for i in range(1, 13)],
    'A Simple Return': [f"{r:.4f}" for r in monthly_discrete_a],
    'B Simple Return': [f"{r:.4f}" for r in monthly_discrete_b],
    'A Log Return': [f"{r:.6f}" for r in monthly_log_a],
    'B Log Return': [f"{r:.6f}" for r in monthly_log_b]
}

raw_df = pd.DataFrame(raw_data)

fig_raw = go.Figure(data=[go.Table(
    header=dict(
        values=['<b>Month</b>', '<b>Product A<br>Value</b>', '<b>Product B<br>Value</b>', 
                '<b>A Simple<br>Return</b>', '<b>B Simple<br>Return</b>',
                '<b>A Log<br>Return</b>', '<b>B Log<br>Return</b>'],
        fill_color='#2C3E50',
        font=dict(color='white', size=10),
        align='center',
        height=40
    ),
    cells=dict(
        values=[raw_df[col] for col in raw_df.columns],
        fill_color=[['#f8f9fa' if i % 2 == 0 else 'white' for i in range(12)]] * 7,
        font=dict(size=10),
        align='center',
        height=26
    )
)])

fig_raw.update_layout(
    title=dict(text="<b>Complete Monthly Data: Products A & B</b>", x=0.5, font=dict(size=13)),
    height=450,
    margin=dict(l=20, r=20, t=60, b=20)
)

fig_raw.show()
```

#### C.2 Statistical Measures Reference {.unnumbered}

| Statistic | Product A | Product B | Formula |
|:----------|:---------:|:---------:|:--------|
| Arithmetic Mean (Simple) | `{python} f"{np.mean(monthly_discrete_a):.6f}"` | `{python} f"{np.mean(monthly_discrete_b):.6f}"` | $\bar{r} = \frac{1}{n}\sum_{i=1}^{n} r_i$ |
| Geometric Mean | `{python} f"{((1+annual_return_a)**(1/12)-1):.6f}"` | `{python} f"{((1+annual_return_b)**(1/12)-1):.6f}"` | $\bar{r}_g = \left(\prod_{i=1}^{n}(1+r_i)\right)^{1/n} - 1$ |
| Log Return Mean | `{python} f"{np.mean(monthly_log_a):.6f}"` | `{python} f"{np.mean(monthly_log_b):.6f}"` | $\bar{R} = \frac{1}{n}\sum_{i=1}^{n} R_i$ |
| Variance (Simple) | `{python} f"{np.var(monthly_discrete_a, ddof=1):.8f}"` | `{python} f"{np.var(monthly_discrete_b, ddof=1):.8f}"` | $\sigma^2 = \frac{1}{n-1}\sum_{i=1}^{n}(r_i - \bar{r})^2$ |
| Standard Deviation | `{python} f"{np.std(monthly_discrete_a, ddof=1):.6f}"` | `{python} f"{np.std(monthly_discrete_b, ddof=1):.6f}"` | $\sigma = \sqrt{\sigma^2}$ |

</details>

<details markdown="1">
<summary style="font-size: 1.3em; font-weight: bold; cursor: pointer; padding: 12px 15px; background-color: #ecf0f1; border-left: 5px solid #2c3e50; margin-bottom: 15px; border-radius: 4px;">Appendix C: Mathematical Derivations <em>(click to expand)</em></summary>

<div class="appendix-content">

**C.1 Continuous Compounding Derivation**

Starting from discrete compounding at frequency $n$ per year:

$$V(t) = V_0 \cdot \left(1 + \frac{r_{nom}}{n}\right)^{nt}$$

Taking the limit as $n \to \infty$:

$$\lim_{n \to \infty} \left(1 + \frac{r_{nom}}{n}\right)^{nt} = e^{r_{nom} \cdot t}$$

This follows from the definition: $e = \lim_{n \to \infty} \left(1 + \frac{1}{n}\right)^n$

**Key Identity:** For small $r$, $\ln(1+r) \approx r - \frac{r^2}{2} + \frac{r^3}{3} - ...$

**C.2 Minimum Variance Portfolio Derivation**

For a two-asset portfolio with weights $w$ and $(1-w)$:

$$\sigma_p^2 = w^2\sigma_1^2 + (1-w)^2\sigma_2^2 + 2w(1-w)\sigma_1\sigma_2\rho$$

Taking the first-order condition $\frac{\partial \sigma_p^2}{\partial w} = 0$:

$$2w\sigma_1^2 - 2(1-w)\sigma_2^2 + 2(1-2w)\sigma_1\sigma_2\rho = 0$$

Solving for $w$:

$$w^* = \frac{\sigma_2^2 - \sigma_1\sigma_2\rho}{\sigma_1^2 + \sigma_2^2 - 2\sigma_1\sigma_2\rho}$$

**C.3 Volatility Drag Formula**

For a portfolio with arithmetic mean return $\mu$ and variance $\sigma^2$:

$$\text{Geometric Mean} \approx \mu - \frac{\sigma^2}{2}$$

This "volatility drag" explains why:
- Higher volatility reduces compound growth
- Arithmetic mean overestimates long-term performance
- Risk matters even for expected return

</div>
</details>

<details markdown="1">
<summary style="font-size: 1.3em; font-weight: bold; cursor: pointer; padding: 12px 15px; background-color: #ecf0f1; border-left: 5px solid #2c3e50; margin-bottom: 15px; border-radius: 4px;">Appendix D: References & Further Reading <em>(click to expand)</em></summary>

<div class="appendix-content">

**Academic References**

1. **DeMiguel, V., Garlappi, L., & Uppal, R.** (2009). Optimal versus naive diversification: How inefficient is the 1/N portfolio strategy? *Review of Financial Studies*, 22(5), 1915-1953.
   - Landmark study on estimation error in portfolio optimization. Long but Very interesting read.

2. **Michaud, R. O.** (1989). The Markowitz optimization enigma: Is 'optimized' optimal? *Financial Analysts Journal*, 45(1), 31-42.
   - "Error maximization" property of mean-variance optimization

3. **Ledoit, O., & Wolf, M.** (2004). Honey, I shrunk the sample covariance matrix. *Journal of Portfolio Management*, 30(4), 110-119.
   - Shrinkage estimation for improved covariance matrices

4. **Black, F., & Litterman, R.** (1992). Global portfolio optimization. *Financial Analysts Journal*, 48(5), 28-43.
   - Bayesian approach to combining market equilibrium with investor views. Good paper but goes deep into matrices and stuff.

**Key Concepts Reference**

| Term | Definition |
|:-----|:-----------|
| **Simple Return** | $(P_t - P_{t-1})/P_{t-1}$: percentage change in price |
| **Log Return** | $\ln(P_t/P_{t-1})$: continuously compounded return |
| **Efficient Frontier** | Set of portfolios offering maximum return for each risk level |
| **MVP** | Minimum Variance Portfolio: lowest risk portfolio on frontier |
| **Sharpe Ratio** | $(E[r] - r_f)/\sigma$: risk-adjusted return measure |
| **Correlation** | $\rho = \text{Cov}(X,Y)/(\sigma_X \cdot \sigma_Y)$: linear relationship measure |

</div>
</details>
